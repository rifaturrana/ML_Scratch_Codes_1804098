{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arrRlEP49hpr"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import statistics\n",
        "import pandas as pd\n",
        "data = pd.read_csv('property_listing_data_in_Bangladesh.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(100)\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjzD1SsW3CNz",
        "outputId": "64ca5e75-2df9-49d9-ec99-e273d365c6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7557"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data=data[(data['type'] != 'Duplex') & (data['type'] != 'Building')]\n",
        "new_data['type']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPFCRrjY4Px7",
        "outputId": "4b14d059-b005-4717-92ab-7a77fb57e8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Apartment\n",
              "1       Apartment\n",
              "2       Apartment\n",
              "3       Apartment\n",
              "4       Apartment\n",
              "          ...    \n",
              "7551    Apartment\n",
              "7553    Apartment\n",
              "7554    Apartment\n",
              "7555    Apartment\n",
              "7556    Apartment\n",
              "Name: type, Length: 7489, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = new_data.drop(['title', 'adress', 'type', 'purpose', 'flooPlan', 'url', 'lastUpdated'] ,axis=1)\n",
        "new_data"
      ],
      "metadata": {
        "id": "0j5B_rpC6Moi",
        "outputId": "23163396-2b41-40be-f546-56b14ce1a397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     beds bath        area        price\n",
              "0      3    4   2,200 sqft  50 Thousand\n",
              "1      3    4   1,400 sqft  30 Thousand\n",
              "2      3    4   1,950 sqft  30 Thousand\n",
              "3      3    3   2,000 sqft  35 Thousand\n",
              "4      3    4   1,650 sqft  25 Thousand\n",
              "...   ...  ...         ...          ...\n",
              "7551   3    4   1,888 sqft  50 Thousand\n",
              "7553   3    2     900 sqft  19 Thousand\n",
              "7554   2    2   1,000 sqft  22 Thousand\n",
              "7555   3    4   3,600 sqft    1.75 Lakh\n",
              "7556   4    4   2,600 sqft  90 Thousand\n",
              "\n",
              "[7489 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d607c713-28bc-4422-b063-c7a30ee6d505\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7551</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,888 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7553</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>900 sqft</td>\n",
              "      <td>19 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7554</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1,000 sqft</td>\n",
              "      <td>22 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7555</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3,600 sqft</td>\n",
              "      <td>1.75 Lakh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7556</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2,600 sqft</td>\n",
              "      <td>90 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7489 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d607c713-28bc-4422-b063-c7a30ee6d505')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d607c713-28bc-4422-b063-c7a30ee6d505 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d607c713-28bc-4422-b063-c7a30ee6d505');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['bath'] = new_data['bath'].str.extract('(\\d+)')\n",
        "new_data['bath'] = pd.to_numeric(new_data['bath'])\n",
        "new_data['beds'] = new_data['beds'].str.extract('(\\d+)')\n",
        "new_data['beds'] = pd.to_numeric(new_data['beds'])\n",
        "new_data['area'] = new_data['area'].str.replace(',', '')\n",
        "new_data['area'] = new_data['area'].str.extract('(\\d+)')\n",
        "new_data['area'] = pd.to_numeric(new_data['area'])\n"
      ],
      "metadata": {
        "id": "iaYM9cAC_LYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['price'] = new_data['price'].apply(lambda x: float(x.split(' ')[0]) * 100000.0 if 'Lakh' in x else float(x.split(' ')[0]) * 1000)\n"
      ],
      "metadata": {
        "id": "DCDSoYG72P6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.head(10)"
      ],
      "metadata": {
        "id": "RA8StYxxCusY",
        "outputId": "1f3f766f-0c53-401a-c6fb-bd1db789fc3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   beds  bath  area     price\n",
              "0     3     4  2200   50000.0\n",
              "1     3     4  1400   30000.0\n",
              "2     3     4  1950   30000.0\n",
              "3     3     3  2000   35000.0\n",
              "4     3     4  1650   25000.0\n",
              "5     5     5  3400  110000.0\n",
              "6     3     3  1600   35000.0\n",
              "7     3     3  1250   23000.0\n",
              "8     3     4  2150   40000.0\n",
              "9     3     3  1250   23000.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87b46df9-df78-42b2-a58a-dabbae3c0080\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2200</td>\n",
              "      <td>50000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1400</td>\n",
              "      <td>30000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1950</td>\n",
              "      <td>30000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2000</td>\n",
              "      <td>35000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1650</td>\n",
              "      <td>25000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3400</td>\n",
              "      <td>110000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1600</td>\n",
              "      <td>35000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1250</td>\n",
              "      <td>23000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2150</td>\n",
              "      <td>40000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1250</td>\n",
              "      <td>23000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87b46df9-df78-42b2-a58a-dabbae3c0080')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87b46df9-df78-42b2-a58a-dabbae3c0080 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87b46df9-df78-42b2-a58a-dabbae3c0080');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data=new_data.to_numpy()"
      ],
      "metadata": {
        "id": "BT3ScG1dmsl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(new_data)"
      ],
      "metadata": {
        "id": "xE0fHrOcnyNd",
        "outputId": "670795f3-6fe0-4c2e-c17f-783c40dcab5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data"
      ],
      "metadata": {
        "id": "7rXqjr5ov3tT",
        "outputId": "c098517f-f890-4e5d-b469-362d80af769b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.00e+00, 4.00e+00, 2.20e+03, 5.00e+04],\n",
              "       [3.00e+00, 4.00e+00, 1.40e+03, 3.00e+04],\n",
              "       [3.00e+00, 4.00e+00, 1.95e+03, 3.00e+04],\n",
              "       ...,\n",
              "       [2.00e+00, 2.00e+00, 1.00e+03, 2.20e+04],\n",
              "       [3.00e+00, 4.00e+00, 3.60e+03, 1.75e+05],\n",
              "       [4.00e+00, 4.00e+00, 2.60e+03, 9.00e+04]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "def z_score_scaling(dataset):\n",
        "    mean_vals = np.mean(dataset, axis=0)\n",
        "    stdev_vals = np.std(dataset, axis=0)\n",
        "    scaled_dataset = (dataset - mean_vals) / stdev_vals\n",
        "    return scaled_dataset\n",
        "\n",
        "\n",
        "modified_list=np.array(new_data)\n",
        "X=modified_list[:,:-1]\n",
        "Y=modified_list[:,-1]\n",
        "X=z_score_scaling(X)\n",
        "X=np.array(X)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y, test_size=0.4, random_state=1)\n",
        "X_valid,X_test,y_valid,y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=1)\n",
        "print(\"Train data:\", len(X_train))\n",
        "print(\"Test data:\", len(X_valid))\n",
        "print(\"Validation data:\", len(X_test))"
      ],
      "metadata": {
        "id": "vdcbM1_p2y97",
        "outputId": "f6734d97-d6df-4859-fee3-5bfcda400d62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 4493\n",
            "Test data: 1498\n",
            "Validation data: 1498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_init = 10\n",
        "w_init = np.array([ .2, .2,50])\n",
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b\n",
        "        cost = cost + (f_wb_i - y[i])**2\n",
        "    cost = cost / (2 * m)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "GfALErskOIXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    return dj_db, dj_dw\n",
        "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n"
      ],
      "metadata": {
        "id": "ZVH9ZQnrQMYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
        "\n",
        "    w = copy.deepcopy(w_in)\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "        print(f\"Training Loss: {compute_cost(X_train,y_train,w,b)}\")\n",
        "        print(f\"Validation Loss: {compute_cost(X_valid,y_valid,w,b)}\")\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "pGhID52PQSyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_w = np.zeros_like(w_init)\n",
        "print(initial_w)\n",
        "initial_b = 1\n",
        "iterations = 1500\n",
        "alpha = 0.01\n",
        "# run gradient descent\n",
        "w_final, b_final = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")"
      ],
      "metadata": {
        "id": "RwxitmL0Qqce",
        "outputId": "49ab74c5-bb80-40b4-9b2a-50e09b9a8619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0.]\n",
            "Training Loss: 1223855180.632734\n",
            "Validation Loss: 1498615749.2918622\n",
            "Training Loss: 1199025458.5724714\n",
            "Validation Loss: 1470609018.4702244\n",
            "Training Loss: 1175087235.9186878\n",
            "Validation Loss: 1443585497.0537493\n",
            "Training Loss: 1152002680.7718582\n",
            "Validation Loss: 1417504059.3460221\n",
            "Training Loss: 1129735693.0207024\n",
            "Validation Loss: 1392325442.2883534\n",
            "Training Loss: 1108251822.3658283\n",
            "Validation Loss: 1368012157.9403944\n",
            "Training Loss: 1087518190.2798653\n",
            "Validation Loss: 1344528410.1427484\n",
            "Training Loss: 1067503415.7139624\n",
            "Validation Loss: 1321840015.1600463\n",
            "Training Loss: 1048177544.3696527\n",
            "Validation Loss: 1299914326.112999\n",
            "Training Loss: 1029511981.3638861\n",
            "Validation Loss: 1278720161.0169783\n",
            "Training Loss: 1011479427.1234165\n",
            "Validation Loss: 1258227734.253584\n",
            "Training Loss: 994053816.3525275\n",
            "Validation Loss: 1238408591.3099651\n",
            "Training Loss: 977210259.925892\n",
            "Validation Loss: 1219235546.6287925\n",
            "Training Loss: 960924989.565141\n",
            "Validation Loss: 1200682624.4191697\n",
            "Training Loss: 945175305.1650201\n",
            "Validation Loss: 1182725002.2862034\n",
            "Training Loss: 929939524.6410891\n",
            "Validation Loss: 1165338957.543615\n",
            "Training Loss: 915196936.1774822\n",
            "Validation Loss: 1148501816.0805638\n",
            "Training Loss: 900927752.7587631\n",
            "Validation Loss: 1132191903.6597903\n",
            "Training Loss: 887113068.8759034\n",
            "Validation Loss: 1116388499.5304477\n",
            "Training Loss: 873734819.30138\n",
            "Validation Loss: 1101071792.244279\n",
            "Training Loss: 860775739.8337039\n",
            "Validation Loss: 1086222837.5694797\n",
            "Training Loss: 848219329.9165214\n",
            "Validation Loss: 1071823518.4014249\n",
            "Training Loss: 836049817.0416921\n",
            "Validation Loss: 1057856506.5745116\n",
            "Training Loss: 824252122.8507354\n",
            "Validation Loss: 1044305226.4838787\n",
            "Training Loss: 812811830.852493\n",
            "Validation Loss: 1031153820.4301298\n",
            "Training Loss: 801715155.6793545\n",
            "Validation Loss: 1018387115.6045331\n",
            "Training Loss: 790948913.8078938\n",
            "Validation Loss: 1005990592.6359785\n",
            "Training Loss: 780500495.6734354\n",
            "Validation Loss: 993950355.6248505\n",
            "Training Loss: 770357839.1113651\n",
            "Validation Loss: 982253103.5926367\n",
            "Training Loss: 760509404.0614945\n",
            "Validation Loss: 970886103.2793851\n",
            "Training Loss: 750944148.4745756\n",
            "Validation Loss: 959837163.2245276\n",
            "Training Loss: 741651505.3631724\n",
            "Validation Loss: 949094609.0695956\n",
            "Training Loss: 732621360.9419185\n",
            "Validation Loss: 938647260.0244386\n",
            "Training Loss: 723844033.8047191\n",
            "Validation Loss: 928484406.4411994\n",
            "Training Loss: 715310255.0890831\n",
            "Validation Loss: 918595788.4431298\n",
            "Training Loss: 707011149.5801569\n",
            "Validation Loss: 908971575.5578512\n",
            "Training Loss: 698938217.709352\n",
            "Validation Loss: 899602347.306985\n",
            "Training Loss: 691083318.4045081\n",
            "Validation Loss: 890479074.7065789\n",
            "Training Loss: 683438652.750867\n",
            "Validation Loss: 881593102.6347544\n",
            "Training Loss: 675996748.4237528\n",
            "Validation Loss: 872936133.0252584\n",
            "Training Loss: 668750444.8561347\n",
            "Validation Loss: 864500208.8474526\n",
            "Training Loss: 661692879.1056771\n",
            "Validation Loss: 856277698.8353345\n",
            "Training Loss: 654817472.3878235\n",
            "Validation Loss: 848261282.9297631\n",
            "Training Loss: 648117917.2429061\n",
            "Validation Loss: 840443938.4000435\n",
            "Training Loss: 641588165.3070534\n",
            "Validation Loss: 832818926.6124338\n",
            "Training Loss: 635222415.6578008\n",
            "Validation Loss: 825379780.4147904\n",
            "Training Loss: 629015103.7070383\n",
            "Validation Loss: 818120292.1080935\n",
            "Training Loss: 622960890.6149938\n",
            "Validation Loss: 811034501.976887\n",
            "Training Loss: 617054653.2004074\n",
            "Validation Loss: 804116687.3520886\n",
            "Training Loss: 611291474.3231454\n",
            "Validation Loss: 797361352.1809196\n",
            "Training Loss: 605666633.7166121\n",
            "Validation Loss: 790763217.0798078\n",
            "Training Loss: 600175599.2485813\n",
            "Validation Loss: 784317209.8474251\n",
            "Training Loss: 594814018.5899153\n",
            "Validation Loss: 778018456.4159561\n",
            "Training Loss: 589577711.271699\n",
            "Validation Loss: 771862272.2199186\n",
            "Training Loss: 584462661.1123017\n",
            "Validation Loss: 765844153.9626865\n",
            "Training Loss: 579465008.9966959\n",
            "Validation Loss: 759959771.7619114\n",
            "Training Loss: 574581045.9912573\n",
            "Validation Loss: 754204961.6559421\n",
            "Training Loss: 569807206.7780346\n",
            "Validation Loss: 748575718.4541283\n",
            "Training Loss: 565140063.3933578\n",
            "Validation Loss: 743068188.9148158\n",
            "Training Loss: 560576319.2561758\n",
            "Validation Loss: 737678665.2355131\n",
            "Training Loss: 556112803.4725039\n",
            "Validation Loss: 732403578.8405224\n",
            "Training Loss: 551746465.4026945\n",
            "Validation Loss: 727239494.4520216\n",
            "Training Loss: 547474369.4791764\n",
            "Validation Loss: 722183104.4312184\n",
            "Training Loss: 543293690.2627162\n",
            "Validation Loss: 717231223.3768842\n",
            "Training Loss: 539201707.7258624\n",
            "Validation Loss: 712380782.969131\n",
            "Training Loss: 535195802.7528556\n",
            "Validation Loss: 707628827.046951\n",
            "Training Loss: 531273452.8457389\n",
            "Validation Loss: 702972506.90849\n",
            "Training Loss: 527432228.02684397\n",
            "Validation Loss: 698409076.8236265\n",
            "Training Loss: 523669786.9284542\n",
            "Validation Loss: 693935889.7489139\n",
            "Training Loss: 519983873.06074464\n",
            "Validation Loss: 689550393.2353679\n",
            "Training Loss: 516372311.2495517\n",
            "Validation Loss: 685250125.52013\n",
            "Training Loss: 512833004.2360134\n",
            "Validation Loss: 681032711.793322\n",
            "Training Loss: 509363929.43044484\n",
            "Validation Loss: 676895860.6320186\n",
            "Training Loss: 505963135.81307405\n",
            "Validation Loss: 672837360.5934433\n",
            "Training Loss: 502628740.97490823\n",
            "Validation Loss: 668855076.9600033\n",
            "Training Loss: 499358928.29193425\n",
            "Validation Loss: 664946948.6290991\n",
            "Training Loss: 496151944.22655416\n",
            "Validation Loss: 661110985.1409408\n",
            "Training Loss: 493006095.75017726\n",
            "Validation Loss: 657345263.838004\n",
            "Training Loss: 489919747.8813143\n",
            "Validation Loss: 653647927.1499462\n",
            "Training Loss: 486891321.333783\n",
            "Validation Loss: 650017179.9982408\n",
            "Training Loss: 483919290.2698304\n",
            "Validation Loss: 646451287.314922\n",
            "Training Loss: 481002180.1532919\n",
            "Validation Loss: 642948571.6701785\n",
            "Training Loss: 478138565.6980818\n",
            "Validation Loss: 639507411.0038018\n",
            "Training Loss: 475327068.9076182\n",
            "Validation Loss: 636126236.4556165\n",
            "Training Loss: 472566357.2008874\n",
            "Validation Loss: 632803530.2904272\n",
            "Training Loss: 469855141.62112963\n",
            "Validation Loss: 629537823.9130292\n",
            "Training Loss: 467192175.12330997\n",
            "Validation Loss: 626327695.9692385\n",
            "Training Loss: 464576250.9367328\n",
            "Validation Loss: 623171770.5289097\n",
            "Training Loss: 462006200.99925464\n",
            "Validation Loss: 620068715.3472356\n",
            "Training Loss: 459480894.45982486\n",
            "Validation Loss: 617017240.2007204\n",
            "Training Loss: 456999236.2462014\n",
            "Validation Loss: 614016095.2944174\n",
            "Training Loss: 454560165.6947583\n",
            "Validation Loss: 611064069.7371789\n",
            "Training Loss: 452162655.23960674\n",
            "Validation Loss: 608159990.0818247\n",
            "Training Loss: 449805709.15824753\n",
            "Validation Loss: 605302718.9272406\n",
            "Training Loss: 447488362.37112355\n",
            "Validation Loss: 602491153.5796587\n",
            "Training Loss: 445209679.29266757\n",
            "Validation Loss: 599724224.7703593\n",
            "Training Loss: 442968752.7314502\n",
            "Validation Loss: 597000895.4272975\n",
            "Training Loss: 440764702.8371202\n",
            "Validation Loss: 594320159.4981841\n",
            "Training Loss: 438596676.09209543\n",
            "Validation Loss: 591681040.8227094\n",
            "Training Loss: 436463844.3459018\n",
            "Validation Loss: 589082592.0516977\n",
            "Training Loss: 434365403.89021355\n",
            "Validation Loss: 586523893.6110722\n",
            "Training Loss: 432300574.57276607\n",
            "Validation Loss: 584004052.7086469\n",
            "Training Loss: 430268598.94834733\n",
            "Validation Loss: 581522202.38177\n",
            "Training Loss: 428268741.4652244\n",
            "Validation Loss: 579077500.5840697\n",
            "Training Loss: 426300287.6853676\n",
            "Validation Loss: 576669129.3094827\n",
            "Training Loss: 424362543.5369355\n",
            "Validation Loss: 574296293.7519661\n",
            "Training Loss: 422454834.5976214\n",
            "Validation Loss: 571958221.4992595\n",
            "Training Loss: 420576505.4074075\n",
            "Validation Loss: 569654161.7592248\n",
            "Training Loss: 418726918.80944806\n",
            "Validation Loss: 567383384.617294\n",
            "Training Loss: 416905455.3177966\n",
            "Validation Loss: 565145180.323674\n",
            "Training Loss: 415111512.510806\n",
            "Validation Loss: 562938858.608984\n",
            "Training Loss: 413344504.44899714\n",
            "Validation Loss: 560763748.0270813\n",
            "Training Loss: 411603861.11639917\n",
            "Validation Loss: 558619195.3238769\n",
            "Training Loss: 409889027.8842186\n",
            "Validation Loss: 556504564.8310313\n",
            "Training Loss: 408199464.99591714\n",
            "Validation Loss: 554419237.8833942\n",
            "Training Loss: 406534647.0727163\n",
            "Validation Loss: 552362612.2592177\n",
            "Training Loss: 404894062.6386536\n",
            "Validation Loss: 550334101.6420958\n",
            "Training Loss: 403277213.6643051\n",
            "Validation Loss: 548333135.103741\n",
            "Training Loss: 401683615.12837446\n",
            "Validation Loss: 546359156.6066576\n",
            "Training Loss: 400112794.5963338\n",
            "Validation Loss: 544411624.5258833\n",
            "Training Loss: 398564291.8154434\n",
            "Validation Loss: 542490011.188973\n",
            "Training Loss: 397037658.32534194\n",
            "Validation Loss: 540593802.4334254\n",
            "Training Loss: 395532457.0836186\n",
            "Validation Loss: 538722497.180846\n",
            "Training Loss: 394048262.1056491\n",
            "Validation Loss: 536875607.0271075\n",
            "Training Loss: 392584658.11812615\n",
            "Validation Loss: 535052655.8478152\n",
            "Training Loss: 391141240.2256932\n",
            "Validation Loss: 533253179.4184908\n",
            "Training Loss: 389717613.5900782\n",
            "Validation Loss: 531476725.0487714\n",
            "Training Loss: 388313393.12123436\n",
            "Validation Loss: 529722851.23010904\n",
            "Training Loss: 386928203.1799815\n",
            "Validation Loss: 527991127.2963552\n",
            "Training Loss: 385561677.291612\n",
            "Validation Loss: 526281133.09672743\n",
            "Training Loss: 384213457.8700379\n",
            "Validation Loss: 524592458.6806049\n",
            "Training Loss: 382883195.9520289\n",
            "Validation Loss: 522924703.9937232\n",
            "Training Loss: 381570550.94110924\n",
            "Validation Loss: 521277478.58521765\n",
            "Training Loss: 380275190.3606975\n",
            "Validation Loss: 519650401.3251347\n",
            "Training Loss: 378996789.616145\n",
            "Validation Loss: 518043100.1319616\n",
            "Training Loss: 377735031.7652412\n",
            "Validation Loss: 516455211.70975345\n",
            "Training Loss: 376489607.2968884\n",
            "Validation Loss: 514886381.29447377\n",
            "Training Loss: 375260213.9175917\n",
            "Validation Loss: 513336262.40918916\n",
            "Training Loss: 374046556.3454338\n",
            "Validation Loss: 511804516.627741\n",
            "Training Loss: 372848346.1112251\n",
            "Validation Loss: 510290813.3465654\n",
            "Training Loss: 371665301.36657107\n",
            "Validation Loss: 508794829.564326\n",
            "Training Loss: 370497146.6985296\n",
            "Validation Loss: 507316249.66906357\n",
            "Training Loss: 369343612.95061207\n",
            "Validation Loss: 505854765.23253775\n",
            "Training Loss: 368204437.0498939\n",
            "Validation Loss: 504410074.81152165\n",
            "Training Loss: 367079361.8399588\n",
            "Validation Loss: 502981883.7557025\n",
            "Training Loss: 365968135.9194454\n",
            "Validation Loss: 501569904.0220181\n",
            "Training Loss: 364870513.4860024\n",
            "Validation Loss: 500173853.99511224\n",
            "Training Loss: 363786254.18540126\n",
            "Validation Loss: 498793458.31368214\n",
            "Training Loss: 362715122.96563596\n",
            "Validation Loss: 497428447.70252424\n",
            "Training Loss: 361656889.9357771\n",
            "Validation Loss: 496078558.8100151\n",
            "Training Loss: 360611330.2294712\n",
            "Validation Loss: 494743534.0508569\n",
            "Training Loss: 359578223.87276673\n",
            "Validation Loss: 493423121.4538486\n",
            "Training Loss: 358557355.65627015\n",
            "Validation Loss: 492117074.51452476\n",
            "Training Loss: 357548515.0113189\n",
            "Validation Loss: 490825152.0524714\n",
            "Training Loss: 356551495.89011973\n",
            "Validation Loss: 489547118.0731217\n",
            "Training Loss: 355566096.6496266\n",
            "Validation Loss: 488282741.633894\n",
            "Training Loss: 354592119.9390794\n",
            "Validation Loss: 487031796.7144857\n",
            "Training Loss: 353629372.59102434\n",
            "Validation Loss: 485794062.09118897\n",
            "Training Loss: 352677665.5156934\n",
            "Validation Loss: 484569321.2150588\n",
            "Training Loss: 351736813.59863615\n",
            "Validation Loss: 483357362.0938241\n",
            "Training Loss: 350806635.60146445\n",
            "Validation Loss: 482157977.1773635\n",
            "Training Loss: 349886954.06558776\n",
            "Validation Loss: 480970963.2466458\n",
            "Training Loss: 348977595.21888584\n",
            "Validation Loss: 479796121.3060253\n",
            "Training Loss: 348078388.88510436\n",
            "Validation Loss: 478633256.47870827\n",
            "Training Loss: 347189168.39599854\n",
            "Validation Loss: 477482177.90535116\n",
            "Training Loss: 346309770.5060436\n",
            "Validation Loss: 476342698.6456348\n",
            "Training Loss: 345440035.30962646\n",
            "Validation Loss: 475214635.5827238\n",
            "Training Loss: 344579806.1606924\n",
            "Validation Loss: 474097809.3305054\n",
            "Training Loss: 343728929.5946578\n",
            "Validation Loss: 472992044.1435157\n",
            "Training Loss: 342887255.2526324\n",
            "Validation Loss: 471897167.8294524\n",
            "Training Loss: 342054635.80774015\n",
            "Validation Loss: 470813011.66420215\n",
            "Training Loss: 341230926.8935765\n",
            "Validation Loss: 469739410.30926716\n",
            "Training Loss: 340415987.0346565\n",
            "Validation Loss: 468676201.73153764\n",
            "Training Loss: 339609677.5788346\n",
            "Validation Loss: 467623227.1253117\n",
            "Training Loss: 338811862.6315751\n",
            "Validation Loss: 466580330.8365004\n",
            "Training Loss: 338022408.9920767\n",
            "Validation Loss: 465547360.2889438\n",
            "Training Loss: 337241186.09112\n",
            "Validation Loss: 464524165.91273373\n",
            "Training Loss: 336468065.9306284\n",
            "Validation Loss: 463510601.07454073\n",
            "Training Loss: 335702923.0248864\n",
            "Validation Loss: 462506522.00982374\n",
            "Training Loss: 334945634.3433035\n",
            "Validation Loss: 461511787.75688493\n",
            "Training Loss: 334196079.254768\n",
            "Validation Loss: 460526260.0927062\n",
            "Training Loss: 333454139.47343034\n",
            "Validation Loss: 459549803.47050244\n",
            "Training Loss: 332719699.005966\n",
            "Validation Loss: 458582284.9589601\n",
            "Training Loss: 331992644.10020924\n",
            "Validation Loss: 457623574.1830708\n",
            "Training Loss: 331272863.195142\n",
            "Validation Loss: 456673543.26653016\n",
            "Training Loss: 330560246.8721684\n",
            "Validation Loss: 455732066.77567077\n",
            "Training Loss: 329854687.8076814\n",
            "Validation Loss: 454799021.6648337\n",
            "Training Loss: 329156080.7268292\n",
            "Validation Loss: 453874287.2231804\n",
            "Training Loss: 328464322.35846853\n",
            "Validation Loss: 452957745.0228736\n",
            "Training Loss: 327779311.3912806\n",
            "Validation Loss: 452049278.8685882\n",
            "Training Loss: 327100948.4309886\n",
            "Validation Loss: 451148774.74832416\n",
            "Training Loss: 326429135.9586516\n",
            "Validation Loss: 450256120.78546816\n",
            "Training Loss: 325763778.29000646\n",
            "Validation Loss: 449371207.1920683\n",
            "Training Loss: 325104781.5358378\n",
            "Validation Loss: 448493926.22329205\n",
            "Training Loss: 324452053.56331635\n",
            "Validation Loss: 447624172.13302785\n",
            "Training Loss: 323805503.9582837\n",
            "Validation Loss: 446761841.1305956\n",
            "Training Loss: 323165043.9885034\n",
            "Validation Loss: 445906831.3385309\n",
            "Training Loss: 322530586.56775814\n",
            "Validation Loss: 445059042.75141424\n",
            "Training Loss: 321902046.22086966\n",
            "Validation Loss: 444218377.19572735\n",
            "Training Loss: 321279339.04952914\n",
            "Validation Loss: 443384738.290675\n",
            "Training Loss: 320662382.6989673\n",
            "Validation Loss: 442558031.4099812\n",
            "Training Loss: 320051096.3254336\n",
            "Validation Loss: 441738163.64459366\n",
            "Training Loss: 319445400.5644331\n",
            "Validation Loss: 440925043.76632434\n",
            "Training Loss: 318845217.4997179\n",
            "Validation Loss: 440118582.1923282\n",
            "Training Loss: 318250470.6330461\n",
            "Validation Loss: 439318690.9504592\n",
            "Training Loss: 317661084.85461175\n",
            "Validation Loss: 438525283.6454455\n",
            "Training Loss: 317076986.4141875\n",
            "Validation Loss: 437738275.42588115\n",
            "Training Loss: 316498102.89294636\n",
            "Validation Loss: 436957582.9519751\n",
            "Training Loss: 315924363.1759335\n",
            "Validation Loss: 436183124.36409473\n",
            "Training Loss: 315355697.4251683\n",
            "Validation Loss: 435414819.25201595\n",
            "Training Loss: 314792037.0533843\n",
            "Validation Loss: 434652588.62492585\n",
            "Training Loss: 314233314.69836086\n",
            "Validation Loss: 433896354.8820949\n",
            "Training Loss: 313679464.19784915\n",
            "Validation Loss: 433146041.7842603\n",
            "Training Loss: 313130420.56504935\n",
            "Validation Loss: 432401574.42564076\n",
            "Training Loss: 312586119.96469826\n",
            "Validation Loss: 431662879.206631\n",
            "Training Loss: 312046499.68963164\n",
            "Validation Loss: 430929883.8070965\n",
            "Training Loss: 311511498.13790977\n",
            "Validation Loss: 430202517.1602909\n",
            "Training Loss: 310981054.7904828\n",
            "Validation Loss: 429480709.4273733\n",
            "Training Loss: 310455110.1892934\n",
            "Validation Loss: 428764391.9725016\n",
            "Training Loss: 309933605.9159258\n",
            "Validation Loss: 428053497.33848214\n",
            "Training Loss: 309416484.5707038\n",
            "Validation Loss: 427347959.22298867\n",
            "Training Loss: 308903689.7522531\n",
            "Validation Loss: 426647712.4552941\n",
            "Training Loss: 308395166.03752\n",
            "Validation Loss: 425952692.9735474\n",
            "Training Loss: 307890858.9622333\n",
            "Validation Loss: 425262837.8025411\n",
            "Training Loss: 307390715.0017994\n",
            "Validation Loss: 424578085.03198993\n",
            "Training Loss: 306894681.5525817\n",
            "Validation Loss: 423898373.7952721\n",
            "Training Loss: 306402706.9136542\n",
            "Validation Loss: 423223644.24867105\n",
            "Training Loss: 305914740.2688849\n",
            "Validation Loss: 422553837.5510485\n",
            "Training Loss: 305430731.6694529\n",
            "Validation Loss: 421888895.8439803\n",
            "Training Loss: 304950632.0167182\n",
            "Validation Loss: 421228762.2323313\n",
            "Training Loss: 304474393.045492\n",
            "Validation Loss: 420573380.7652441\n",
            "Training Loss: 304001967.30762726\n",
            "Validation Loss: 419922696.41755134\n",
            "Training Loss: 303533308.1559928\n",
            "Validation Loss: 419276655.071598\n",
            "Training Loss: 303068369.72878224\n",
            "Validation Loss: 418635203.4994487\n",
            "Training Loss: 302607106.93415004\n",
            "Validation Loss: 417998289.345482\n",
            "Training Loss: 302149475.4351934\n",
            "Validation Loss: 417365861.1093743\n",
            "Training Loss: 301695431.6352208\n",
            "Validation Loss: 416737868.129432\n",
            "Training Loss: 301244932.6633823\n",
            "Validation Loss: 416114260.56629723\n",
            "Training Loss: 300797936.36055547\n",
            "Validation Loss: 415494989.38699293\n",
            "Training Loss: 300354401.26555806\n",
            "Validation Loss: 414880006.3493251\n",
            "Training Loss: 299914286.60164297\n",
            "Validation Loss: 414269263.98658955\n",
            "Training Loss: 299477552.26327866\n",
            "Validation Loss: 413662715.59264165\n",
            "Training Loss: 299044158.8032041\n",
            "Validation Loss: 413060315.2072527\n",
            "Training Loss: 298614067.4197537\n",
            "Validation Loss: 412462017.6017935\n",
            "Training Loss: 298187239.94445926\n",
            "Validation Loss: 411867778.26521796\n",
            "Training Loss: 297763638.8298895\n",
            "Validation Loss: 411277553.3903374\n",
            "Training Loss: 297343227.1377685\n",
            "Validation Loss: 410691299.8603906\n",
            "Training Loss: 296925968.5273063\n",
            "Validation Loss: 410108975.23589265\n",
            "Training Loss: 296511827.24382013\n",
            "Validation Loss: 409530537.7417649\n",
            "Training Loss: 296100768.10753644\n",
            "Validation Loss: 408955946.2547228\n",
            "Training Loss: 295692756.5026664\n",
            "Validation Loss: 408385160.29094684\n",
            "Training Loss: 295287758.3666874\n",
            "Validation Loss: 407818139.9939832\n",
            "Training Loss: 294885740.17984843\n",
            "Validation Loss: 407254846.12293065\n",
            "Training Loss: 294486668.9548876\n",
            "Validation Loss: 406695240.04083747\n",
            "Training Loss: 294090512.22697276\n",
            "Validation Loss: 406139283.70336765\n",
            "Training Loss: 293697238.0438329\n",
            "Validation Loss: 405586939.6476849\n",
            "Training Loss: 293306814.9561024\n",
            "Validation Loss: 405038170.9815707\n",
            "Training Loss: 292919212.0078578\n",
            "Validation Loss: 404492941.37277186\n",
            "Training Loss: 292534398.72734225\n",
            "Validation Loss: 403951215.0385567\n",
            "Training Loss: 292152345.1178924\n",
            "Validation Loss: 403412956.73550045\n",
            "Training Loss: 291773021.6490394\n",
            "Validation Loss: 402878131.74946225\n",
            "Training Loss: 291396399.2477896\n",
            "Validation Loss: 402346705.8857863\n",
            "Training Loss: 291022449.29008806\n",
            "Validation Loss: 401818645.45968777\n",
            "Training Loss: 290651143.59244734\n",
            "Validation Loss: 401293917.286841\n",
            "Training Loss: 290282454.40375525\n",
            "Validation Loss: 400772488.67416656\n",
            "Training Loss: 289916354.3972401\n",
            "Validation Loss: 400254327.41079277\n",
            "Training Loss: 289552816.6625962\n",
            "Validation Loss: 399739401.7592045\n",
            "Training Loss: 289191814.69828194\n",
            "Validation Loss: 399227680.44658095\n",
            "Training Loss: 288833322.40394926\n",
            "Validation Loss: 398719132.6562984\n",
            "Training Loss: 288477314.0730445\n",
            "Validation Loss: 398213728.01960695\n",
            "Training Loss: 288123764.3855536\n",
            "Validation Loss: 397711436.60747516\n",
            "Training Loss: 287772648.4008872\n",
            "Validation Loss: 397212228.92260253\n",
            "Training Loss: 287423941.5509085\n",
            "Validation Loss: 396716075.89159065\n",
            "Training Loss: 287077619.6331053\n",
            "Validation Loss: 396222948.8572676\n",
            "Training Loss: 286733658.8038959\n",
            "Validation Loss: 395732819.57117254\n",
            "Training Loss: 286392035.5720632\n",
            "Validation Loss: 395245660.1861867\n",
            "Training Loss: 286052726.7923375\n",
            "Validation Loss: 394761443.2493146\n",
            "Training Loss: 285715709.6590691\n",
            "Validation Loss: 394280141.6946069\n",
            "Training Loss: 285380961.7000802\n",
            "Validation Loss: 393801728.83622444\n",
            "Training Loss: 285048460.77058816\n",
            "Validation Loss: 393326178.36164373\n",
            "Training Loss: 284718185.04727066\n",
            "Validation Loss: 392853464.3249933\n",
            "Training Loss: 284390113.0224656\n",
            "Validation Loss: 392383561.1405265\n",
            "Training Loss: 284064223.49844944\n",
            "Validation Loss: 391916443.5762174\n",
            "Training Loss: 283740495.5818551\n",
            "Validation Loss: 391452086.7474918\n",
            "Training Loss: 283418908.67819446\n",
            "Validation Loss: 390990466.11107576\n",
            "Training Loss: 283099442.4864773\n",
            "Validation Loss: 390531557.45896834\n",
            "Training Loss: 282782076.9939513\n",
            "Validation Loss: 390075336.9125345\n",
            "Training Loss: 282466792.4709408\n",
            "Validation Loss: 389621780.9167096\n",
            "Training Loss: 282153569.465777\n",
            "Validation Loss: 389170866.2343218\n",
            "Training Loss: 281842388.7998472\n",
            "Validation Loss: 388722569.9405273\n",
            "Training Loss: 281533231.56271917\n",
            "Validation Loss: 388276869.41734904\n",
            "Training Loss: 281226079.10737884\n",
            "Validation Loss: 387833742.34832966\n",
            "Training Loss: 280920913.04554814\n",
            "Validation Loss: 387393166.7132823\n",
            "Training Loss: 280617715.2431081\n",
            "Validation Loss: 386955120.7831498\n",
            "Training Loss: 280316467.8155903\n",
            "Validation Loss: 386519583.11495525\n",
            "Training Loss: 280017153.123783\n",
            "Validation Loss: 386086532.5468683\n",
            "Training Loss: 279719753.76940256\n",
            "Validation Loss: 385655948.19334674\n",
            "Training Loss: 279424252.5908556\n",
            "Validation Loss: 385227809.44038355\n",
            "Training Loss: 279130632.65907735\n",
            "Validation Loss: 384802095.94085\n",
            "Training Loss: 278838877.27347404\n",
            "Validation Loss: 384378787.6099168\n",
            "Training Loss: 278548969.9579171\n",
            "Validation Loss: 383957864.6205797\n",
            "Training Loss: 278260894.4568124\n",
            "Validation Loss: 383539307.399253\n",
            "Training Loss: 277974634.731283\n",
            "Validation Loss: 383123096.62146914\n",
            "Training Loss: 277690174.9553836\n",
            "Validation Loss: 382709213.20763785\n",
            "Training Loss: 277407499.5124164\n",
            "Validation Loss: 382297638.3189124\n",
            "Training Loss: 277126592.9912945\n",
            "Validation Loss: 381888353.3531121\n",
            "Training Loss: 276847440.1829997\n",
            "Validation Loss: 381481339.9407406\n",
            "Training Loss: 276570026.0770948\n",
            "Validation Loss: 381076579.9410713\n",
            "Training Loss: 276294335.8583022\n",
            "Validation Loss: 380674055.4383074\n",
            "Training Loss: 276020354.9031566\n",
            "Validation Loss: 380273748.7378221\n",
            "Training Loss: 275748068.7767126\n",
            "Validation Loss: 379875642.3624678\n",
            "Training Loss: 275477463.2293249\n",
            "Validation Loss: 379479719.0489477\n",
            "Training Loss: 275208524.1934813\n",
            "Validation Loss: 379085961.74427295\n",
            "Training Loss: 274941237.7807042\n",
            "Validation Loss: 378694353.60227257\n",
            "Training Loss: 274675590.27850515\n",
            "Validation Loss: 378304877.98017883\n",
            "Training Loss: 274411568.147401\n",
            "Validation Loss: 377917518.43526614\n",
            "Training Loss: 274149158.01799065\n",
            "Validation Loss: 377532258.72157604\n",
            "Training Loss: 273888346.68808144\n",
            "Validation Loss: 377149082.7866757\n",
            "Training Loss: 273629121.1198684\n",
            "Validation Loss: 376767974.7685016\n",
            "Training Loss: 273371468.4371785\n",
            "Validation Loss: 376388918.9922454\n",
            "Training Loss: 273115375.9227549\n",
            "Validation Loss: 376011899.96731615\n",
            "Training Loss: 272860831.01560104\n",
            "Validation Loss: 375636902.3843413\n",
            "Training Loss: 272607821.30837786\n",
            "Validation Loss: 375263911.11223924\n",
            "Training Loss: 272356334.54482937\n",
            "Validation Loss: 374892911.1953374\n",
            "Training Loss: 272106358.6172926\n",
            "Validation Loss: 374523887.85054916\n",
            "Training Loss: 271857881.5642274\n",
            "Validation Loss: 374156826.46460444\n",
            "Training Loss: 271610891.567789\n",
            "Validation Loss: 373791712.59132326\n",
            "Training Loss: 271365376.95148\n",
            "Validation Loss: 373428531.94896024\n",
            "Training Loss: 271121326.1778084\n",
            "Validation Loss: 373067270.4175715\n",
            "Training Loss: 270878727.8460121\n",
            "Validation Loss: 372707914.03645194\n",
            "Training Loss: 270637570.689822\n",
            "Validation Loss: 372350449.0016176\n",
            "Training Loss: 270397843.5752669\n",
            "Validation Loss: 371994861.6633224\n",
            "Training Loss: 270159535.4985142\n",
            "Validation Loss: 371641138.52363485\n",
            "Training Loss: 269922635.58375925\n",
            "Validation Loss: 371289266.23405313\n",
            "Training Loss: 269687133.08115005\n",
            "Validation Loss: 370939231.59316295\n",
            "Training Loss: 269453017.3647538\n",
            "Validation Loss: 370591021.5443451\n",
            "Training Loss: 269220277.9305612\n",
            "Validation Loss: 370244623.1735212\n",
            "Training Loss: 268988904.394512\n",
            "Validation Loss: 369900023.7069374\n",
            "Training Loss: 268758886.4905989\n",
            "Validation Loss: 369557210.5089988\n",
            "Training Loss: 268530214.0689556\n",
            "Validation Loss: 369216171.08013654\n",
            "Training Loss: 268302877.09401804\n",
            "Validation Loss: 368876893.05471194\n",
            "Training Loss: 268076865.64270338\n",
            "Validation Loss: 368539364.1989663\n",
            "Training Loss: 267852169.902625\n",
            "Validation Loss: 368203572.40900505\n",
            "Training Loss: 267628780.17034554\n",
            "Validation Loss: 367869505.70881724\n",
            "Training Loss: 267406686.84966186\n",
            "Validation Loss: 367537152.24833214\n",
            "Training Loss: 267185880.4499064\n",
            "Validation Loss: 367206500.3015167\n",
            "Training Loss: 266966351.58431008\n",
            "Validation Loss: 366877538.264492\n",
            "Training Loss: 266748090.9683674\n",
            "Validation Loss: 366550254.65370935\n",
            "Training Loss: 266531089.41824284\n",
            "Validation Loss: 366224638.1041309\n",
            "Training Loss: 266315337.849209\n",
            "Validation Loss: 365900677.367469\n",
            "Training Loss: 266100827.2741138\n",
            "Validation Loss: 365578361.31043786\n",
            "Training Loss: 265887548.801872\n",
            "Validation Loss: 365257678.91304964\n",
            "Training Loss: 265675493.63598064\n",
            "Validation Loss: 364938619.2669315\n",
            "Training Loss: 265464653.07307407\n",
            "Validation Loss: 364621171.57368296\n",
            "Training Loss: 265255018.50149453\n",
            "Validation Loss: 364305325.1432605\n",
            "Training Loss: 265046581.3998945\n",
            "Validation Loss: 363991069.39237696\n",
            "Training Loss: 264839333.33586153\n",
            "Validation Loss: 363678393.8429543\n",
            "Training Loss: 264633265.96457478\n",
            "Validation Loss: 363367288.12058413\n",
            "Training Loss: 264428371.02747265\n",
            "Validation Loss: 363057741.9530235\n",
            "Training Loss: 264224640.3509582\n",
            "Validation Loss: 362749745.1687189\n",
            "Training Loss: 264022065.84513116\n",
            "Validation Loss: 362443287.6953566\n",
            "Training Loss: 263820639.5025169\n",
            "Validation Loss: 362138359.55843055\n",
            "Training Loss: 263620353.3968528\n",
            "Validation Loss: 361834950.8798439\n",
            "Training Loss: 263421199.6818803\n",
            "Validation Loss: 361533051.87654245\n",
            "Training Loss: 263223170.59014028\n",
            "Validation Loss: 361232652.859149\n",
            "Training Loss: 263026258.43184245\n",
            "Validation Loss: 360933744.230647\n",
            "Training Loss: 262830455.5936848\n",
            "Validation Loss: 360636316.4850752\n",
            "Training Loss: 262635754.53776106\n",
            "Validation Loss: 360340360.2062373\n",
            "Training Loss: 262442147.80044284\n",
            "Validation Loss: 360045866.06646025\n",
            "Training Loss: 262249627.9913038\n",
            "Validation Loss: 359752824.8253458\n",
            "Training Loss: 262058187.79205674\n",
            "Validation Loss: 359461227.32856005\n",
            "Training Loss: 261867819.9554988\n",
            "Validation Loss: 359171064.50663894\n",
            "Training Loss: 261678517.30451292\n",
            "Validation Loss: 358882327.37381554\n",
            "Training Loss: 261490272.73102853\n",
            "Validation Loss: 358595007.026874\n",
            "Training Loss: 261303079.19505978\n",
            "Validation Loss: 358309094.6440115\n",
            "Training Loss: 261116929.7237184\n",
            "Validation Loss: 358024581.4837276\n",
            "Training Loss: 260931817.41026863\n",
            "Validation Loss: 357741458.8837326\n",
            "Training Loss: 260747735.4131901\n",
            "Validation Loss: 357459718.25987154\n",
            "Training Loss: 260564676.95524967\n",
            "Validation Loss: 357179351.1050766\n",
            "Training Loss: 260382635.32261002\n",
            "Validation Loss: 356900348.9883159\n",
            "Training Loss: 260201603.86393505\n",
            "Validation Loss: 356622703.5535861\n",
            "Training Loss: 260021575.9895188\n",
            "Validation Loss: 356346406.51891124\n",
            "Training Loss: 259842545.17042792\n",
            "Validation Loss: 356071449.6753457\n",
            "Training Loss: 259664504.93766713\n",
            "Validation Loss: 355797824.8860222\n",
            "Training Loss: 259487448.88134283\n",
            "Validation Loss: 355525524.08519006\n",
            "Training Loss: 259311370.6498611\n",
            "Validation Loss: 355254539.27728623\n",
            "Training Loss: 259136263.94911784\n",
            "Validation Loss: 354984862.53601056\n",
            "Training Loss: 258962122.54173103\n",
            "Validation Loss: 354716486.0034309\n",
            "Training Loss: 258788940.2462579\n",
            "Validation Loss: 354449401.88908476\n",
            "Training Loss: 258616710.93643993\n",
            "Validation Loss: 354183602.4691148\n",
            "Training Loss: 258445428.54047093\n",
            "Validation Loss: 353919080.085409\n",
            "Training Loss: 258275087.04024827\n",
            "Validation Loss: 353655827.14475113\n",
            "Training Loss: 258105680.47066978\n",
            "Validation Loss: 353393836.1179962\n",
            "Training Loss: 257937202.91892228\n",
            "Validation Loss: 353133099.53926\n",
            "Training Loss: 257769648.52379075\n",
            "Validation Loss: 352873610.00510293\n",
            "Training Loss: 257603011.47498158\n",
            "Validation Loss: 352615360.1737549\n",
            "Training Loss: 257437286.01243752\n",
            "Validation Loss: 352358342.76433027\n",
            "Training Loss: 257272466.42569426\n",
            "Validation Loss: 352102550.55607617\n",
            "Training Loss: 257108547.0532342\n",
            "Validation Loss: 351847976.38760865\n",
            "Training Loss: 256945522.2818427\n",
            "Validation Loss: 351594613.15618634\n",
            "Training Loss: 256783386.54599208\n",
            "Validation Loss: 351342453.8169801\n",
            "Training Loss: 256622134.3272144\n",
            "Validation Loss: 351091491.3823625\n",
            "Training Loss: 256461760.15351897\n",
            "Validation Loss: 350841718.9212007\n",
            "Training Loss: 256302258.59877503\n",
            "Validation Loss: 350593129.55817574\n",
            "Training Loss: 256143624.2821473\n",
            "Validation Loss: 350345716.47309226\n",
            "Training Loss: 255985851.86752054\n",
            "Validation Loss: 350099472.9002192\n",
            "Training Loss: 255828936.06291997\n",
            "Validation Loss: 349854392.1276333\n",
            "Training Loss: 255672871.61997858\n",
            "Validation Loss: 349610467.4965643\n",
            "Training Loss: 255517653.33337894\n",
            "Validation Loss: 349367692.40076953\n",
            "Training Loss: 255363276.0403182\n",
            "Validation Loss: 349126060.28590035\n",
            "Training Loss: 255209734.61999443\n",
            "Validation Loss: 348885564.6488924\n",
            "Training Loss: 255057023.99306887\n",
            "Validation Loss: 348646199.0373578\n",
            "Training Loss: 254905139.12117112\n",
            "Validation Loss: 348407957.0489885\n",
            "Training Loss: 254754075.00639617\n",
            "Validation Loss: 348170832.3309755\n",
            "Training Loss: 254603826.69080532\n",
            "Validation Loss: 347934818.5794201\n",
            "Training Loss: 254454389.2559511\n",
            "Validation Loss: 347699909.53877974\n",
            "Training Loss: 254305757.8223919\n",
            "Validation Loss: 347466099.0013051\n",
            "Training Loss: 254157927.54923692\n",
            "Validation Loss: 347233380.80648506\n",
            "Training Loss: 254010893.63366762\n",
            "Validation Loss: 347001748.84050876\n",
            "Training Loss: 253864651.31050062\n",
            "Validation Loss: 346771197.03573793\n",
            "Training Loss: 253719195.85173047\n",
            "Validation Loss: 346541719.3701744\n",
            "Training Loss: 253574522.56610078\n",
            "Validation Loss: 346313309.8669533\n",
            "Training Loss: 253430626.79866594\n",
            "Validation Loss: 346085962.5938261\n",
            "Training Loss: 253287503.9303675\n",
            "Validation Loss: 345859671.66266555\n",
            "Training Loss: 253145149.37762174\n",
            "Validation Loss: 345634431.22897416\n",
            "Training Loss: 253003558.5919049\n",
            "Validation Loss: 345410235.49139744\n",
            "Training Loss: 252862727.05934867\n",
            "Validation Loss: 345187078.6912483\n",
            "Training Loss: 252722650.30034882\n",
            "Validation Loss: 344964955.11203605\n",
            "Training Loss: 252583323.8691615\n",
            "Validation Loss: 344743859.07900554\n",
            "Training Loss: 252444743.35353312\n",
            "Validation Loss: 344523784.95868015\n",
            "Training Loss: 252306904.37431332\n",
            "Validation Loss: 344304727.1584119\n",
            "Training Loss: 252169802.58508238\n",
            "Validation Loss: 344086680.1259493\n",
            "Training Loss: 252033433.67178074\n",
            "Validation Loss: 343869638.3489889\n",
            "Training Loss: 251897793.35236672\n",
            "Validation Loss: 343653596.3547544\n",
            "Training Loss: 251762877.3764341\n",
            "Validation Loss: 343438548.7095779\n",
            "Training Loss: 251628681.52487975\n",
            "Validation Loss: 343224490.0184745\n",
            "Training Loss: 251495201.60956073\n",
            "Validation Loss: 343011414.9247445\n",
            "Training Loss: 251362433.4729417\n",
            "Validation Loss: 342799318.1095619\n",
            "Training Loss: 251230372.98777744\n",
            "Validation Loss: 342588194.291582\n",
            "Training Loss: 251099016.05677077\n",
            "Validation Loss: 342378038.22654945\n",
            "Training Loss: 250968358.6122612\n",
            "Validation Loss: 342168844.70690984\n",
            "Training Loss: 250838396.61589593\n",
            "Validation Loss: 341960608.56143445\n",
            "Training Loss: 250709126.05832368\n",
            "Validation Loss: 341753324.6548442\n",
            "Training Loss: 250580542.95888734\n",
            "Validation Loss: 341546987.88744\n",
            "Training Loss: 250452643.36530903\n",
            "Validation Loss: 341341593.1947423\n",
            "Training Loss: 250325423.3534063\n",
            "Validation Loss: 341137135.54713076\n",
            "Training Loss: 250198879.02678505\n",
            "Validation Loss: 340933609.9494939\n",
            "Training Loss: 250073006.5165578\n",
            "Validation Loss: 340731011.44088066\n",
            "Training Loss: 249947801.98105282\n",
            "Validation Loss: 340529335.09415466\n",
            "Training Loss: 249823261.60552946\n",
            "Validation Loss: 340328576.0156652\n",
            "Training Loss: 249699381.6019184\n",
            "Validation Loss: 340128729.34490454\n",
            "Training Loss: 249576158.2085256\n",
            "Validation Loss: 339929790.2541868\n",
            "Training Loss: 249453587.68977296\n",
            "Validation Loss: 339731753.94832265\n",
            "Training Loss: 249331666.33594173\n",
            "Validation Loss: 339534615.66430104\n",
            "Training Loss: 249210390.4629004\n",
            "Validation Loss: 339338370.6709737\n",
            "Training Loss: 249089756.41184828\n",
            "Validation Loss: 339143014.2687477\n",
            "Training Loss: 248969760.5490701\n",
            "Validation Loss: 338948541.7892798\n",
            "Training Loss: 248850399.2656801\n",
            "Validation Loss: 338754948.595173\n",
            "Training Loss: 248731668.9773736\n",
            "Validation Loss: 338562230.07968134\n",
            "Training Loss: 248613566.12419096\n",
            "Validation Loss: 338370381.6664195\n",
            "Training Loss: 248496087.17027932\n",
            "Validation Loss: 338179398.8090722\n",
            "Training Loss: 248379228.60364744\n",
            "Validation Loss: 337989276.99110734\n",
            "Training Loss: 248262986.93594348\n",
            "Validation Loss: 337800011.72549844\n",
            "Training Loss: 248147358.70222402\n",
            "Validation Loss: 337611598.55445033\n",
            "Training Loss: 248032340.46072918\n",
            "Validation Loss: 337424033.0491201\n",
            "Training Loss: 247917928.7926551\n",
            "Validation Loss: 337237310.8093488\n",
            "Training Loss: 247804120.30193964\n",
            "Validation Loss: 337051427.4634017\n",
            "Training Loss: 247690911.61504766\n",
            "Validation Loss: 336866378.667699\n",
            "Training Loss: 247578299.3807574\n",
            "Validation Loss: 336682160.10656387\n",
            "Training Loss: 247466280.26994422\n",
            "Validation Loss: 336498767.4919621\n",
            "Training Loss: 247354850.9753857\n",
            "Validation Loss: 336316196.5632527\n",
            "Training Loss: 247244008.21154827\n",
            "Validation Loss: 336134443.08694106\n",
            "Training Loss: 247133748.71438617\n",
            "Validation Loss: 335953502.85643137\n",
            "Training Loss: 247024069.2411535\n",
            "Validation Loss: 335773371.6917886\n",
            "Training Loss: 246914966.57019132\n",
            "Validation Loss: 335594045.43949455\n",
            "Training Loss: 246806437.50075248\n",
            "Validation Loss: 335415519.97221833\n",
            "Training Loss: 246698478.85279652\n",
            "Validation Loss: 335237791.18857956\n",
            "Training Loss: 246591087.46681416\n",
            "Validation Loss: 335060855.0129242\n",
            "Training Loss: 246484260.20363367\n",
            "Validation Loss: 334884707.3950914\n",
            "Training Loss: 246377993.94424665\n",
            "Validation Loss: 334709344.31019884\n",
            "Training Loss: 246272285.5896126\n",
            "Validation Loss: 334534761.75841373\n",
            "Training Loss: 246167132.06050053\n",
            "Validation Loss: 334360955.7647401\n",
            "Training Loss: 246062530.29730248\n",
            "Validation Loss: 334187922.3788022\n",
            "Training Loss: 245958477.25985953\n",
            "Validation Loss: 334015657.67463833\n",
            "Training Loss: 245854969.9272969\n",
            "Validation Loss: 333844157.7504817\n",
            "Training Loss: 245752005.29784754\n",
            "Validation Loss: 333673418.7285593\n",
            "Training Loss: 245649580.38870093\n",
            "Validation Loss: 333503436.7548871\n",
            "Training Loss: 245547692.2358193\n",
            "Validation Loss: 333334207.9990722\n",
            "Training Loss: 245446337.89379275\n",
            "Validation Loss: 333165728.6541036\n",
            "Training Loss: 245345514.43566653\n",
            "Validation Loss: 332997994.9361694\n",
            "Training Loss: 245245218.952795\n",
            "Validation Loss: 332831003.08444965\n",
            "Training Loss: 245145448.55467635\n",
            "Validation Loss: 332664749.360935\n",
            "Training Loss: 245046200.36880416\n",
            "Validation Loss: 332499230.0502322\n",
            "Training Loss: 244947471.54051468\n",
            "Validation Loss: 332334441.45937544\n",
            "Training Loss: 244849259.23283505\n",
            "Validation Loss: 332170379.9176484\n",
            "Training Loss: 244751560.62634453\n",
            "Validation Loss: 332007041.7763943\n",
            "Training Loss: 244654372.91900712\n",
            "Validation Loss: 331844423.40883887\n",
            "Training Loss: 244557693.3260542\n",
            "Validation Loss: 331682521.2099162\n",
            "Training Loss: 244461519.07982716\n",
            "Validation Loss: 331521331.59608626\n",
            "Training Loss: 244365847.42963582\n",
            "Validation Loss: 331360851.0051616\n",
            "Training Loss: 244270675.6416207\n",
            "Validation Loss: 331201075.8961456\n",
            "Training Loss: 244176000.99862635\n",
            "Validation Loss: 331042002.74904704\n",
            "Training Loss: 244081820.80005744\n",
            "Validation Loss: 330883628.0647243\n",
            "Training Loss: 243988132.36173517\n",
            "Validation Loss: 330725948.3647155\n",
            "Training Loss: 243894933.01578647\n",
            "Validation Loss: 330568960.1910742\n",
            "Training Loss: 243802220.11049777\n",
            "Validation Loss: 330412660.10620916\n",
            "Training Loss: 243709991.01019007\n",
            "Validation Loss: 330257044.69272447\n",
            "Training Loss: 243618243.0950965\n",
            "Validation Loss: 330102110.5532581\n",
            "Training Loss: 243526973.76122823\n",
            "Validation Loss: 329947854.3103324\n",
            "Training Loss: 243436180.42025578\n",
            "Validation Loss: 329794272.6061882\n",
            "Training Loss: 243345860.4993871\n",
            "Validation Loss: 329641362.1026489\n",
            "Training Loss: 243256011.44123918\n",
            "Validation Loss: 329489119.48094976\n",
            "Training Loss: 243166630.7037261\n",
            "Validation Loss: 329337541.4416056\n",
            "Training Loss: 243077715.7599366\n",
            "Validation Loss: 329186624.7042545\n",
            "Training Loss: 242989264.09801742\n",
            "Validation Loss: 329036366.0075105\n",
            "Training Loss: 242901273.22105452\n",
            "Validation Loss: 328886762.10882765\n",
            "Training Loss: 242813740.64696726\n",
            "Validation Loss: 328737809.78434855\n",
            "Training Loss: 242726663.90838328\n",
            "Validation Loss: 328589505.828768\n",
            "Training Loss: 242640040.55253834\n",
            "Validation Loss: 328441847.055191\n",
            "Training Loss: 242553868.1411588\n",
            "Validation Loss: 328294830.29499954\n",
            "Training Loss: 242468144.2503477\n",
            "Validation Loss: 328148452.397707\n",
            "Training Loss: 242382866.47049415\n",
            "Validation Loss: 328002710.2308359\n",
            "Training Loss: 242298032.406148\n",
            "Validation Loss: 327857600.679769\n",
            "Training Loss: 242213639.6759201\n",
            "Validation Loss: 327713120.6476313\n",
            "Training Loss: 242129685.91238463\n",
            "Validation Loss: 327569267.05515414\n",
            "Training Loss: 242046168.7619653\n",
            "Validation Loss: 327426036.84053963\n",
            "Training Loss: 241963085.8848408\n",
            "Validation Loss: 327283426.95934546\n",
            "Training Loss: 241880434.95483586\n",
            "Validation Loss: 327141434.3843464\n",
            "Training Loss: 241798213.65933266\n",
            "Validation Loss: 327000056.10541445\n",
            "Training Loss: 241716419.69916755\n",
            "Validation Loss: 326859289.12939894\n",
            "Training Loss: 241635050.7885212\n",
            "Validation Loss: 326719130.4799952\n",
            "Training Loss: 241554104.65484175\n",
            "Validation Loss: 326579577.1976289\n",
            "Training Loss: 241473579.03873453\n",
            "Validation Loss: 326440626.3393372\n",
            "Training Loss: 241393471.69387043\n",
            "Validation Loss: 326302274.9786463\n",
            "Training Loss: 241313780.38690597\n",
            "Validation Loss: 326164520.2054542\n",
            "Training Loss: 241234502.89735857\n",
            "Validation Loss: 326027359.12591684\n",
            "Training Loss: 241155637.01755568\n",
            "Validation Loss: 325890788.8623337\n",
            "Training Loss: 241077180.5525052\n",
            "Validation Loss: 325754806.55302656\n",
            "Training Loss: 240999131.31984106\n",
            "Validation Loss: 325619409.35223895\n",
            "Training Loss: 240921487.14970553\n",
            "Validation Loss: 325484594.43001115\n",
            "Training Loss: 240844245.88467345\n",
            "Validation Loss: 325350358.9720794\n",
            "Training Loss: 240767405.37966794\n",
            "Validation Loss: 325216700.17976415\n",
            "Training Loss: 240690963.50186908\n",
            "Validation Loss: 325083615.2698581\n",
            "Training Loss: 240614918.13063297\n",
            "Validation Loss: 324951101.47452396\n",
            "Training Loss: 240539267.1574044\n",
            "Validation Loss: 324819156.0411843\n",
            "Training Loss: 240464008.48563078\n",
            "Validation Loss: 324687776.2324171\n",
            "Training Loss: 240389140.0306825\n",
            "Validation Loss: 324556959.325855\n",
            "Training Loss: 240314659.71977216\n",
            "Validation Loss: 324426702.61408085\n",
            "Training Loss: 240240565.49188143\n",
            "Validation Loss: 324297003.40451795\n",
            "Training Loss: 240166855.29764998\n",
            "Validation Loss: 324167859.0193455\n",
            "Training Loss: 240093527.0993371\n",
            "Validation Loss: 324039266.7953777\n",
            "Training Loss: 240020578.8707164\n",
            "Validation Loss: 323911224.0839862\n",
            "Training Loss: 239948008.5970021\n",
            "Validation Loss: 323783728.2509819\n",
            "Training Loss: 239875814.2747824\n",
            "Validation Loss: 323656776.6765314\n",
            "Training Loss: 239803993.91192847\n",
            "Validation Loss: 323530366.75505674\n",
            "Training Loss: 239732545.52752772\n",
            "Validation Loss: 323404495.89513797\n",
            "Training Loss: 239661467.15180868\n",
            "Validation Loss: 323279161.5194179\n",
            "Training Loss: 239590756.8260632\n",
            "Validation Loss: 323154361.0645164\n",
            "Training Loss: 239520412.60257262\n",
            "Validation Loss: 323030091.980925\n",
            "Training Loss: 239450432.54454112\n",
            "Validation Loss: 322906351.73292685\n",
            "Training Loss: 239380814.72601876\n",
            "Validation Loss: 322783137.7984998\n",
            "Training Loss: 239311557.23182684\n",
            "Validation Loss: 322660447.6692227\n",
            "Training Loss: 239242658.15749314\n",
            "Validation Loss: 322538278.85019475\n",
            "Training Loss: 239174115.60917896\n",
            "Validation Loss: 322416628.8599413\n",
            "Training Loss: 239105927.70361394\n",
            "Validation Loss: 322295495.2303264\n",
            "Training Loss: 239038092.56801346\n",
            "Validation Loss: 322174875.50646776\n",
            "Training Loss: 238970608.34003162\n",
            "Validation Loss: 322054767.2466479\n",
            "Training Loss: 238903473.16767636\n",
            "Validation Loss: 321935168.0222304\n",
            "Training Loss: 238836685.20924953\n",
            "Validation Loss: 321816075.4175772\n",
            "Training Loss: 238770242.63327447\n",
            "Validation Loss: 321697487.02995956\n",
            "Training Loss: 238704143.61844411\n",
            "Validation Loss: 321579400.46948075\n",
            "Training Loss: 238638386.35354015\n",
            "Validation Loss: 321461813.3589882\n",
            "Training Loss: 238572969.03737742\n",
            "Validation Loss: 321344723.3339955\n",
            "Training Loss: 238507889.8787326\n",
            "Validation Loss: 321228128.04260296\n",
            "Training Loss: 238443147.0962932\n",
            "Validation Loss: 321112025.14540964\n",
            "Training Loss: 238378738.9185831\n",
            "Validation Loss: 320996412.3154424\n",
            "Training Loss: 238314663.58389905\n",
            "Validation Loss: 320881287.23807335\n",
            "Training Loss: 238250919.34026146\n",
            "Validation Loss: 320766647.6109395\n",
            "Training Loss: 238187504.4453412\n",
            "Validation Loss: 320652491.1438709\n",
            "Training Loss: 238124417.16640157\n",
            "Validation Loss: 320538815.55880725\n",
            "Training Loss: 238061655.7802463\n",
            "Validation Loss: 320425618.58972746\n",
            "Training Loss: 237999218.57313964\n",
            "Validation Loss: 320312897.9825708\n",
            "Training Loss: 237937103.8407751\n",
            "Validation Loss: 320200651.49516016\n",
            "Training Loss: 237875309.88819072\n",
            "Validation Loss: 320088876.897135\n",
            "Training Loss: 237813835.02973035\n",
            "Validation Loss: 319977571.96986955\n",
            "Training Loss: 237752677.58896884\n",
            "Validation Loss: 319866734.50640386\n",
            "Training Loss: 237691835.89866915\n",
            "Validation Loss: 319756362.31137234\n",
            "Training Loss: 237631308.3007209\n",
            "Validation Loss: 319646453.20092857\n",
            "Training Loss: 237571093.14607695\n",
            "Validation Loss: 319537005.0026764\n",
            "Training Loss: 237511188.79470944\n",
            "Validation Loss: 319428015.5555991\n",
            "Training Loss: 237451593.61554587\n",
            "Validation Loss: 319319482.7099883\n",
            "Training Loss: 237392305.98641387\n",
            "Validation Loss: 319211404.3273766\n",
            "Training Loss: 237333324.29399878\n",
            "Validation Loss: 319103778.28046554\n",
            "Training Loss: 237274646.93377203\n",
            "Validation Loss: 318996602.45306003\n",
            "Training Loss: 237216272.3099473\n",
            "Validation Loss: 318889874.74000067\n",
            "Training Loss: 237158198.83542415\n",
            "Validation Loss: 318783593.0470913\n",
            "Training Loss: 237100424.93174464\n",
            "Validation Loss: 318677755.29103965\n",
            "Training Loss: 237042949.0290289\n",
            "Validation Loss: 318572359.39938676\n",
            "Training Loss: 236985769.5659225\n",
            "Validation Loss: 318467403.3104419\n",
            "Training Loss: 236928884.9895571\n",
            "Validation Loss: 318362884.97322065\n",
            "Training Loss: 236872293.7554889\n",
            "Validation Loss: 318258802.34737194\n",
            "Training Loss: 236815994.32765296\n",
            "Validation Loss: 318155153.40312374\n",
            "Training Loss: 236759985.1783121\n",
            "Validation Loss: 318051936.1212163\n",
            "Training Loss: 236704264.78800237\n",
            "Validation Loss: 317949148.4928331\n",
            "Training Loss: 236648831.64549035\n",
            "Validation Loss: 317846788.51954746\n",
            "Training Loss: 236593684.2477216\n",
            "Validation Loss: 317744854.2132543\n",
            "Training Loss: 236538821.09976828\n",
            "Validation Loss: 317643343.59610915\n",
            "Training Loss: 236484240.71478337\n",
            "Validation Loss: 317542254.70047116\n",
            "Training Loss: 236429941.61395782\n",
            "Validation Loss: 317441585.5688363\n",
            "Training Loss: 236375922.32646418\n",
            "Validation Loss: 317341334.25378156\n",
            "Training Loss: 236322181.3894149\n",
            "Validation Loss: 317241498.8179031\n",
            "Training Loss: 236268717.3478098\n",
            "Validation Loss: 317142077.33376104\n",
            "Training Loss: 236215528.75449708\n",
            "Validation Loss: 317043067.8838096\n",
            "Training Loss: 236162614.170123\n",
            "Validation Loss: 316944468.5603553\n",
            "Training Loss: 236109972.1630867\n",
            "Validation Loss: 316846277.46548235\n",
            "Training Loss: 236057601.30948958\n",
            "Validation Loss: 316748492.71100926\n",
            "Training Loss: 236005500.19310004\n",
            "Validation Loss: 316651112.4184183\n",
            "Training Loss: 235953667.40529916\n",
            "Validation Loss: 316554134.7188129\n",
            "Training Loss: 235902101.54504845\n",
            "Validation Loss: 316457557.7528473\n",
            "Training Loss: 235850801.21882713\n",
            "Validation Loss: 316361379.67068464\n",
            "Training Loss: 235799765.0406\n",
            "Validation Loss: 316265598.6319279\n",
            "Training Loss: 235748991.63177946\n",
            "Validation Loss: 316170212.8055763\n",
            "Training Loss: 235698479.62116948\n",
            "Validation Loss: 316075220.36996317\n",
            "Training Loss: 235648227.6449279\n",
            "Validation Loss: 315980619.5127055\n",
            "Training Loss: 235598234.34652963\n",
            "Validation Loss: 315886408.43064946\n",
            "Training Loss: 235548498.37671\n",
            "Validation Loss: 315792585.3298172\n",
            "Training Loss: 235499018.39343956\n",
            "Validation Loss: 315699148.4253503\n",
            "Training Loss: 235449793.0618697\n",
            "Validation Loss: 315606095.94146264\n",
            "Training Loss: 235400821.05430177\n",
            "Validation Loss: 315513426.1113856\n",
            "Training Loss: 235352101.05012944\n",
            "Validation Loss: 315421137.1773152\n",
            "Training Loss: 235303631.73581874\n",
            "Validation Loss: 315329227.3903603\n",
            "Training Loss: 235255411.80485347\n",
            "Validation Loss: 315237695.01049525\n",
            "Training Loss: 235207439.95769414\n",
            "Validation Loss: 315146538.3065064\n",
            "Training Loss: 235159714.90175134\n",
            "Validation Loss: 315055755.55593747\n",
            "Training Loss: 235112235.35132974\n",
            "Validation Loss: 314965345.0450503\n",
            "Training Loss: 235065000.02759746\n",
            "Validation Loss: 314875305.0687658\n",
            "Training Loss: 235018007.6585499\n",
            "Validation Loss: 314785633.93061256\n",
            "Training Loss: 234971256.97895855\n",
            "Validation Loss: 314696329.94269073\n",
            "Training Loss: 234924746.7303469\n",
            "Validation Loss: 314607391.42561096\n",
            "Training Loss: 234878475.66094315\n",
            "Validation Loss: 314518816.7084498\n",
            "Training Loss: 234832442.5256446\n",
            "Validation Loss: 314430604.12870556\n",
            "Training Loss: 234786646.08597684\n",
            "Validation Loss: 314342752.03224474\n",
            "Training Loss: 234741085.1100611\n",
            "Validation Loss: 314255258.7732582\n",
            "Training Loss: 234695758.3725746\n",
            "Validation Loss: 314168122.71421325\n",
            "Training Loss: 234650664.6547133\n",
            "Validation Loss: 314081342.22580606\n",
            "Training Loss: 234605802.74415404\n",
            "Validation Loss: 313994915.68691844\n",
            "Training Loss: 234561171.43502167\n",
            "Validation Loss: 313908841.4845696\n",
            "Training Loss: 234516769.52784663\n",
            "Validation Loss: 313823118.0138669\n",
            "Training Loss: 234472595.82953632\n",
            "Validation Loss: 313737743.67796946\n",
            "Training Loss: 234428649.1533309\n",
            "Validation Loss: 313652716.8880333\n",
            "Training Loss: 234384928.3187762\n",
            "Validation Loss: 313568036.06317186\n",
            "Training Loss: 234341432.1516786\n",
            "Validation Loss: 313483699.6304117\n",
            "Training Loss: 234298159.4840829\n",
            "Validation Loss: 313399706.02464604\n",
            "Training Loss: 234255109.1542231\n",
            "Validation Loss: 313316053.68859357\n",
            "Training Loss: 234212280.00649947\n",
            "Validation Loss: 313232741.07275116\n",
            "Training Loss: 234169670.8914349\n",
            "Validation Loss: 313149766.6353532\n",
            "Training Loss: 234127280.66565084\n",
            "Validation Loss: 313067128.84232885\n",
            "Training Loss: 234085108.1918188\n",
            "Validation Loss: 312984826.1672591\n",
            "Training Loss: 234043152.33864233\n",
            "Validation Loss: 312902857.09133124\n",
            "Training Loss: 234001411.98081475\n",
            "Validation Loss: 312821220.1033004\n",
            "Training Loss: 233959885.9989816\n",
            "Validation Loss: 312739913.6994464\n",
            "Training Loss: 233918573.27972075\n",
            "Validation Loss: 312658936.383534\n",
            "Training Loss: 233877472.715497\n",
            "Validation Loss: 312578286.666766\n",
            "Training Loss: 233836583.2046392\n",
            "Validation Loss: 312497963.06775\n",
            "Training Loss: 233795903.6512954\n",
            "Validation Loss: 312417964.1124485\n",
            "Training Loss: 233755432.96541318\n",
            "Validation Loss: 312338288.3341479\n",
            "Training Loss: 233715170.06270242\n",
            "Validation Loss: 312258934.2734148\n",
            "Training Loss: 233675113.86459807\n",
            "Validation Loss: 312179900.4780473\n",
            "Training Loss: 233635263.29823905\n",
            "Validation Loss: 312101185.5030525\n",
            "Training Loss: 233595617.29642949\n",
            "Validation Loss: 312022787.91058946\n",
            "Training Loss: 233556174.79760736\n",
            "Validation Loss: 311944706.2699436\n",
            "Training Loss: 233516934.74582008\n",
            "Validation Loss: 311866939.15747875\n",
            "Training Loss: 233477896.0906819\n",
            "Validation Loss: 311789485.1566029\n",
            "Training Loss: 233439057.78735432\n",
            "Validation Loss: 311712342.85772765\n",
            "Training Loss: 233400418.7965117\n",
            "Validation Loss: 311635510.85823333\n",
            "Training Loss: 233361978.08431306\n",
            "Validation Loss: 311558987.76242554\n",
            "Training Loss: 233323734.62235832\n",
            "Validation Loss: 311482772.18150175\n",
            "Training Loss: 233285687.387682\n",
            "Validation Loss: 311406862.73351324\n",
            "Training Loss: 233247835.36270458\n",
            "Validation Loss: 311331258.0433256\n",
            "Training Loss: 233210177.5352111\n",
            "Validation Loss: 311255956.7425854\n",
            "Training Loss: 233172712.89831957\n",
            "Validation Loss: 311180957.46967924\n",
            "Training Loss: 233135440.45045087\n",
            "Validation Loss: 311106258.86969966\n",
            "Training Loss: 233098359.19530067\n",
            "Validation Loss: 311031859.59440833\n",
            "Training Loss: 233061468.14181492\n",
            "Validation Loss: 310957758.3022001\n",
            "Training Loss: 233024766.30415288\n",
            "Validation Loss: 310883953.65806633\n",
            "Training Loss: 232988252.70166287\n",
            "Validation Loss: 310810444.33356285\n",
            "Training Loss: 232951926.35885748\n",
            "Validation Loss: 310737229.00676763\n",
            "Training Loss: 232915786.30537727\n",
            "Validation Loss: 310664306.3622507\n",
            "Training Loss: 232879831.57597125\n",
            "Validation Loss: 310591675.0910399\n",
            "Training Loss: 232844061.21045902\n",
            "Validation Loss: 310519333.8905818\n",
            "Training Loss: 232808474.253719\n",
            "Validation Loss: 310447281.4647106\n",
            "Training Loss: 232773069.75564364\n",
            "Validation Loss: 310375516.52361465\n",
            "Training Loss: 232737846.77111906\n",
            "Validation Loss: 310304037.7837959\n",
            "Training Loss: 232702804.36000398\n",
            "Validation Loss: 310232843.9680455\n",
            "Training Loss: 232667941.58709323\n",
            "Validation Loss: 310161933.8054013\n",
            "Training Loss: 232633257.5220922\n",
            "Validation Loss: 310091306.03112054\n",
            "Training Loss: 232598751.23960096\n",
            "Validation Loss: 310020959.38664347\n",
            "Training Loss: 232564421.8190733\n",
            "Validation Loss: 309950892.619561\n",
            "Training Loss: 232530268.34479755\n",
            "Validation Loss: 309881104.4835832\n",
            "Training Loss: 232496289.90586895\n",
            "Validation Loss: 309811593.73850113\n",
            "Training Loss: 232462485.59616837\n",
            "Validation Loss: 309742359.1501647\n",
            "Training Loss: 232428854.5143276\n",
            "Validation Loss: 309673399.4904394\n",
            "Training Loss: 232395395.76371264\n",
            "Validation Loss: 309604713.53717905\n",
            "Training Loss: 232362108.45238703\n",
            "Validation Loss: 309536300.074197\n",
            "Training Loss: 232328991.6931008\n",
            "Validation Loss: 309468157.8912299\n",
            "Training Loss: 232296044.60325107\n",
            "Validation Loss: 309400285.783906\n",
            "Training Loss: 232263266.30486974\n",
            "Validation Loss: 309332682.5537179\n",
            "Training Loss: 232230655.92458215\n",
            "Validation Loss: 309265347.0079874\n",
            "Training Loss: 232198212.59360495\n",
            "Validation Loss: 309198277.95983696\n",
            "Training Loss: 232165935.44769943\n",
            "Validation Loss: 309131474.22815526\n",
            "Training Loss: 232133823.62716213\n",
            "Validation Loss: 309064934.6375751\n",
            "Training Loss: 232101876.27678627\n",
            "Validation Loss: 308998658.01843256\n",
            "Training Loss: 232070092.54585648\n",
            "Validation Loss: 308932643.2067434\n",
            "Training Loss: 232038471.58810437\n",
            "Validation Loss: 308866889.04417163\n",
            "Training Loss: 232007012.56170133\n",
            "Validation Loss: 308801394.3779978\n",
            "Training Loss: 231975714.62922335\n",
            "Validation Loss: 308736158.06109303\n",
            "Training Loss: 231944576.95763156\n",
            "Validation Loss: 308671178.95188564\n",
            "Training Loss: 231913598.7182495\n",
            "Validation Loss: 308606455.9143344\n",
            "Training Loss: 231882779.0867401\n",
            "Validation Loss: 308541987.81789774\n",
            "Training Loss: 231852117.2430818\n",
            "Validation Loss: 308477773.5375056\n",
            "Training Loss: 231821612.37153876\n",
            "Validation Loss: 308413811.9535329\n",
            "Training Loss: 231791263.66064587\n",
            "Validation Loss: 308350101.9517643\n",
            "Training Loss: 231761070.3031917\n",
            "Validation Loss: 308286642.4233741\n",
            "Training Loss: 231731031.4961803\n",
            "Validation Loss: 308223432.26489156\n",
            "Training Loss: 231701146.44081107\n",
            "Validation Loss: 308160470.37817776\n",
            "Training Loss: 231671414.34247524\n",
            "Validation Loss: 308097755.67039007\n",
            "Training Loss: 231641834.41070685\n",
            "Validation Loss: 308035287.05396414\n",
            "Training Loss: 231612405.85917816\n",
            "Validation Loss: 307973063.4465803\n",
            "Training Loss: 231583127.9056731\n",
            "Validation Loss: 307911083.77113533\n",
            "Training Loss: 231553999.77206412\n",
            "Validation Loss: 307849346.95571965\n",
            "Training Loss: 231525020.6842902\n",
            "Validation Loss: 307787851.93358535\n",
            "Training Loss: 231496189.87233707\n",
            "Validation Loss: 307726597.6431234\n",
            "Training Loss: 231467506.57021147\n",
            "Validation Loss: 307665583.0278327\n",
            "Training Loss: 231438970.01592514\n",
            "Validation Loss: 307604807.0362964\n",
            "Training Loss: 231410579.45147175\n",
            "Validation Loss: 307544268.62215567\n",
            "Training Loss: 231382334.12280208\n",
            "Validation Loss: 307483966.74408096\n",
            "Training Loss: 231354233.27980652\n",
            "Validation Loss: 307423900.365747\n",
            "Training Loss: 231326276.1762909\n",
            "Validation Loss: 307364068.45580816\n",
            "Training Loss: 231298462.06996644\n",
            "Validation Loss: 307304469.98786926\n",
            "Training Loss: 231270790.22241038\n",
            "Validation Loss: 307245103.940462\n",
            "Training Loss: 231243259.89905724\n",
            "Validation Loss: 307185969.2970195\n",
            "Training Loss: 231215870.36917737\n",
            "Validation Loss: 307127065.0458511\n",
            "Training Loss: 231188620.90586182\n",
            "Validation Loss: 307068390.180115\n",
            "Training Loss: 231161510.78598422\n",
            "Validation Loss: 307009943.69779456\n",
            "Training Loss: 231134539.2901991\n",
            "Validation Loss: 306951724.6016746\n",
            "Training Loss: 231107705.70291495\n",
            "Validation Loss: 306893731.89931124\n",
            "Training Loss: 231081009.3122712\n",
            "Validation Loss: 306835964.60301685\n",
            "Training Loss: 231054449.4101242\n",
            "Validation Loss: 306778421.7298228\n",
            "Training Loss: 231028025.29201978\n",
            "Validation Loss: 306721102.3014669\n",
            "Training Loss: 231001736.25718343\n",
            "Validation Loss: 306664005.3443603\n",
            "Training Loss: 230975581.60849324\n",
            "Validation Loss: 306607129.889571\n",
            "Training Loss: 230949560.652465\n",
            "Validation Loss: 306550474.9727908\n",
            "Training Loss: 230923672.69922638\n",
            "Validation Loss: 306494039.6343186\n",
            "Training Loss: 230897917.06250817\n",
            "Validation Loss: 306437822.9190378\n",
            "Training Loss: 230872293.05961227\n",
            "Validation Loss: 306381823.8763813\n",
            "Training Loss: 230846800.01140425\n",
            "Validation Loss: 306326041.5603215\n",
            "Training Loss: 230821437.24229026\n",
            "Validation Loss: 306270475.02934235\n",
            "Training Loss: 230796204.08019853\n",
            "Validation Loss: 306215123.3464097\n",
            "Training Loss: 230771099.8565512\n",
            "Validation Loss: 306159985.5789583\n",
            "Training Loss: 230746123.90626687\n",
            "Validation Loss: 306105060.7988612\n",
            "Training Loss: 230721275.56772307\n",
            "Validation Loss: 306050348.0824104\n",
            "Training Loss: 230696554.18274525\n",
            "Validation Loss: 305995846.5102959\n",
            "Training Loss: 230671959.09658822\n",
            "Validation Loss: 305941555.1675766\n",
            "Training Loss: 230647489.65791976\n",
            "Validation Loss: 305887473.1436649\n",
            "Training Loss: 230623145.2187962\n",
            "Validation Loss: 305833599.5323008\n",
            "Training Loss: 230598925.13465247\n",
            "Validation Loss: 305779933.4315296\n",
            "Training Loss: 230574828.76428005\n",
            "Validation Loss: 305726473.9436794\n",
            "Training Loss: 230550855.46980628\n",
            "Validation Loss: 305673220.17534375\n",
            "Training Loss: 230527004.6166862\n",
            "Validation Loss: 305620171.23735315\n",
            "Training Loss: 230503275.5736721\n",
            "Validation Loss: 305567326.2447566\n",
            "Training Loss: 230479667.71280828\n",
            "Validation Loss: 305514684.3168016\n",
            "Training Loss: 230456180.40940407\n",
            "Validation Loss: 305462244.57691014\n",
            "Training Loss: 230432813.04202437\n",
            "Validation Loss: 305410006.1526561\n",
            "Training Loss: 230409564.99246395\n",
            "Validation Loss: 305357968.17575026\n",
            "Training Loss: 230386435.64574298\n",
            "Validation Loss: 305306129.7820111\n",
            "Training Loss: 230363424.39007294\n",
            "Validation Loss: 305254490.11134917\n",
            "Training Loss: 230340530.61685833\n",
            "Validation Loss: 305203048.30774546\n",
            "Training Loss: 230317753.7206611\n",
            "Validation Loss: 305151803.5192279\n",
            "Training Loss: 230295093.0992001\n",
            "Validation Loss: 305100754.8978545\n",
            "Training Loss: 230272548.1533248\n",
            "Validation Loss: 305049901.5996891\n",
            "Training Loss: 230250118.28700444\n",
            "Validation Loss: 304999242.78478587\n",
            "Training Loss: 230227802.9073079\n",
            "Validation Loss: 304948777.6171619\n",
            "Training Loss: 230205601.42438215\n",
            "Validation Loss: 304898505.26478267\n",
            "Training Loss: 230183513.25145254\n",
            "Validation Loss: 304848424.8995427\n",
            "Training Loss: 230161537.80478844\n",
            "Validation Loss: 304798535.6972397\n",
            "Training Loss: 230139674.50369933\n",
            "Validation Loss: 304748836.8375598\n",
            "Training Loss: 230117922.77050987\n",
            "Validation Loss: 304699327.50405777\n",
            "Training Loss: 230096282.0305531\n",
            "Validation Loss: 304650006.88413024\n",
            "Training Loss: 230074751.712147\n",
            "Validation Loss: 304600874.1690086\n",
            "Training Loss: 230053331.2465824\n",
            "Validation Loss: 304551928.5537289\n",
            "Training Loss: 230032020.06810603\n",
            "Validation Loss: 304503169.23711586\n",
            "Training Loss: 230010817.6139095\n",
            "Validation Loss: 304454595.4217666\n",
            "Training Loss: 229989723.3241039\n",
            "Validation Loss: 304406206.31402504\n",
            "Training Loss: 229968736.64171714\n",
            "Validation Loss: 304358001.1239727\n",
            "Training Loss: 229947857.01266408\n",
            "Validation Loss: 304309979.0653964\n",
            "Training Loss: 229927083.88574502\n",
            "Validation Loss: 304262139.35578287\n",
            "Training Loss: 229906416.71261987\n",
            "Validation Loss: 304214481.21629244\n",
            "Training Loss: 229885854.94780424\n",
            "Validation Loss: 304167003.8717428\n",
            "Training Loss: 229865398.0486435\n",
            "Validation Loss: 304119706.5505882\n",
            "Training Loss: 229845045.47530335\n",
            "Validation Loss: 304072588.484903\n",
            "Training Loss: 229824796.69075155\n",
            "Validation Loss: 304025648.9103645\n",
            "Training Loss: 229804651.1607517\n",
            "Validation Loss: 303978887.0662314\n",
            "Training Loss: 229784608.35383496\n",
            "Validation Loss: 303932302.19533044\n",
            "Training Loss: 229764667.74130177\n",
            "Validation Loss: 303885893.5440315\n",
            "Training Loss: 229744828.79718766\n",
            "Validation Loss: 303839660.3622382\n",
            "Training Loss: 229725090.99827296\n",
            "Validation Loss: 303793601.9033627\n",
            "Training Loss: 229705453.82404134\n",
            "Validation Loss: 303747717.42431164\n",
            "Training Loss: 229685916.75668958\n",
            "Validation Loss: 303702006.18546766\n",
            "Training Loss: 229666479.28110147\n",
            "Validation Loss: 303656467.4506739\n",
            "Training Loss: 229647140.8848264\n",
            "Validation Loss: 303611100.4872126\n",
            "Training Loss: 229627901.0580938\n",
            "Validation Loss: 303565904.56579065\n",
            "Training Loss: 229608759.29376054\n",
            "Validation Loss: 303520878.96052307\n",
            "Training Loss: 229589715.08731994\n",
            "Validation Loss: 303476022.94890976\n",
            "Training Loss: 229570767.936891\n",
            "Validation Loss: 303431335.8118303\n",
            "Training Loss: 229551917.34319767\n",
            "Validation Loss: 303386816.83351237\n",
            "Training Loss: 229533162.8095474\n",
            "Validation Loss: 303342465.3015279\n",
            "Training Loss: 229514503.84183127\n",
            "Validation Loss: 303298280.50677043\n",
            "Training Loss: 229495939.94850633\n",
            "Validation Loss: 303254261.743435\n",
            "Training Loss: 229477470.64057586\n",
            "Validation Loss: 303210408.3090075\n",
            "Training Loss: 229459095.43157974\n",
            "Validation Loss: 303166719.50424653\n",
            "Training Loss: 229440813.83759126\n",
            "Validation Loss: 303123194.63316464\n",
            "Training Loss: 229422625.3771806\n",
            "Validation Loss: 303079833.0030168\n",
            "Training Loss: 229404529.57142958\n",
            "Validation Loss: 303036633.9242794\n",
            "Training Loss: 229386525.94389555\n",
            "Validation Loss: 302993596.7106335\n",
            "Training Loss: 229368614.02061135\n",
            "Validation Loss: 302950720.67895555\n",
            "Training Loss: 229350793.33006737\n",
            "Validation Loss: 302908005.14929384\n",
            "Training Loss: 229333063.40319887\n",
            "Validation Loss: 302865449.44485795\n",
            "Training Loss: 229315423.77337813\n",
            "Validation Loss: 302823052.8919977\n",
            "Training Loss: 229297873.97638893\n",
            "Validation Loss: 302780814.82019305\n",
            "Training Loss: 229280413.5504351\n",
            "Validation Loss: 302738734.5620371\n",
            "Training Loss: 229263042.0361055\n",
            "Validation Loss: 302696811.4532153\n",
            "Training Loss: 229245758.97637308\n",
            "Validation Loss: 302655044.8324965\n",
            "Training Loss: 229228563.91658598\n",
            "Validation Loss: 302613434.0417167\n",
            "Training Loss: 229211456.40444154\n",
            "Validation Loss: 302571978.4257566\n",
            "Training Loss: 229194435.9899898\n",
            "Validation Loss: 302530677.3325387\n",
            "Training Loss: 229177502.2256068\n",
            "Validation Loss: 302489530.11300063\n",
            "Training Loss: 229160654.665997\n",
            "Validation Loss: 302448536.12108564\n",
            "Training Loss: 229143892.86816826\n",
            "Validation Loss: 302407694.7137291\n",
            "Training Loss: 229127216.391425\n",
            "Validation Loss: 302367005.2508387\n",
            "Training Loss: 229110624.79735664\n",
            "Validation Loss: 302326467.0952834\n",
            "Training Loss: 229094117.649825\n",
            "Validation Loss: 302286079.61287683\n",
            "Training Loss: 229077694.51495096\n",
            "Validation Loss: 302245842.1723648\n",
            "Training Loss: 229061354.96110997\n",
            "Validation Loss: 302205754.14540774\n",
            "Training Loss: 229045098.5589072\n",
            "Validation Loss: 302165814.9065682\n",
            "Training Loss: 229028924.88117406\n",
            "Validation Loss: 302126023.83329386\n",
            "Training Loss: 229012833.5029615\n",
            "Validation Loss: 302086380.30591\n",
            "Training Loss: 228996824.0015167\n",
            "Validation Loss: 302046883.7075958\n",
            "Training Loss: 228980895.95627782\n",
            "Validation Loss: 302007533.4243768\n",
            "Training Loss: 228965048.9488619\n",
            "Validation Loss: 301968328.84510916\n",
            "Training Loss: 228949282.56305936\n",
            "Validation Loss: 301929269.36146426\n",
            "Training Loss: 228933596.38480586\n",
            "Validation Loss: 301890354.3679153\n",
            "Training Loss: 228917990.00219542\n",
            "Validation Loss: 301851583.26172376\n",
            "Training Loss: 228902463.00544322\n",
            "Validation Loss: 301812955.44292784\n",
            "Training Loss: 228887014.98689562\n",
            "Validation Loss: 301774470.3143235\n",
            "Training Loss: 228871645.5410069\n",
            "Validation Loss: 301736127.28145534\n",
            "Training Loss: 228856354.26433295\n",
            "Validation Loss: 301697925.7526006\n",
            "Training Loss: 228841140.75551695\n",
            "Validation Loss: 301659865.13875604\n",
            "Training Loss: 228826004.61528474\n",
            "Validation Loss: 301621944.8536267\n",
            "Training Loss: 228810945.4464256\n",
            "Validation Loss: 301584164.31360835\n",
            "Training Loss: 228795962.85378894\n",
            "Validation Loss: 301546522.93777627\n",
            "Training Loss: 228781056.44426534\n",
            "Validation Loss: 301509020.1478761\n",
            "Training Loss: 228766225.82679203\n",
            "Validation Loss: 301471655.3682997\n",
            "Training Loss: 228751470.6123176\n",
            "Validation Loss: 301434428.0260868\n",
            "Training Loss: 228736790.41380924\n",
            "Validation Loss: 301397337.550899\n",
            "Training Loss: 228722184.8462467\n",
            "Validation Loss: 301360383.3750124\n",
            "Training Loss: 228707653.52658814\n",
            "Validation Loss: 301323564.933307\n",
            "Training Loss: 228693196.0737869\n",
            "Validation Loss: 301286881.6632472\n",
            "Training Loss: 228678812.1087612\n",
            "Validation Loss: 301250333.0048767\n",
            "Training Loss: 228664501.25439602\n",
            "Validation Loss: 301213918.4007995\n",
            "Training Loss: 228650263.13552704\n",
            "Validation Loss: 301177637.29617083\n",
            "Training Loss: 228636097.37892857\n",
            "Validation Loss: 301141489.1386835\n",
            "Training Loss: 228622003.61331153\n",
            "Validation Loss: 301105473.3785555\n",
            "Training Loss: 228607981.46930408\n",
            "Validation Loss: 301069589.46851546\n",
            "Training Loss: 228594030.5794488\n",
            "Validation Loss: 301033836.86379635\n",
            "Training Loss: 228580150.57818738\n",
            "Validation Loss: 300998215.0221164\n",
            "Training Loss: 228566341.10185906\n",
            "Validation Loss: 300962723.4036668\n",
            "Training Loss: 228552601.78867346\n",
            "Validation Loss: 300927361.47110987\n",
            "Training Loss: 228538932.27872387\n",
            "Validation Loss: 300892128.68955225\n",
            "Training Loss: 228525332.21395835\n",
            "Validation Loss: 300857024.5265418\n",
            "Training Loss: 228511801.23817945\n",
            "Validation Loss: 300822048.4520568\n",
            "Training Loss: 228498338.99703002\n",
            "Validation Loss: 300787199.93848604\n",
            "Training Loss: 228484945.13799363\n",
            "Validation Loss: 300752478.4606255\n",
            "Training Loss: 228471619.3103676\n",
            "Validation Loss: 300717883.4956631\n",
            "Training Loss: 228458361.16527444\n",
            "Validation Loss: 300683414.52316356\n",
            "Training Loss: 228445170.35562953\n",
            "Validation Loss: 300649071.0250618\n",
            "Training Loss: 228432046.53614748\n",
            "Validation Loss: 300614852.4856496\n",
            "Training Loss: 228418989.36333457\n",
            "Validation Loss: 300580758.3915648\n",
            "Training Loss: 228405998.4954663\n",
            "Validation Loss: 300546788.2317768\n",
            "Training Loss: 228393073.59258807\n",
            "Validation Loss: 300512941.49757755\n",
            "Training Loss: 228380214.31650162\n",
            "Validation Loss: 300479217.68257093\n",
            "Training Loss: 228367420.33076394\n",
            "Validation Loss: 300445616.28265977\n",
            "Training Loss: 228354691.3006609\n",
            "Validation Loss: 300412136.7960349\n",
            "Training Loss: 228342026.89322045\n",
            "Validation Loss: 300378778.72316504\n",
            "Training Loss: 228329426.77718064\n",
            "Validation Loss: 300345541.5667821\n",
            "Training Loss: 228316890.62300295\n",
            "Validation Loss: 300312424.8318765\n",
            "Training Loss: 228304418.10284543\n",
            "Validation Loss: 300279428.02567965\n",
            "Training Loss: 228292008.8905655\n",
            "Validation Loss: 300246550.6576561\n",
            "Training Loss: 228279662.6617011\n",
            "Validation Loss: 300213792.23949367\n",
            "Training Loss: 228267379.09347284\n",
            "Validation Loss: 300181152.28509057\n",
            "Training Loss: 228255157.86476618\n",
            "Validation Loss: 300148630.3105444\n",
            "Training Loss: 228242998.6561283\n",
            "Validation Loss: 300116225.8341419\n",
            "Training Loss: 228230901.14975828\n",
            "Validation Loss: 300083938.3763504\n",
            "Training Loss: 228218865.02949485\n",
            "Validation Loss: 300051767.4598026\n",
            "Training Loss: 228206889.98081338\n",
            "Validation Loss: 300019712.60929203\n",
            "Training Loss: 228194975.69081172\n",
            "Validation Loss: 299987773.35175663\n",
            "Training Loss: 228183121.84820682\n",
            "Validation Loss: 299955949.21627283\n",
            "Training Loss: 228171328.14332607\n",
            "Validation Loss: 299924239.7340396\n",
            "Training Loss: 228159594.26809096\n",
            "Validation Loss: 299892644.4383775\n",
            "Training Loss: 228147919.91602182\n",
            "Validation Loss: 299861162.86470467\n",
            "Training Loss: 228136304.7822176\n",
            "Validation Loss: 299829794.5505428\n",
            "Training Loss: 228124748.56335127\n",
            "Validation Loss: 299798539.0354933\n",
            "Training Loss: 228113250.9576726\n",
            "Validation Loss: 299767395.8612334\n",
            "Training Loss: 228101811.66497853\n",
            "Validation Loss: 299736364.5715041\n",
            "Training Loss: 228090430.38662392\n",
            "Validation Loss: 299705444.712105\n",
            "Training Loss: 228079106.82550305\n",
            "Validation Loss: 299674635.83087504\n",
            "Training Loss: 228067840.6860442\n",
            "Validation Loss: 299643937.47769207\n",
            "Training Loss: 228056631.67420733\n",
            "Validation Loss: 299613349.20445627\n",
            "Training Loss: 228045479.49746695\n",
            "Validation Loss: 299582870.56508523\n",
            "Training Loss: 228034383.86480924\n",
            "Validation Loss: 299552501.11549747\n",
            "Training Loss: 228023344.48672256\n",
            "Validation Loss: 299522240.41361165\n",
            "Training Loss: 228012361.07519272\n",
            "Validation Loss: 299492088.0193297\n",
            "Training Loss: 228001433.34369075\n",
            "Validation Loss: 299462043.4945311\n",
            "Training Loss: 227990561.00716788\n",
            "Validation Loss: 299432106.4030596\n",
            "Training Loss: 227979743.78204867\n",
            "Validation Loss: 299402276.31071764\n",
            "Training Loss: 227968981.3862186\n",
            "Validation Loss: 299372552.7852557\n",
            "Training Loss: 227958273.53902227\n",
            "Validation Loss: 299342935.39636105\n",
            "Training Loss: 227947619.96125552\n",
            "Validation Loss: 299313423.71565074\n",
            "Training Loss: 227937020.3751491\n",
            "Validation Loss: 299284017.31665874\n",
            "Training Loss: 227926474.50437406\n",
            "Validation Loss: 299254715.77483195\n",
            "Training Loss: 227915982.07402766\n",
            "Validation Loss: 299225518.6675171\n",
            "Training Loss: 227905542.81062132\n",
            "Validation Loss: 299196425.5739516\n",
            "Training Loss: 227895156.44208068\n",
            "Validation Loss: 299167436.0752563\n",
            "Training Loss: 227884822.69773805\n",
            "Validation Loss: 299138549.75442535\n",
            "Training Loss: 227874541.3083185\n",
            "Validation Loss: 299109766.19631654\n",
            "Training Loss: 227864312.0059443\n",
            "Validation Loss: 299081084.9876433\n",
            "Training Loss: 227854134.52411115\n",
            "Validation Loss: 299052505.7169638\n",
            "Training Loss: 227844008.59769705\n",
            "Validation Loss: 299024027.97467726\n",
            "Training Loss: 227833933.9629435\n",
            "Validation Loss: 298995651.35300744\n",
            "Training Loss: 227823910.3574569\n",
            "Validation Loss: 298967375.4459997\n",
            "Training Loss: 227813937.52019376\n",
            "Validation Loss: 298939199.84951055\n",
            "Training Loss: 227804015.19146293\n",
            "Validation Loss: 298911124.16119677\n",
            "Training Loss: 227794143.11290815\n",
            "Validation Loss: 298883147.9805122\n",
            "Training Loss: 227784321.02750847\n",
            "Validation Loss: 298855270.9086887\n",
            "Training Loss: 227774548.67956844\n",
            "Validation Loss: 298827492.5487432\n",
            "Training Loss: 227764825.81471318\n",
            "Validation Loss: 298799812.50545204\n",
            "Training Loss: 227755152.17987767\n",
            "Validation Loss: 298772230.38535607\n",
            "Training Loss: 227745527.52330515\n",
            "Validation Loss: 298744745.796745\n",
            "Training Loss: 227735951.59453318\n",
            "Validation Loss: 298717358.34964925\n",
            "Training Loss: 227726424.14440107\n",
            "Validation Loss: 298690067.6558354\n",
            "Training Loss: 227716944.92501754\n",
            "Validation Loss: 298662873.32879335\n",
            "Training Loss: 227707513.68978575\n",
            "Validation Loss: 298635774.983731\n",
            "Training Loss: 227698130.19337142\n",
            "Validation Loss: 298608772.23756653\n",
            "Training Loss: 227688794.19170982\n",
            "Validation Loss: 298581864.70891637\n",
            "Training Loss: 227679505.44199198\n",
            "Validation Loss: 298555052.0180923\n",
            "Training Loss: 227670263.70266348\n",
            "Validation Loss: 298528333.7870858\n",
            "Training Loss: 227661068.7334123\n",
            "Validation Loss: 298501709.6395697\n",
            "Training Loss: 227651920.2951689\n",
            "Validation Loss: 298475179.2008816\n",
            "Training Loss: 227642818.1500954\n",
            "Validation Loss: 298448742.0980202\n",
            "Training Loss: 227633762.0615794\n",
            "Validation Loss: 298422397.9596368\n",
            "Training Loss: 227624751.7942342\n",
            "Validation Loss: 298396146.4160274\n",
            "Training Loss: 227615787.11387667\n",
            "Validation Loss: 298369987.0991237\n",
            "Training Loss: 227606867.78753746\n",
            "Validation Loss: 298343919.6424841\n",
            "Training Loss: 227597993.58344766\n",
            "Validation Loss: 298317943.68129224\n",
            "Training Loss: 227589164.27103388\n",
            "Validation Loss: 298292058.85234046\n",
            "Training Loss: 227580379.62090757\n",
            "Validation Loss: 298266264.7940265\n",
            "Training Loss: 227571639.4048651\n",
            "Validation Loss: 298240561.14635044\n",
            "Training Loss: 227562943.3958806\n",
            "Validation Loss: 298214947.5508956\n",
            "Training Loss: 227554291.36809888\n",
            "Validation Loss: 298189423.6508321\n",
            "Training Loss: 227545683.09682205\n",
            "Validation Loss: 298163989.0909029\n",
            "Training Loss: 227537118.35852197\n",
            "Validation Loss: 298138643.5174196\n",
            "Training Loss: 227528596.93080908\n",
            "Validation Loss: 298113386.5782502\n",
            "Training Loss: 227520118.59245068\n",
            "Validation Loss: 298088217.9228198\n",
            "Training Loss: 227511683.12334746\n",
            "Validation Loss: 298063137.20209324\n",
            "Training Loss: 227503290.30453622\n",
            "Validation Loss: 298038144.06857574\n",
            "Training Loss: 227494939.9181873\n",
            "Validation Loss: 298013238.17630285\n",
            "Training Loss: 227486631.7475858\n",
            "Validation Loss: 297988419.1808306\n",
            "Training Loss: 227478365.57713363\n",
            "Validation Loss: 297963686.73923147\n",
            "Training Loss: 227470141.19235244\n",
            "Validation Loss: 297939040.51008904\n",
            "Training Loss: 227461958.37985724\n",
            "Validation Loss: 297914480.15348136\n",
            "Training Loss: 227453816.92737424\n",
            "Validation Loss: 297890005.33098674\n",
            "Training Loss: 227445716.62371048\n",
            "Validation Loss: 297865615.70566726\n",
            "Training Loss: 227437657.25877059\n",
            "Validation Loss: 297841310.94206417\n",
            "Training Loss: 227429638.6235388\n",
            "Validation Loss: 297817090.7061926\n",
            "Training Loss: 227421660.51007506\n",
            "Validation Loss: 297792954.66553265\n",
            "Training Loss: 227413722.7115074\n",
            "Validation Loss: 297768902.4890245\n",
            "Training Loss: 227405825.02203938\n",
            "Validation Loss: 297744933.84705794\n",
            "Training Loss: 227397967.23692116\n",
            "Validation Loss: 297721048.41146713\n",
            "Training Loss: 227390149.15246737\n",
            "Validation Loss: 297697245.8555253\n",
            "Training Loss: 227382370.56603947\n",
            "Validation Loss: 297673525.8539386\n",
            "Training Loss: 227374631.27603966\n",
            "Validation Loss: 297649888.0828313\n",
            "Training Loss: 227366931.08191025\n",
            "Validation Loss: 297626332.2197536\n",
            "Training Loss: 227359269.78412324\n",
            "Validation Loss: 297602857.94365776\n",
            "Training Loss: 227351647.18418548\n",
            "Validation Loss: 297579464.9349067\n",
            "Training Loss: 227344063.08462003\n",
            "Validation Loss: 297556152.87525624\n",
            "Training Loss: 227336517.2889627\n",
            "Validation Loss: 297532921.4478554\n",
            "Training Loss: 227329009.60177398\n",
            "Validation Loss: 297509770.3372368\n",
            "Training Loss: 227321539.82860804\n",
            "Validation Loss: 297486699.22930914\n",
            "Training Loss: 227314107.77602264\n",
            "Validation Loss: 297463707.81135416\n",
            "Training Loss: 227306713.25157702\n",
            "Validation Loss: 297440795.77201635\n",
            "Training Loss: 227299356.06381464\n",
            "Validation Loss: 297417962.8012987\n",
            "Training Loss: 227292036.02226943\n",
            "Validation Loss: 297395208.59055525\n",
            "Training Loss: 227284752.93745285\n",
            "Validation Loss: 297372532.83248687\n",
            "Training Loss: 227277506.62084943\n",
            "Validation Loss: 297349935.22113204\n",
            "Training Loss: 227270296.88491967\n",
            "Validation Loss: 297327415.4518592\n",
            "Training Loss: 227263123.54308498\n",
            "Validation Loss: 297304973.221368\n",
            "Training Loss: 227255986.40972832\n",
            "Validation Loss: 297282608.22767425\n",
            "Training Loss: 227248885.30019006\n",
            "Validation Loss: 297260320.1701076\n",
            "Training Loss: 227241820.03075466\n",
            "Validation Loss: 297238108.74930656\n",
            "Training Loss: 227234790.4186576\n",
            "Validation Loss: 297215973.6672089\n",
            "Training Loss: 227227796.28207287\n",
            "Validation Loss: 297193914.6270494\n",
            "Training Loss: 227220837.4401121\n",
            "Validation Loss: 297171931.333349\n",
            "Training Loss: 227213913.7128114\n",
            "Validation Loss: 297150023.49191517\n",
            "Training Loss: 227207024.9211427\n",
            "Validation Loss: 297128190.80982876\n",
            "Training Loss: 227200170.88698936\n",
            "Validation Loss: 297106432.99544245\n",
            "Training Loss: 227193351.43315738\n",
            "Validation Loss: 297084749.7583755\n",
            "Training Loss: 227186566.3833606\n",
            "Validation Loss: 297063140.80950165\n",
            "Training Loss: 227179815.56222108\n",
            "Validation Loss: 297041605.86095065\n",
            "Training Loss: 227173098.79526487\n",
            "Validation Loss: 297020144.6260991\n",
            "Training Loss: 227166415.90890902\n",
            "Validation Loss: 296998756.81956404\n",
            "Training Loss: 227159766.73047453\n",
            "Validation Loss: 296977442.15719545\n",
            "Training Loss: 227153151.08815938\n",
            "Validation Loss: 296956200.3560767\n",
            "Training Loss: 227146568.8110497\n",
            "Validation Loss: 296935031.1345111\n",
            "Training Loss: 227140019.7291121\n",
            "Validation Loss: 296913934.212023\n",
            "Training Loss: 227133503.67318895\n",
            "Validation Loss: 296892909.30934465\n",
            "Training Loss: 227127020.4749845\n",
            "Validation Loss: 296871956.14841825\n",
            "Training Loss: 227120569.9670761\n",
            "Validation Loss: 296851074.4523851\n",
            "Training Loss: 227114151.9829003\n",
            "Validation Loss: 296830263.9455815\n",
            "Training Loss: 227107766.35674804\n",
            "Validation Loss: 296809524.35353345\n",
            "Training Loss: 227101412.92376494\n",
            "Validation Loss: 296788855.40294856\n",
            "Training Loss: 227095091.51993933\n",
            "Validation Loss: 296768256.82171834\n",
            "Training Loss: 227088801.98211\n",
            "Validation Loss: 296747728.3389003\n",
            "Training Loss: 227082544.1479478\n",
            "Validation Loss: 296727269.6847223\n",
            "Training Loss: 227076317.85596427\n",
            "Validation Loss: 296706880.59057486\n",
            "Training Loss: 227070122.9454961\n",
            "Validation Loss: 296686560.78900254\n",
            "Training Loss: 227063959.25670776\n",
            "Validation Loss: 296666310.0137009\n",
            "Training Loss: 227057826.63058272\n",
            "Validation Loss: 296646127.99951255\n",
            "Training Loss: 227051724.90892965\n",
            "Validation Loss: 296626014.4824199\n",
            "Training Loss: 227045653.93435818\n",
            "Validation Loss: 296605969.19953483\n",
            "Training Loss: 227039613.55029845\n",
            "Validation Loss: 296585991.889108\n",
            "Training Loss: 227033603.60097823\n",
            "Validation Loss: 296566082.29050666\n",
            "Training Loss: 227027623.931428\n",
            "Validation Loss: 296546240.14421916\n",
            "Training Loss: 227021674.38747302\n",
            "Validation Loss: 296526465.19184786\n",
            "Training Loss: 227015754.81573457\n",
            "Validation Loss: 296506757.17610335\n",
            "Training Loss: 227009865.06361908\n",
            "Validation Loss: 296487115.84079826\n",
            "Training Loss: 227004004.9793177\n",
            "Validation Loss: 296467540.93084407\n",
            "Training Loss: 226998174.4117992\n",
            "Validation Loss: 296448032.1922451\n",
            "Training Loss: 226992373.2108144\n",
            "Validation Loss: 296428589.3720933\n",
            "Training Loss: 226986601.2268789\n",
            "Validation Loss: 296409212.2185631\n",
            "Training Loss: 226980858.3112809\n",
            "Validation Loss: 296389900.4809063\n",
            "Training Loss: 226975144.3160713\n",
            "Validation Loss: 296370653.9094476\n",
            "Training Loss: 226969459.0940607\n",
            "Validation Loss: 296351472.2555787\n",
            "Training Loss: 226963802.49881706\n",
            "Validation Loss: 296332355.2717559\n",
            "Training Loss: 226958174.38465822\n",
            "Validation Loss: 296313302.7114892\n",
            "Training Loss: 226952574.60665062\n",
            "Validation Loss: 296294314.3293447\n",
            "Training Loss: 226947003.0206081\n",
            "Validation Loss: 296275389.8809344\n",
            "Training Loss: 226941459.48308206\n",
            "Validation Loss: 296256529.12291443\n",
            "Training Loss: 226935943.85136205\n",
            "Validation Loss: 296237731.8129788\n",
            "Training Loss: 226930455.98346815\n",
            "Validation Loss: 296218997.7098523\n",
            "Training Loss: 226924995.73815227\n",
            "Validation Loss: 296200326.57329124\n",
            "Training Loss: 226919562.97489005\n",
            "Validation Loss: 296181718.16407144\n",
            "Training Loss: 226914157.55388072\n",
            "Validation Loss: 296163172.24399364\n",
            "Training Loss: 226908779.33603802\n",
            "Validation Loss: 296144688.5758662\n",
            "Training Loss: 226903428.18299308\n",
            "Validation Loss: 296126266.9235109\n",
            "Training Loss: 226898103.9570855\n",
            "Validation Loss: 296107907.0517531\n",
            "Training Loss: 226892806.52136067\n",
            "Validation Loss: 296089608.7264168\n",
            "Training Loss: 226887535.73957124\n",
            "Validation Loss: 296071371.7143225\n",
            "Training Loss: 226882291.4761631\n",
            "Validation Loss: 296053195.7832807\n",
            "Training Loss: 226877073.5962886\n",
            "Validation Loss: 296035080.7020886\n",
            "Training Loss: 226871881.96577373\n",
            "Validation Loss: 296017026.24052566\n",
            "Training Loss: 226866716.45115337\n",
            "Validation Loss: 295999032.169347\n",
            "Training Loss: 226861576.91963646\n",
            "Validation Loss: 295981098.26027775\n",
            "Training Loss: 226856463.23911437\n",
            "Validation Loss: 295963224.2860158\n",
            "Training Loss: 226851375.2781587\n",
            "Validation Loss: 295945410.02021974\n",
            "Training Loss: 226846312.9060144\n",
            "Validation Loss: 295927655.2375071\n",
            "Training Loss: 226841275.99259907\n",
            "Validation Loss: 295909959.7134497\n",
            "Training Loss: 226836264.40849283\n",
            "Validation Loss: 295892323.2245701\n",
            "Training Loss: 226831278.02494633\n",
            "Validation Loss: 295874745.54833657\n",
            "Training Loss: 226826316.7138662\n",
            "Validation Loss: 295857226.4631573\n",
            "Training Loss: 226821380.3478204\n",
            "Validation Loss: 295839765.7483781\n",
            "Training Loss: 226816468.8000256\n",
            "Validation Loss: 295822363.18427885\n",
            "Training Loss: 226811581.94435507\n",
            "Validation Loss: 295805018.5520655\n",
            "Training Loss: 226806719.65532148\n",
            "Validation Loss: 295787731.6338674\n",
            "Training Loss: 226801881.80808803\n",
            "Validation Loss: 295770502.21273595\n",
            "Training Loss: 226797068.27845556\n",
            "Validation Loss: 295753330.0726377\n",
            "Training Loss: 226792278.94286162\n",
            "Validation Loss: 295736214.9984473\n",
            "Training Loss: 226787513.6783807\n",
            "Validation Loss: 295719156.77595085\n",
            "Training Loss: 226782772.36271283\n",
            "Validation Loss: 295702155.1918333\n",
            "Training Loss: 226778054.87418884\n",
            "Validation Loss: 295685210.0336805\n",
            "Training Loss: 226773361.09176034\n",
            "Validation Loss: 295668321.08997166\n",
            "Training Loss: 226768690.8950045\n",
            "Validation Loss: 295651488.1500764\n",
            "Training Loss: 226764044.16411203\n",
            "Validation Loss: 295634711.0042509\n",
            "Training Loss: 226759420.77989277\n",
            "Validation Loss: 295617989.44363326\n",
            "Training Loss: 226754820.62375852\n",
            "Validation Loss: 295601323.2602398\n",
            "Training Loss: 226750243.5777397\n",
            "Validation Loss: 295584712.2469607\n",
            "Training Loss: 226745689.5244635\n",
            "Validation Loss: 295568156.19755673\n",
            "Training Loss: 226741158.34716785\n",
            "Validation Loss: 295551654.90665233\n",
            "Training Loss: 226736649.92967844\n",
            "Validation Loss: 295535208.1697379\n",
            "Training Loss: 226732164.15642533\n",
            "Validation Loss: 295518815.78315747\n",
            "Training Loss: 226727700.91242373\n",
            "Validation Loss: 295502477.5441125\n",
            "Training Loss: 226723260.08328965\n",
            "Validation Loss: 295486193.2506526\n",
            "Training Loss: 226718841.5552095\n",
            "Validation Loss: 295469962.70167506\n",
            "Training Loss: 226714445.2149668\n",
            "Validation Loss: 295453785.69691736\n",
            "Training Loss: 226710070.9499194\n",
            "Validation Loss: 295437662.0369579\n",
            "Training Loss: 226705718.6480048\n",
            "Validation Loss: 295421591.5232076\n",
            "Training Loss: 226701388.19773117\n",
            "Validation Loss: 295405573.95790976\n",
            "Training Loss: 226697079.48817953\n",
            "Validation Loss: 295389609.1441341\n",
            "Training Loss: 226692792.40900397\n",
            "Validation Loss: 295373696.8857722\n",
            "Training Loss: 226688526.8504171\n",
            "Validation Loss: 295357836.9875368\n",
            "Training Loss: 226684282.70319638\n",
            "Validation Loss: 295342029.2549557\n",
            "Training Loss: 226680059.8586818\n",
            "Validation Loss: 295326273.49436784\n",
            "Training Loss: 226675858.20877126\n",
            "Validation Loss: 295310569.5129215\n",
            "Training Loss: 226671677.64590716\n",
            "Validation Loss: 295294917.1185681\n",
            "Training Loss: 226667518.0630925\n",
            "Validation Loss: 295279316.1200611\n",
            "Training Loss: 226663379.35387456\n",
            "Validation Loss: 295263766.3269493\n",
            "Training Loss: 226659261.41234756\n",
            "Validation Loss: 295248267.549578\n",
            "Training Loss: 226655164.13314608\n",
            "Validation Loss: 295232819.5990782\n",
            "Training Loss: 226651087.41144785\n",
            "Validation Loss: 295217422.2873701\n",
            "Training Loss: 226647031.14296365\n",
            "Validation Loss: 295202075.42715603\n",
            "Training Loss: 226642995.22393855\n",
            "Validation Loss: 295186778.83191645\n",
            "Training Loss: 226638979.55115247\n",
            "Validation Loss: 295171532.3159074\n",
            "Training Loss: 226634984.02191567\n",
            "Validation Loss: 295156335.6941578\n",
            "Training Loss: 226631008.534054\n",
            "Validation Loss: 295141188.78246427\n",
            "Training Loss: 226627052.98592666\n",
            "Validation Loss: 295126091.39738715\n",
            "Training Loss: 226623117.27641228\n",
            "Validation Loss: 295111043.3562499\n",
            "Training Loss: 226619201.3049021\n",
            "Validation Loss: 295096044.4771338\n",
            "Training Loss: 226615304.97130933\n",
            "Validation Loss: 295081094.5788736\n",
            "Training Loss: 226611428.17605045\n",
            "Validation Loss: 295066193.48105514\n",
            "Training Loss: 226607570.82006243\n",
            "Validation Loss: 295051341.00401306\n",
            "Training Loss: 226603732.8047812\n",
            "Validation Loss: 295036536.9688238\n",
            "Training Loss: 226599914.03215185\n",
            "Validation Loss: 295021781.1973069\n",
            "Training Loss: 226596114.4046208\n",
            "Validation Loss: 295007073.51201713\n",
            "Training Loss: 226592333.82513067\n",
            "Validation Loss: 294992413.7362468\n",
            "Training Loss: 226588572.19712657\n",
            "Validation Loss: 294977801.69401526\n",
            "Training Loss: 226584829.4245407\n",
            "Validation Loss: 294963237.21007043\n",
            "Training Loss: 226581105.41180634\n",
            "Validation Loss: 294948720.109885\n",
            "Training Loss: 226577400.06383622\n",
            "Validation Loss: 294934250.21965176\n",
            "Training Loss: 226573713.28603542\n",
            "Validation Loss: 294919827.36628044\n",
            "Training Loss: 226570044.98429275\n",
            "Validation Loss: 294905451.3773965\n",
            "Training Loss: 226566395.0649766\n",
            "Validation Loss: 294891122.0813347\n",
            "Training Loss: 226562763.43493462\n",
            "Validation Loss: 294876839.307139\n",
            "Training Loss: 226559150.0014964\n",
            "Validation Loss: 294862602.884557\n",
            "Training Loss: 226555554.67245752\n",
            "Validation Loss: 294848412.6440382\n",
            "Training Loss: 226551977.35609132\n",
            "Validation Loss: 294834268.41672873\n",
            "Training Loss: 226548417.9611376\n",
            "Validation Loss: 294820170.0344715\n",
            "Training Loss: 226544876.3968037\n",
            "Validation Loss: 294806117.32980144\n",
            "Training Loss: 226541352.57276452\n",
            "Validation Loss: 294792110.13594013\n",
            "Training Loss: 226537846.3991508\n",
            "Validation Loss: 294778148.2867963\n",
            "Training Loss: 226534357.78655824\n",
            "Validation Loss: 294764231.6169618\n",
            "Training Loss: 226530886.6460375\n",
            "Validation Loss: 294750359.96170515\n",
            "Training Loss: 226527432.88909698\n",
            "Validation Loss: 294736533.1569733\n",
            "Training Loss: 226523996.42769256\n",
            "Validation Loss: 294722751.0393873\n",
            "Training Loss: 226520577.17423502\n",
            "Validation Loss: 294709013.44623464\n",
            "Training Loss: 226517175.04158399\n",
            "Validation Loss: 294695320.2154738\n",
            "Training Loss: 226513789.94303855\n",
            "Validation Loss: 294681671.18572664\n",
            "Training Loss: 226510421.79234704\n",
            "Validation Loss: 294668066.1962738\n",
            "Training Loss: 226507070.50369763\n",
            "Validation Loss: 294654505.0870558\n",
            "Training Loss: 226503735.99171603\n",
            "Validation Loss: 294640987.69867104\n",
            "Training Loss: 226500418.17146307\n",
            "Validation Loss: 294627513.87236357\n",
            "Training Loss: 226497116.9584388\n",
            "Validation Loss: 294614083.4500321\n",
            "Training Loss: 226493832.26857206\n",
            "Validation Loss: 294600696.27421993\n",
            "Training Loss: 226490564.01821914\n",
            "Validation Loss: 294587352.1881153\n",
            "Training Loss: 226487312.1241712\n",
            "Validation Loss: 294574051.03554314\n",
            "Training Loss: 226484076.50363553\n",
            "Validation Loss: 294560792.6609702\n",
            "Training Loss: 226480857.07425186\n",
            "Validation Loss: 294547576.90949494\n",
            "Training Loss: 226477653.7540754\n",
            "Validation Loss: 294534403.6268502\n",
            "Training Loss: 226474466.46157774\n",
            "Validation Loss: 294521272.65939575\n",
            "Training Loss: 226471295.11565572\n",
            "Validation Loss: 294508183.85411936\n",
            "Training Loss: 226468139.6356134\n",
            "Validation Loss: 294495137.0586313\n",
            "Training Loss: 226464999.94116908\n",
            "Validation Loss: 294482132.1211606\n",
            "Training Loss: 226461875.95245522\n",
            "Validation Loss: 294469168.8905589\n",
            "Training Loss: 226458767.59000614\n",
            "Validation Loss: 294456247.21628636\n",
            "Training Loss: 226455674.77476537\n",
            "Validation Loss: 294443366.9484232\n",
            "Training Loss: 226452597.4280829\n",
            "Validation Loss: 294430527.9376508\n",
            "Training Loss: 226449535.47170594\n",
            "Validation Loss: 294417730.0352635\n",
            "Training Loss: 226446488.82778355\n",
            "Validation Loss: 294404973.0931577\n",
            "Training Loss: 226443457.41886333\n",
            "Validation Loss: 294392256.9638314\n",
            "Training Loss: 226440441.16788942\n",
            "Validation Loss: 294379581.5003791\n",
            "Training Loss: 226437439.9981947\n",
            "Validation Loss: 294366946.5564943\n",
            "Training Loss: 226434453.83351284\n",
            "Validation Loss: 294354351.9864628\n",
            "Training Loss: 226431482.59795672\n",
            "Validation Loss: 294341797.64515936\n",
            "Training Loss: 226428526.21603262\n",
            "Validation Loss: 294329283.3880485\n",
            "Training Loss: 226425584.61263022\n",
            "Validation Loss: 294316809.07118106\n",
            "Training Loss: 226422657.71302828\n",
            "Validation Loss: 294304374.55118585\n",
            "Training Loss: 226419745.44288063\n",
            "Validation Loss: 294291979.6852792\n",
            "Training Loss: 226416847.72822124\n",
            "Validation Loss: 294279624.3312484\n",
            "Training Loss: 226413964.49546617\n",
            "Validation Loss: 294267308.3474588\n",
            "Training Loss: 226411095.67140475\n",
            "Validation Loss: 294255031.5928458\n",
            "Training Loss: 226408241.18320003\n",
            "Validation Loss: 294242793.9269181\n",
            "Training Loss: 226405400.95838743\n",
            "Validation Loss: 294230595.2097488\n",
            "Training Loss: 226402574.92487067\n",
            "Validation Loss: 294218435.3019776\n",
            "Training Loss: 226399763.0109255\n",
            "Validation Loss: 294206314.0648054\n",
            "Training Loss: 226396965.14519134\n",
            "Validation Loss: 294194231.3599908\n",
            "Training Loss: 226394181.25667015\n",
            "Validation Loss: 294182187.04985434\n",
            "Training Loss: 226391411.27473125\n",
            "Validation Loss: 294170180.9972656\n",
            "Training Loss: 226388655.12909845\n",
            "Validation Loss: 294158213.065652\n",
            "Training Loss: 226385912.74985772\n",
            "Validation Loss: 294146283.11898667\n",
            "Training Loss: 226383184.0674526\n",
            "Validation Loss: 294134391.02179205\n",
            "Training Loss: 226380469.0126844\n",
            "Validation Loss: 294122536.63913333\n",
            "Training Loss: 226377767.51669917\n",
            "Validation Loss: 294110719.83662206\n",
            "Training Loss: 226375079.51099983\n",
            "Validation Loss: 294098940.4804047\n",
            "Training Loss: 226372404.92743957\n",
            "Validation Loss: 294087198.43716943\n",
            "Training Loss: 226369743.6982164\n",
            "Validation Loss: 294075493.57413894\n",
            "Training Loss: 226367095.75587824\n",
            "Validation Loss: 294063825.75906676\n",
            "Training Loss: 226364461.03331363\n",
            "Validation Loss: 294052194.8602381\n",
            "Training Loss: 226361839.46375352\n",
            "Validation Loss: 294040600.7464676\n",
            "Training Loss: 226359230.98077425\n",
            "Validation Loss: 294029043.2870937\n",
            "Training Loss: 226356635.51828554\n",
            "Validation Loss: 294017522.3519798\n",
            "Training Loss: 226354053.01053602\n",
            "Validation Loss: 294006037.81150824\n",
            "Training Loss: 226351483.3921108\n",
            "Validation Loss: 293994589.5365825\n",
            "Training Loss: 226348926.5979289\n",
            "Validation Loss: 293983177.39862204\n",
            "Training Loss: 226346382.5632407\n",
            "Validation Loss: 293971801.26955926\n",
            "Training Loss: 226343851.22362345\n",
            "Validation Loss: 293960461.0218393\n",
            "Training Loss: 226341332.5149905\n",
            "Validation Loss: 293949156.5284193\n",
            "Training Loss: 226338826.37357625\n",
            "Validation Loss: 293937887.6627593\n",
            "Training Loss: 226336332.73594236\n",
            "Validation Loss: 293926654.2988277\n",
            "Training Loss: 226333851.53897244\n",
            "Validation Loss: 293915456.31109375\n",
            "Training Loss: 226331382.71987492\n",
            "Validation Loss: 293904293.57452965\n",
            "Training Loss: 226328926.21617612\n",
            "Validation Loss: 293893165.9646048\n",
            "Training Loss: 226326481.96572107\n",
            "Validation Loss: 293882073.3572855\n",
            "Training Loss: 226324049.9066706\n",
            "Validation Loss: 293871015.62903064\n",
            "Training Loss: 226321629.97750816\n",
            "Validation Loss: 293859992.65679264\n",
            "Training Loss: 226319222.11701727\n",
            "Validation Loss: 293849004.3180124\n",
            "Training Loss: 226316826.26430765\n",
            "Validation Loss: 293838050.4906185\n",
            "Training Loss: 226314442.35879058\n",
            "Validation Loss: 293827131.0530288\n",
            "Training Loss: 226312070.34018943\n",
            "Validation Loss: 293816245.8841373\n",
            "Training Loss: 226309710.14853284\n",
            "Validation Loss: 293805394.86332417\n",
            "Training Loss: 226307361.72416082\n",
            "Validation Loss: 293794577.8704483\n",
            "Training Loss: 226305025.00770912\n",
            "Validation Loss: 293783794.78584284\n",
            "Training Loss: 226302699.94012254\n",
            "Validation Loss: 293773045.4903194\n",
            "Training Loss: 226300386.4626433\n",
            "Validation Loss: 293762329.86515796\n",
            "Training Loss: 226298084.51681608\n",
            "Validation Loss: 293751647.79211277\n",
            "Training Loss: 226295794.0444801\n",
            "Validation Loss: 293740999.1534056\n",
            "Training Loss: 226293514.98777205\n",
            "Validation Loss: 293730383.8317239\n",
            "Training Loss: 226291247.28912672\n",
            "Validation Loss: 293719801.71022046\n",
            "Training Loss: 226288990.89126796\n",
            "Validation Loss: 293709252.6725094\n",
            "Training Loss: 226286745.73721093\n",
            "Validation Loss: 293698736.60266584\n",
            "Training Loss: 226284511.7702671\n",
            "Validation Loss: 293688253.3852246\n",
            "Training Loss: 226282288.93403357\n",
            "Validation Loss: 293677802.9051736\n",
            "Training Loss: 226280077.17239106\n",
            "Validation Loss: 293667385.04795843\n",
            "Training Loss: 226277876.4295112\n",
            "Validation Loss: 293656999.6994758\n",
            "Training Loss: 226275686.64984748\n",
            "Validation Loss: 293646646.7460724\n",
            "Training Loss: 226273507.7781359\n",
            "Validation Loss: 293636326.0745408\n",
            "Training Loss: 226271339.7593955\n",
            "Validation Loss: 293626037.5721262\n",
            "Training Loss: 226269182.53892598\n",
            "Validation Loss: 293615781.1265129\n",
            "Training Loss: 226267036.0623059\n",
            "Validation Loss: 293605556.6258298\n",
            "Training Loss: 226264900.27538824\n",
            "Validation Loss: 293595363.9586454\n",
            "Training Loss: 226262775.124304\n",
            "Validation Loss: 293585203.0139688\n",
            "Training Loss: 226260660.55545506\n",
            "Validation Loss: 293575073.6812415\n",
            "Training Loss: 226258556.51552165\n",
            "Validation Loss: 293564975.8503449\n",
            "Training Loss: 226256462.95145354\n",
            "Validation Loss: 293554909.4115924\n",
            "Training Loss: 226254379.8104653\n",
            "Validation Loss: 293544874.2557243\n",
            "Training Loss: 226252307.04004896\n",
            "Validation Loss: 293534870.2739141\n",
            "Training Loss: 226250244.5879585\n",
            "Validation Loss: 293524897.35776097\n",
            "Training Loss: 226248192.40221646\n",
            "Validation Loss: 293514955.39928997\n",
            "Training Loss: 226246150.43110523\n",
            "Validation Loss: 293505044.29094976\n",
            "Training Loss: 226244118.62317806\n",
            "Validation Loss: 293495163.92560965\n",
            "Training Loss: 226242096.9272435\n",
            "Validation Loss: 293485314.1965594\n",
            "Training Loss: 226240085.29237443\n",
            "Validation Loss: 293475494.99750704\n",
            "Training Loss: 226238083.66789874\n",
            "Validation Loss: 293465706.22257745\n",
            "Training Loss: 226236092.0034089\n",
            "Validation Loss: 293455947.76630795\n",
            "Training Loss: 226234110.24874872\n",
            "Validation Loss: 293446219.5236493\n",
            "Training Loss: 226232138.3540192\n",
            "Validation Loss: 293436521.38996494\n",
            "Training Loss: 226230176.26957196\n",
            "Validation Loss: 293426853.2610235\n",
            "Training Loss: 226228223.94601965\n",
            "Validation Loss: 293417215.0330027\n",
            "Training Loss: 226226281.33421382\n",
            "Validation Loss: 293407606.6024872\n",
            "Training Loss: 226224348.38526773\n",
            "Validation Loss: 293398027.866464\n",
            "Training Loss: 226222425.05053622\n",
            "Validation Loss: 293388478.7223216\n",
            "Training Loss: 226220511.28162557\n",
            "Validation Loss: 293378959.06784874\n",
            "Training Loss: 226218607.0303883\n",
            "Validation Loss: 293369468.80123293\n",
            "Training Loss: 226216712.2489157\n",
            "Validation Loss: 293360007.8210573\n",
            "Training Loss: 226214826.8895513\n",
            "Validation Loss: 293350576.0263019\n",
            "Training Loss: 226212950.9048763\n",
            "Validation Loss: 293341173.3163395\n",
            "Training Loss: 226211084.24771243\n",
            "Validation Loss: 293331799.59093297\n",
            "Training Loss: 226209226.87112287\n",
            "Validation Loss: 293322454.7502374\n",
            "Training Loss: 226207378.7284081\n",
            "Validation Loss: 293313138.6947913\n",
            "Training Loss: 226205539.77311382\n",
            "Validation Loss: 293303851.32552725\n",
            "Training Loss: 226203709.95901135\n",
            "Validation Loss: 293294592.5437564\n",
            "Training Loss: 226201889.240108\n",
            "Validation Loss: 293285362.25117517\n",
            "Training Loss: 226200077.5706536\n",
            "Validation Loss: 293276160.349861\n",
            "Training Loss: 226198274.9051247\n",
            "Validation Loss: 293266986.74227184\n",
            "Training Loss: 226196481.1982288\n",
            "Validation Loss: 293257841.3312428\n",
            "Training Loss: 226194696.40490472\n",
            "Validation Loss: 293248724.01998687\n",
            "Training Loss: 226192920.4803194\n",
            "Validation Loss: 293239634.7120923\n",
            "Training Loss: 226191153.379873\n",
            "Validation Loss: 293230573.3115167\n",
            "Training Loss: 226189395.0591821\n",
            "Validation Loss: 293221539.72259516\n",
            "Training Loss: 226187645.47410333\n",
            "Validation Loss: 293212533.8500288\n",
            "Training Loss: 226185904.5806991\n",
            "Validation Loss: 293203555.59888905\n",
            "Training Loss: 226184172.3352757\n",
            "Validation Loss: 293194604.8746122\n",
            "Training Loss: 226182448.69434527\n",
            "Validation Loss: 293185681.5830028\n",
            "Training Loss: 226180733.614649\n",
            "Validation Loss: 293176785.6302271\n",
            "Training Loss: 226179027.05314332\n",
            "Validation Loss: 293167916.9228138\n",
            "Training Loss: 226177328.96700963\n",
            "Validation Loss: 293159075.3676535\n",
            "Training Loss: 226175639.31364098\n",
            "Validation Loss: 293150260.8719927\n",
            "Training Loss: 226173958.05064985\n",
            "Validation Loss: 293141473.34343946\n",
            "Training Loss: 226172285.1358634\n",
            "Validation Loss: 293132712.6899551\n",
            "Training Loss: 226170620.52732357\n",
            "Validation Loss: 293123978.81985724\n",
            "Training Loss: 226168964.18328667\n",
            "Validation Loss: 293115271.64181376\n",
            "Training Loss: 226167316.06221536\n",
            "Validation Loss: 293106591.0648462\n",
            "Training Loss: 226165676.12278956\n",
            "Validation Loss: 293097936.99832535\n",
            "Training Loss: 226164044.32390004\n",
            "Validation Loss: 293089309.35197115\n",
            "Training Loss: 226162420.6246388\n",
            "Validation Loss: 293080708.03584933\n",
            "Training Loss: 226160804.98430997\n",
            "Validation Loss: 293072132.96037185\n",
            "Training Loss: 226159197.36242825\n",
            "Validation Loss: 293063584.03629255\n",
            "Training Loss: 226157597.71870908\n",
            "Validation Loss: 293055061.17471045\n",
            "Training Loss: 226156006.01307228\n",
            "Validation Loss: 293046564.2870643\n",
            "Training Loss: 226154422.20564216\n",
            "Validation Loss: 293038093.28513354\n",
            "Training Loss: 226152846.25675035\n",
            "Validation Loss: 293029648.081034\n",
            "Training Loss: 226151278.1269202\n",
            "Validation Loss: 293021228.58721733\n",
            "Training Loss: 226149717.77688277\n",
            "Validation Loss: 293012834.7164739\n",
            "Training Loss: 226148165.1675707\n",
            "Validation Loss: 293004466.38192534\n",
            "Training Loss: 226146620.26010442\n",
            "Validation Loss: 292996123.4970255\n",
            "Training Loss: 226145083.01580852\n",
            "Validation Loss: 292987805.9755606\n",
            "Training Loss: 226143553.39621\n",
            "Validation Loss: 292979513.7316442\n",
            "Training Loss: 226142031.363021\n",
            "Validation Loss: 292971246.6797202\n",
            "Training Loss: 226140516.87815094\n",
            "Validation Loss: 292963004.73455775\n",
            "Training Loss: 226139009.90370527\n",
            "Validation Loss: 292954787.8112522\n",
            "Training Loss: 226137510.40197775\n",
            "Validation Loss: 292946595.8252213\n",
            "Training Loss: 226136018.33545837\n",
            "Validation Loss: 292938428.692208\n",
            "Training Loss: 226134533.66682217\n",
            "Validation Loss: 292930286.32827306\n",
            "Training Loss: 226133056.35894045\n",
            "Validation Loss: 292922168.6497999\n",
            "Training Loss: 226131586.37486565\n",
            "Validation Loss: 292914075.57348794\n",
            "Training Loss: 226130123.67784053\n",
            "Validation Loss: 292906007.0163565\n",
            "Training Loss: 226128668.23129588\n",
            "Validation Loss: 292897962.89573836\n",
            "Training Loss: 226127219.9988461\n",
            "Validation Loss: 292889943.12927985\n",
            "Training Loss: 226125778.94429407\n",
            "Validation Loss: 292881947.63494337\n",
            "Training Loss: 226124345.03161794\n",
            "Validation Loss: 292873976.3310018\n",
            "Training Loss: 226122918.22498903\n",
            "Validation Loss: 292866029.1360367\n",
            "Training Loss: 226121498.48875183\n",
            "Validation Loss: 292858105.9689403\n",
            "Training Loss: 226120085.78743678\n",
            "Validation Loss: 292850206.74891263\n",
            "Training Loss: 226118680.0857503\n",
            "Validation Loss: 292842331.3954601\n",
            "Training Loss: 226117281.34858394\n",
            "Validation Loss: 292834479.8283933\n",
            "Training Loss: 226115889.54100215\n",
            "Validation Loss: 292826651.96782804\n",
            "Training Loss: 226114504.62824523\n",
            "Validation Loss: 292818847.7341813\n",
            "Training Loss: 226113126.57573622\n",
            "Validation Loss: 292811067.04817253\n",
            "Training Loss: 226111755.34906575\n",
            "Validation Loss: 292803309.8308208\n",
            "Training Loss: 226110390.91400608\n",
            "Validation Loss: 292795576.003443\n",
            "Training Loss: 226109033.236501\n",
            "Validation Loss: 292787865.4876554\n",
            "Training Loss: 226107682.28266275\n",
            "Validation Loss: 292780178.2053678\n",
            "Training Loss: 226106338.0187801\n",
            "Validation Loss: 292772514.0787882\n",
            "Training Loss: 226105000.4113106\n",
            "Validation Loss: 292764873.0304141\n",
            "Training Loss: 226103669.42688218\n",
            "Validation Loss: 292757254.98304135\n",
            "Training Loss: 226102345.03229234\n",
            "Validation Loss: 292749659.8597502\n",
            "Training Loss: 226101027.19450745\n",
            "Validation Loss: 292742087.5839171\n",
            "Training Loss: 226099715.88066083\n",
            "Validation Loss: 292734538.0792023\n",
            "Training Loss: 226098411.05804932\n",
            "Validation Loss: 292727011.26955783\n",
            "Training Loss: 226097112.6941417\n",
            "Validation Loss: 292719507.07921803\n",
            "Training Loss: 226095820.75656724\n",
            "Validation Loss: 292712025.4327056\n",
            "Training Loss: 226094535.2131219\n",
            "Validation Loss: 292704566.25482625\n",
            "Training Loss: 226093256.03175688\n",
            "Validation Loss: 292697129.4706668\n",
            "Training Loss: 226091983.18060076\n",
            "Validation Loss: 292689715.0055973\n",
            "Training Loss: 226090716.62793013\n",
            "Validation Loss: 292682322.7852689\n",
            "Training Loss: 226089456.3421862\n",
            "Validation Loss: 292674952.73560965\n",
            "Training Loss: 226088202.29197726\n",
            "Validation Loss: 292667604.78282785\n",
            "Training Loss: 226086954.44605696\n",
            "Validation Loss: 292660278.85340536\n",
            "Training Loss: 226085712.7733464\n",
            "Validation Loss: 292652974.8741037\n",
            "Training Loss: 226084477.24292806\n",
            "Validation Loss: 292645692.7719582\n",
            "Training Loss: 226083247.8240294\n",
            "Validation Loss: 292638432.47427464\n",
            "Training Loss: 226082024.4860425\n",
            "Validation Loss: 292631193.9086335\n",
            "Training Loss: 226080807.1985128\n",
            "Validation Loss: 292623977.00288427\n",
            "Training Loss: 226079595.93113384\n",
            "Validation Loss: 292616781.68514967\n",
            "Training Loss: 226078390.65376294\n",
            "Validation Loss: 292609607.8838188\n",
            "Training Loss: 226077191.3364033\n",
            "Validation Loss: 292602455.52754873\n",
            "Training Loss: 226075997.94921324\n",
            "Validation Loss: 292595324.5452629\n",
            "Training Loss: 226074810.46249598\n",
            "Validation Loss: 292588214.8661524\n",
            "Training Loss: 226073628.84671515\n",
            "Validation Loss: 292581126.41966724\n",
            "Training Loss: 226072453.0724769\n",
            "Validation Loss: 292574059.1355289\n",
            "Training Loss: 226071283.1105329\n",
            "Validation Loss: 292567012.9437132\n",
            "Training Loss: 226070118.93179715\n",
            "Validation Loss: 292559987.7744605\n",
            "Training Loss: 226068960.50731412\n",
            "Validation Loss: 292552983.5582723\n",
            "Training Loss: 226067807.80828428\n",
            "Validation Loss: 292546000.22590506\n",
            "Training Loss: 226066660.806052\n",
            "Validation Loss: 292539037.70837706\n",
            "Training Loss: 226065519.47210863\n",
            "Validation Loss: 292532095.9369612\n",
            "Training Loss: 226064383.77808425\n",
            "Validation Loss: 292525174.84318614\n",
            "Training Loss: 226063253.69575995\n",
            "Validation Loss: 292518274.3588354\n",
            "Training Loss: 226062129.19705433\n",
            "Validation Loss: 292511394.415945\n",
            "Training Loss: 226061010.25402948\n",
            "Validation Loss: 292504534.94680494\n",
            "Training Loss: 226059896.83888692\n",
            "Validation Loss: 292497695.8839569\n",
            "Training Loss: 226058788.92397267\n",
            "Validation Loss: 292490877.16019034\n",
            "Training Loss: 226057686.48176944\n",
            "Validation Loss: 292484078.7085458\n",
            "Training Loss: 226056589.48490623\n",
            "Validation Loss: 292477300.46231353\n",
            "Training Loss: 226055497.90613812\n",
            "Validation Loss: 292470542.3550278\n",
            "Training Loss: 226054411.7183654\n",
            "Validation Loss: 292463804.32046986\n",
            "Training Loss: 226053330.89462948\n",
            "Validation Loss: 292457086.29266876\n",
            "Training Loss: 226052255.4080997\n",
            "Validation Loss: 292450388.2058944\n",
            "Training Loss: 226051185.23208657\n",
            "Validation Loss: 292443709.9946629\n",
            "Training Loss: 226050120.34003517\n",
            "Validation Loss: 292437051.59372985\n",
            "Training Loss: 226049060.70551953\n",
            "Validation Loss: 292430412.93809307\n",
            "Training Loss: 226048006.3022552\n",
            "Validation Loss: 292423793.96298945\n",
            "Training Loss: 226046957.10408586\n",
            "Validation Loss: 292417194.6038967\n",
            "Training Loss: 226045913.08499068\n",
            "Validation Loss: 292410614.79653066\n",
            "Training Loss: 226044874.21907485\n",
            "Validation Loss: 292404054.4768429\n",
            "Training Loss: 226043840.4805786\n",
            "Validation Loss: 292397513.581022\n",
            "Training Loss: 226042811.8438738\n",
            "Validation Loss: 292390992.04549235\n",
            "Training Loss: 226041788.28345752\n",
            "Validation Loss: 292384489.80690956\n",
            "Training Loss: 226040769.77396017\n",
            "Validation Loss: 292378006.80216813\n",
            "Training Loss: 226039756.29013753\n",
            "Validation Loss: 292371542.9683881\n",
            "Training Loss: 226038747.806876\n",
            "Validation Loss: 292365098.24292654\n",
            "Training Loss: 226037744.29918125\n",
            "Validation Loss: 292358672.5633671\n",
            "Training Loss: 226036745.74219808\n",
            "Validation Loss: 292352265.8675258\n",
            "Training Loss: 226035752.11118296\n",
            "Validation Loss: 292345878.0934446\n",
            "Training Loss: 226034763.38152978\n",
            "Validation Loss: 292339509.17939323\n",
            "Training Loss: 226033779.5287455\n",
            "Validation Loss: 292333159.0638704\n",
            "Training Loss: 226032800.52847096\n",
            "Validation Loss: 292326827.68559766\n",
            "Training Loss: 226031826.35646522\n",
            "Validation Loss: 292320514.98352283\n",
            "Training Loss: 226030856.98861006\n",
            "Validation Loss: 292314220.8968186\n",
            "Training Loss: 226029892.4009055\n",
            "Validation Loss: 292307945.3648768\n",
            "Training Loss: 226028932.56948116\n",
            "Validation Loss: 292301688.327315\n",
            "Training Loss: 226027977.4705824\n",
            "Validation Loss: 292295449.72396964\n",
            "Training Loss: 226027027.08057505\n",
            "Validation Loss: 292289229.4948997\n",
            "Training Loss: 226026081.3759407\n",
            "Validation Loss: 292283027.58038\n",
            "Training Loss: 226025140.33328792\n",
            "Validation Loss: 292276843.9209066\n",
            "Training Loss: 226024203.92933384\n",
            "Validation Loss: 292270678.45719117\n",
            "Training Loss: 226023272.14092284\n",
            "Validation Loss: 292264531.13016164\n",
            "Training Loss: 226022344.94501105\n",
            "Validation Loss: 292258401.8809644\n",
            "Training Loss: 226021422.318669\n",
            "Validation Loss: 292252290.65095955\n",
            "Training Loss: 226020504.23908663\n",
            "Validation Loss: 292246197.381716\n",
            "Training Loss: 226019590.68356842\n",
            "Validation Loss: 292240122.0150237\n",
            "Training Loss: 226018681.6295331\n",
            "Validation Loss: 292234064.49288\n",
            "Training Loss: 226017777.05451286\n",
            "Validation Loss: 292228024.75749505\n",
            "Training Loss: 226016876.93615523\n",
            "Validation Loss: 292222002.751288\n",
            "Training Loss: 226015981.25221705\n",
            "Validation Loss: 292215998.4168896\n",
            "Training Loss: 226015089.98057225\n",
            "Validation Loss: 292210011.697137\n",
            "Training Loss: 226014203.09920114\n",
            "Validation Loss: 292204042.53507733\n",
            "Training Loss: 226013320.58619994\n",
            "Validation Loss: 292198090.87396246\n",
            "Training Loss: 226012442.41977203\n",
            "Validation Loss: 292192156.657252\n",
            "Training Loss: 226011568.57823092\n",
            "Validation Loss: 292186239.8286119\n",
            "Training Loss: 226010699.0400058\n",
            "Validation Loss: 292180340.3319107\n",
            "Training Loss: 226009833.78362542\n",
            "Validation Loss: 292174458.11121905\n",
            "Training Loss: 226008972.78773293\n",
            "Validation Loss: 292168593.11081415\n",
            "Training Loss: 226008116.03107518\n",
            "Validation Loss: 292162745.27517414\n",
            "Training Loss: 226007263.4925113\n",
            "Validation Loss: 292156914.5489761\n",
            "Training Loss: 226006415.15100506\n",
            "Validation Loss: 292151100.87709886\n",
            "Training Loss: 226005570.98562342\n",
            "Validation Loss: 292145304.2046228\n",
            "Training Loss: 226004730.97554272\n",
            "Validation Loss: 292139524.4768219\n",
            "Training Loss: 226003895.1000443\n",
            "Validation Loss: 292133761.63917476\n",
            "Training Loss: 226003063.33850878\n",
            "Validation Loss: 292128015.63735145\n",
            "Training Loss: 226002235.67042726\n",
            "Validation Loss: 292122286.4172191\n",
            "Training Loss: 226001412.0753908\n",
            "Validation Loss: 292116573.9248448\n",
            "Training Loss: 226000592.53309658\n",
            "Validation Loss: 292110878.1064843\n",
            "Training Loss: 225999777.0233405\n",
            "Validation Loss: 292105198.90859216\n",
            "Training Loss: 225998965.52602267\n",
            "Validation Loss: 292099536.2778119\n",
            "Training Loss: 225998158.02114317\n",
            "Validation Loss: 292093890.16098374\n",
            "Training Loss: 225997354.48880076\n",
            "Validation Loss: 292088260.5051363\n",
            "Training Loss: 225996554.9092021\n",
            "Validation Loss: 292082647.2574891\n",
            "Training Loss: 225995759.26264536\n",
            "Validation Loss: 292077050.3654523\n",
            "Training Loss: 225994967.5295326\n",
            "Validation Loss: 292071469.77662605\n",
            "Training Loss: 225994179.69036412\n",
            "Validation Loss: 292065905.43879867\n",
            "Training Loss: 225993395.72573864\n",
            "Validation Loss: 292060357.29994655\n",
            "Training Loss: 225992615.616351\n",
            "Validation Loss: 292054825.3082298\n",
            "Training Loss: 225991839.3429945\n",
            "Validation Loss: 292049309.41199964\n",
            "Training Loss: 225991066.88656148\n",
            "Validation Loss: 292043809.5597897\n",
            "Training Loss: 225990298.22803462\n",
            "Validation Loss: 292038325.70031846\n",
            "Training Loss: 225989533.3485012\n",
            "Validation Loss: 292032857.78248894\n",
            "Training Loss: 225988772.22913176\n",
            "Validation Loss: 292027405.755388\n",
            "Training Loss: 225988014.85120648\n",
            "Validation Loss: 292021969.5682832\n",
            "Training Loss: 225987261.19608697\n",
            "Validation Loss: 292016549.1706271\n",
            "Training Loss: 225986511.24523774\n",
            "Validation Loss: 292011144.5120469\n",
            "Training Loss: 225985764.98020932\n",
            "Validation Loss: 292005755.542357\n",
            "Training Loss: 225985022.38265017\n",
            "Validation Loss: 292000382.21154845\n",
            "Training Loss: 225984283.43430135\n",
            "Validation Loss: 291995024.46979254\n",
            "Training Loss: 225983548.1169918\n",
            "Validation Loss: 291989682.26743454\n",
            "Training Loss: 225982816.41264883\n",
            "Validation Loss: 291984355.55500114\n",
            "Training Loss: 225982088.30328417\n",
            "Validation Loss: 291979044.28319395\n",
            "Training Loss: 225981363.77100056\n",
            "Validation Loss: 291973748.40289223\n",
            "Training Loss: 225980642.7979966\n",
            "Validation Loss: 291968467.86514795\n",
            "Training Loss: 225979925.36655626\n",
            "Validation Loss: 291963202.6211899\n",
            "Training Loss: 225979211.45905218\n",
            "Validation Loss: 291957952.6224187\n",
            "Training Loss: 225978501.057948\n",
            "Validation Loss: 291952717.82041067\n",
            "Training Loss: 225977794.1457948\n",
            "Validation Loss: 291947498.1669129\n",
            "Training Loss: 225977090.70522946\n",
            "Validation Loss: 291942293.6138435\n",
            "Training Loss: 225976390.71898174\n",
            "Validation Loss: 291937104.1132936\n",
            "Training Loss: 225975694.1698596\n",
            "Validation Loss: 291931929.6175235\n",
            "Training Loss: 225975001.04077038\n",
            "Validation Loss: 291926770.0789633\n",
            "Training Loss: 225974311.31469455\n",
            "Validation Loss: 291921625.4502133\n",
            "Training Loss: 225973624.97470513\n",
            "Validation Loss: 291916495.68403924\n",
            "Training Loss: 225972942.00395998\n",
            "Validation Loss: 291911380.7333771\n",
            "Training Loss: 225972262.38569734\n",
            "Validation Loss: 291906280.5513301\n",
            "Training Loss: 225971586.10324603\n",
            "Validation Loss: 291901195.09116644\n",
            "Training Loss: 225970913.14001697\n",
            "Validation Loss: 291896124.3063209\n",
            "Training Loss: 225970243.47950163\n",
            "Validation Loss: 291891068.15039355\n",
            "Training Loss: 225969577.10527575\n",
            "Validation Loss: 291886026.5771463\n",
            "Training Loss: 225968914.00099793\n",
            "Validation Loss: 291880999.5405081\n",
            "Training Loss: 225968254.15041274\n",
            "Validation Loss: 291875986.99456835\n",
            "Training Loss: 225967597.53734303\n",
            "Validation Loss: 291870988.89358085\n",
            "Training Loss: 225966944.1456896\n",
            "Validation Loss: 291866005.19196016\n",
            "Training Loss: 225966293.9594406\n",
            "Validation Loss: 291861035.84428173\n",
            "Training Loss: 225965646.96266302\n",
            "Validation Loss: 291856080.805283\n",
            "Training Loss: 225965003.13950053\n",
            "Validation Loss: 291851140.02985924\n",
            "Training Loss: 225964362.47418067\n",
            "Validation Loss: 291846213.47306556\n",
            "Training Loss: 225963724.95101067\n",
            "Validation Loss: 291841301.09011656\n",
            "Training Loss: 225963090.5543716\n",
            "Validation Loss: 291836402.8363829\n",
            "Training Loss: 225962459.26872978\n",
            "Validation Loss: 291831518.66739476\n",
            "Training Loss: 225961831.07862216\n",
            "Validation Loss: 291826648.5388367\n",
            "Training Loss: 225961205.96866983\n",
            "Validation Loss: 291821792.4065533\n",
            "Training Loss: 225960583.9235687\n",
            "Validation Loss: 291816950.2265389\n",
            "Training Loss: 225959964.92809469\n",
            "Validation Loss: 291812121.954947\n",
            "Training Loss: 225959348.96708986\n",
            "Validation Loss: 291807307.5480842\n",
            "Training Loss: 225958736.02548808\n",
            "Validation Loss: 291802506.9624104\n",
            "Training Loss: 225958126.0882886\n",
            "Validation Loss: 291797720.1545371\n",
            "Training Loss: 225957519.14056396\n",
            "Validation Loss: 291792947.08123195\n",
            "b,w found by gradient descent: 30659.20,[-6413.30182016 -8762.1214813  44119.86658853] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X,w,b,test):\n",
        "  n=X.shape[0]\n",
        "  for i in range(n):\n",
        "    y_pred.append(np.dot(X[i],w)+b)\n",
        "    print(test[i],y_pred[i])"
      ],
      "metadata": {
        "id": "yb2aLL1Nh8DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "predict(X_test,w_final,b_final,y_test)"
      ],
      "metadata": {
        "id": "EC4r0xdvQ1Hb",
        "outputId": "7696d2bb-b954-4142-cbad-b447237b9c56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000.0 9615.010987082063\n",
            "32000.0 19128.94387587045\n",
            "29000.0 36126.41349453696\n",
            "9500.0 22816.529598916706\n",
            "18000.0 15106.058858115715\n",
            "20000.0 29327.425647070355\n",
            "20000.0 36414.505293849914\n",
            "30000.0 36126.41349453696\n",
            "20000.0 69101.50455474999\n",
            "10000.0 3779.363626010221\n",
            "75000.0 113630.22485799139\n",
            "250000.0 192147.05230848122\n",
            "16000.0 15729.449952137147\n",
            "60000.0 69724.89564877142\n",
            "30000.0 70121.35273186998\n",
            "15000.0 36001.91592533707\n",
            "15000.0 19525.40095896901\n",
            "12000.0 12618.0478277168\n",
            "200000.0 131024.15155975646\n",
            "14000.0 36001.91592533707\n",
            "17000.0 8930.462104670543\n",
            "19000.0 9218.553903983498\n",
            "75000.0 76632.24878002363\n",
            "14000.0 9326.919187769105\n",
            "22000.0 22528.43779960375\n",
            "35000.0 22528.43779960375\n",
            "12000.0 40210.45630068178\n",
            "16000.0 26607.830508083713\n",
            "20000.0 15729.449952137147\n",
            "28000.0 36126.41349453696\n",
            "23000.0 49327.932106371605\n",
            "14000.0 26324.388806435614\n",
            "10500.0 -979.9278672164\n",
            "14000.0 36001.91592533707\n",
            "16000.0 2131.4742572039395\n",
            "50000.0 39129.4503351717\n",
            "20000.0 14029.702990270496\n",
            "11000.0 6215.517063348761\n",
            "23000.0 20148.79205299044\n",
            "24000.0 25927.931723337053\n",
            "12000.0 9218.553903983498\n",
            "22000.0 19417.035675183404\n",
            "13000.0 5927.425264035803\n",
            "100000.0 36414.505293849914\n",
            "14000.0 22924.894882702312\n",
            "16000.0 17769.14630637713\n",
            "16000.0 9506.645703296454\n",
            "15000.0 12618.0478277168\n",
            "18000.0 16017.541751450104\n",
            "50000.0 70121.35273186998\n",
            "90000.0 164730.99899777657\n",
            "18000.0 16017.541751450104\n",
            "18000.0 42925.40134200356\n",
            "15000.0 8930.462104670543\n",
            "9500.0 -4379.421790949702\n",
            "13000.0 -979.9278672164\n",
            "12000.0 12618.0478277168\n",
            "14000.0 12618.0478277168\n",
            "21000.0 12329.956028403845\n",
            "25000.0 36126.41349453696\n",
            "13500.0 2419.5660565168946\n",
            "25000.0 36414.505293849914\n",
            "30000.0 15729.449952137147\n",
            "14000.0 19813.492758281962\n",
            "15500.0 3779.363626010221\n",
            "20000.0 36126.41349453696\n",
            "18000.0 5639.333464722848\n",
            "40000.0 49148.205590844256\n",
            "13000.0 12618.0478277168\n",
            "25000.0 12329.956028403845\n",
            "100000.0 21168.64023011043\n",
            "20000.0 19128.94387587045\n",
            "17000.0 3779.363626010221\n",
            "70000.0 96741.12052311048\n",
            "13000.0 22816.529598916706\n",
            "30000.0 42528.944258905\n",
            "16000.0 4119.313018383546\n",
            "14000.0 39922.36450136882\n",
            "15000.0 42925.40134200356\n",
            "20000.0 16017.541751450104\n",
            "14000.0 2131.4742572039395\n",
            "13500.0 26324.388806435614\n",
            "14000.0 26324.388806435614\n",
            "11000.0 22924.894882702312\n",
            "24000.0 42925.40134200356\n",
            "17000.0 16017.541751450104\n",
            "20000.0 22816.529598916706\n",
            "10000.0 16017.541751450104\n",
            "20000.0 25819.566439551447\n",
            "17000.0 3779.363626010221\n",
            "10000.0 2419.5660565168946\n",
            "32000.0 29327.425647070355\n",
            "20000.0 15729.449952137147\n",
            "28000.0 29327.425647070355\n",
            "50000.0 39129.4503351717\n",
            "35000.0 49436.29739015721\n",
            "60000.0 117714.26766413619\n",
            "13000.0 5927.425264035803\n",
            "150000.0 124513.2555116028\n",
            "280000.0 185704.14613880226\n",
            "160000.0 70121.35273186998\n",
            "15000.0 36001.91592533707\n",
            "23000.0 12726.413111502407\n",
            "32000.0 19128.94387587045\n",
            "80000.0 27627.678685203704\n",
            "22000.0 13349.804205523837\n",
            "23000.0 39634.27270205587\n",
            "15000.0 12329.956028403845\n",
            "30000.0 56523.37703693677\n",
            "12000.0 9218.553903983498\n",
            "100000.0 117426.17586482325\n",
            "32000.0 56523.37703693677\n",
            "16000.0 19400.903389769115\n",
            "15000.0 2419.5660565168946\n",
            "20000.0 19128.94387587045\n",
            "13000.0 12726.413111502407\n",
            "22000.0 12438.321312189451\n",
            "30000.0 32726.919570803657\n",
            "13000.0 5819.059980250204\n",
            "110000.00000000001 104116.291969203\n",
            "22000.0 16017.541751450104\n",
            "40000.0 59922.87096067007\n",
            "75000.0 103828.20016989003\n",
            "12000.0 3495.9219243621155\n",
            "28000.0 18732.48679277189\n",
            "18000.0 19128.94387587045\n",
            "12000.0 12618.0478277168\n",
            "35000.0 68421.60577000333\n",
            "25000.0 39129.4503351717\n",
            "12500.0 6498.95876499686\n",
            "20000.0 19128.94387587045\n",
            "22000.0 22816.529598916706\n",
            "39000.0 63034.27308509042\n",
            "15000.0 25531.474640238495\n",
            "14000.0 -979.9278672164\n",
            "13000.0 22816.529598916706\n",
            "12000.0 -979.9278672164\n",
            "45000.0 63034.27308509042\n",
            "26000.0 20828.6908377371\n",
            "15000.0 36001.91592533707\n",
            "15000.0 14029.702990270496\n",
            "40000.0 63034.27308509042\n",
            "18000.0 7858.756334490179\n",
            "14500.0 22816.529598916706\n",
            "24000.0 19128.94387587045\n",
            "45000.0 77028.70586312219\n",
            "38000.0 26036.29700712266\n",
            "10000.0 -4091.329991636754\n",
            "35000.0 25927.931723337053\n",
            "15000.0 9218.553903983498\n",
            "10000.0 24856.22595315669\n",
            "20000.0 16017.541751450104\n",
            "28000.0 39129.4503351717\n",
            "15000.0 22816.529598916706\n",
            "23000.0 25927.931723337053\n",
            "20000.0 25927.931723337053\n",
            "10000.0 7178.857549743516\n",
            "10500.0 9615.010987082063\n",
            "15000.0 2419.5660565168946\n",
            "26000.0 9218.553903983498\n",
            "18000.0 15729.449952137147\n",
            "30000.0 38166.10984877694\n",
            "140000.0 117426.17586482325\n",
            "30000.0 29327.425647070355\n",
            "15000.0 26324.388806435614\n",
            "11500.0 2419.5660565168946\n",
            "20000.0 29723.882730168916\n",
            "9000.0 6215.517063348761\n",
            "10000.0 -4379.421790949702\n",
            "12000.0 12726.413111502407\n",
            "38000.0 83719.32842680319\n",
            "12000.0 9218.553903983498\n",
            "22000.0 22528.43779960375\n",
            "165000.0 131024.15155975646\n",
            "15000.0 45984.94587336353\n",
            "22000.0 22816.529598916706\n",
            "14000.0 19525.40095896901\n",
            "45000.0 71589.51558514891\n",
            "20000.0 19128.94387587045\n",
            "20000.0 16017.541751450104\n",
            "14000.0 26324.388806435614\n",
            "22000.0 36126.41349453696\n",
            "12000.0 5819.059980250204\n",
            "10000.0 13014.504910815358\n",
            "10000.0 2419.5660565168946\n",
            "15000.0 19525.40095896901\n",
            "10000.0 12906.139627029756\n",
            "40000.0 66037.30992572516\n",
            "14000.0 2131.4742572039395\n",
            "60000.0 66037.30992572516\n",
            "90000.0 76920.34057933658\n",
            "8500.0 -691.8360679034522\n",
            "12000.0 9218.553903983498\n",
            "14000.0 36001.91592533707\n",
            "18000.0 16017.541751450104\n",
            "10000.0 -871.5625834308012\n",
            "130000.0 103828.20016989003\n",
            "9000.0 12618.0478277168\n",
            "25000.0 9213.903806318645\n",
            "9000.0 -4379.421790949702\n",
            "20000.0 49724.38918947017\n",
            "26000.0 29327.425647070355\n",
            "100000.0 63034.27308509042\n",
            "30000.0 35838.321695224\n",
            "100000.0 73124.38957250472\n",
            "22000.0 36126.41349453696\n",
            "10000.0 12618.0478277168\n",
            "14000.0 26216.023522650008\n",
            "25000.0 33406.81835555032\n",
            "15000.0 5819.059980250204\n",
            "24000.0 22528.43779960375\n",
            "25000.0 36126.41349453696\n",
            "27000.0 15729.449952137147\n",
            "10000.0 2707.6578558298497\n",
            "10500.0 16017.541751450104\n",
            "26000.0 13293.29651479861\n",
            "11000.0 5819.059980250204\n",
            "15000.0 15729.449952137147\n",
            "12000.0 16017.541751450104\n",
            "60000.0 73629.21193938889\n",
            "14000.0 9218.553903983498\n",
            "25000.0 42925.40134200356\n",
            "10000.0 2419.5660565168946\n",
            "10000.0 -871.5625834308012\n",
            "35000.0 73124.38957250472\n",
            "14000.0 26324.388806435614\n",
            "10000.0 2707.6578558298497\n",
            "32000.0 29327.425647070355\n",
            "28000.0 36126.41349453696\n",
            "14000.0 16017.541751450104\n",
            "35000.0 41361.63413708624\n",
            "25000.0 36126.41349453696\n",
            "250000.0 106831.23701052478\n",
            "30000.0 36126.41349453696\n",
            "18500.0 2419.5660565168946\n",
            "25000.0 36126.41349453696\n",
            "20000.0 15729.449952137147\n",
            "18000.0 15729.449952137147\n",
            "25500.0 26324.388806435614\n",
            "11500.0 5819.059980250204\n",
            "10000.0 2419.5660565168946\n",
            "15000.0 19525.40095896901\n",
            "20000.0 15729.449952137147\n",
            "25000.0 29327.425647070355\n",
            "16000.0 8930.462104670543\n",
            "80000.0 97317.3041217364\n",
            "18000.0 15729.449952137147\n",
            "10000.0 -583.4707841178497\n",
            "20000.0 46324.895265736865\n",
            "45000.0 29327.425647070355\n",
            "14000.0 12726.413111502407\n",
            "15000.0 19525.40095896901\n",
            "18000.0 16017.541751450104\n",
            "7500.0 9615.010987082063\n",
            "30000.0 19128.94387587045\n",
            "25000.0 36126.41349453696\n",
            "15000.0 16017.541751450104\n",
            "19000.0 19128.94387587045\n",
            "16000.0 2131.4742572039395\n",
            "13000.0 2419.5660565168946\n",
            "14000.0 19525.40095896901\n",
            "15000.0 16017.541751450104\n",
            "17000.0 16017.541751450104\n",
            "30000.0 2527.9313403025008\n",
            "35000.0 45928.4381826383\n",
            "17000.0 16017.541751450104\n",
            "19000.0 12618.0478277168\n",
            "15000.0 18165.60338947569\n",
            "19000.0 29615.51744638331\n",
            "11500.0 16017.541751450104\n",
            "35000.0 53123.88311320347\n",
            "14000.0 19525.40095896901\n",
            "12000.0 19417.035675183404\n",
            "10000.0 2419.5660565168946\n",
            "20000.0 29723.882730168916\n",
            "10000.0 6215.517063348761\n",
            "12000.0 2419.5660565168946\n",
            "17000.0 9218.553903983498\n",
            "70000.0 128021.11471912172\n",
            "13500.0 22816.529598916706\n",
            "20000.0 19417.035675183404\n",
            "15000.0 15785.957642862379\n",
            "15000.0 36001.91592533707\n",
            "15000.0 6498.95876499686\n",
            "10000.0 2419.5660565168946\n",
            "18000.0 16017.541751450104\n",
            "15000.0 22816.529598916706\n",
            "10000.0 2419.5660565168946\n",
            "11000.0 2419.5660565168946\n",
            "11000.0 2419.5660565168946\n",
            "20000.0 16017.541751450104\n",
            "16000.0 22528.43779960375\n",
            "14000.0 12618.0478277168\n",
            "26500.0 25927.931723337053\n",
            "11000.0 3779.363626010221\n",
            "26000.0 49724.38918947017\n",
            "40000.0 22528.43779960375\n",
            "14000.0 9218.553903983498\n",
            "50000.0 39129.4503351717\n",
            "15000.0 22816.529598916706\n",
            "15000.0 19525.40095896901\n",
            "11500.0 2419.5660565168946\n",
            "30000.0 56523.37703693677\n",
            "11500.0 12618.0478277168\n",
            "20000.0 16017.541751450104\n",
            "23000.0 15729.449952137147\n",
            "15000.0 16017.541751450104\n",
            "18000.0 10630.209066537194\n",
            "22000.0 37146.26167165695\n",
            "80000.0 83719.32842680319\n",
            "22000.0 25927.931723337053\n",
            "80000.0 49724.38918947017\n",
            "20000.0 15729.449952137147\n",
            "10000.0 12618.0478277168\n",
            "16000.0 8930.462104670543\n",
            "17000.0 29327.425647070355\n",
            "16000.0 12618.0478277168\n",
            "15000.0 19525.40095896901\n",
            "40000.0 38732.993252073145\n",
            "10000.0 29903.609245696265\n",
            "120000.0 117426.17586482325\n",
            "13000.0 9218.553903983498\n",
            "300000.0 239807.95711922212\n",
            "35000.0 70121.35273186998\n",
            "15500.0 5819.059980250204\n",
            "23000.0 19128.94387587045\n",
            "25000.0 19128.94387587045\n",
            "35000.0 35729.9564114384\n",
            "27000.0 53123.88311320347\n",
            "18000.0 16017.541751450104\n",
            "25000.0 38166.10984877694\n",
            "100000.0 110915.2798166696\n",
            "15000.0 15729.449952137147\n",
            "10500.0 -4379.421790949702\n",
            "13500.0 12329.956028403845\n",
            "120000.0 117930.99823170742\n",
            "14000.0 2419.5660565168946\n",
            "17000.0 17089.24752163047\n",
            "18000.0 15729.449952137147\n",
            "16000.0 15729.449952137147\n",
            "15000.0 33015.01137011661\n",
            "13000.0 -979.9278672164\n",
            "20000.0 18732.48679277189\n",
            "25000.0 49724.38918947017\n",
            "20000.0 8930.462104670543\n",
            "18000.0 22924.894882702312\n",
            "110000.00000000001 117426.17586482325\n",
            "60000.0 59526.41387757151\n",
            "25000.0 39129.4503351717\n",
            "45000.0 85419.07538866984\n",
            "14000.0 5275.140952452872\n",
            "22000.0 19128.94387587045\n",
            "25000.0 9326.919187769105\n",
            "30000.0 29327.425647070355\n",
            "15000.0 19525.40095896901\n",
            "18000.0 7858.756334490179\n",
            "32000.0 19128.94387587045\n",
            "32000.0 29327.425647070355\n",
            "14000.0 9218.553903983498\n",
            "25000.0 16017.541751450104\n",
            "11000.0 16017.541751450104\n",
            "10000.0 -583.4707841178497\n",
            "25000.0 24636.1240323184\n",
            "25000.0 29327.425647070355\n",
            "40000.0 22528.43779960375\n",
            "80000.0 93233.26131559156\n",
            "12000.0 12618.0478277168\n",
            "27000.0 56523.37703693677\n",
            "10000.0 2419.5660565168946\n",
            "16000.0 15729.449952137147\n",
            "16000.0 22816.529598916706\n",
            "10000.0 7178.857549743516\n",
            "25000.0 36126.41349453696\n",
            "65000.0 32330.4624877051\n",
            "15000.0 12618.0478277168\n",
            "16000.0 12726.413111502407\n",
            "12000.0 26324.388806435614\n",
            "45000.0 35838.321695224\n",
            "35000.0 56523.37703693677\n",
            "15000.0 9506.645703296454\n",
            "35000.0 29327.425647070355\n",
            "10000.0 17377.33932094342\n",
            "40000.0 83719.32842680319\n",
            "24000.0 19128.94387587045\n",
            "11000.0 2419.5660565168946\n",
            "19000.0 22528.43779960375\n",
            "15000.0 9218.553903983498\n",
            "150000.0 100716.7980454697\n",
            "22000.0 19525.40095896901\n",
            "20000.0 36522.87057763552\n",
            "19000.0 39525.90741827026\n",
            "150000.0 127516.29235223755\n",
            "16000.0 12618.0478277168\n",
            "15000.0 12329.956028403845\n",
            "11000.0 9218.553903983498\n",
            "14500.0 2419.5660565168946\n",
            "10000.0 9218.553903983498\n",
            "18000.0 20556.731323838438\n",
            "18000.0 29327.425647070355\n",
            "32000.0 19128.94387587045\n",
            "18000.0 29327.425647070355\n",
            "22000.0 26324.388806435614\n",
            "15000.0 2419.5660565168946\n",
            "25000.0 49724.38918947017\n",
            "15000.0 12329.956028403845\n",
            "25000.0 22528.43779960375\n",
            "250000.0 171818.07864455608\n",
            "20000.0 5191.018788563913\n",
            "14000.0 15729.449952137147\n",
            "22000.0 12726.413111502407\n",
            "10000.0 12618.0478277168\n",
            "8500.0 -583.4707841178497\n",
            "25000.0 16017.541751450104\n",
            "16000.0 22816.529598916706\n",
            "50000.0 70121.35273186998\n",
            "14000.0 19813.492758281962\n",
            "10000.0 -4379.421790949702\n",
            "13000.0 5927.425264035803\n",
            "20000.0 49724.38918947017\n",
            "15000.0 -871.5625834308012\n",
            "10000.0 -4379.421790949702\n",
            "18000.0 36126.41349453696\n",
            "11500.0 12618.0478277168\n",
            "12000.0 2816.0231396154522\n",
            "17000.0 3491.2718266972624\n",
            "15000.0 9326.919187769105\n",
            "18000.0 36126.41349453696\n",
            "10000.0 6159.009372623528\n",
            "35000.0 73124.38957250472\n",
            "25000.0 22528.43779960375\n",
            "38000.0 28930.9685639718\n",
            "18000.0 22528.43779960375\n",
            "18000.0 16017.541751450104\n",
            "10000.0 22816.529598916706\n",
            "10500.0 36414.505293849914\n",
            "11000.0 9218.553903983498\n",
            "15000.0 2419.5660565168946\n",
            "100000.0 110915.2798166696\n",
            "30000.0 56523.37703693677\n",
            "12000.0 9615.010987082063\n",
            "19000.0 32835.28485458926\n",
            "27000.0 42925.40134200356\n",
            "13000.0 9218.553903983498\n",
            "12000.0 19417.035675183404\n",
            "12000.0 9218.553903983498\n",
            "23000.0 22528.43779960375\n",
            "15000.0 22924.894882702312\n",
            "13000.0 8930.462104670543\n",
            "19000.0 25927.931723337053\n",
            "12000.0 3779.363626010221\n",
            "25000.0 15729.449952137147\n",
            "21000.0 22528.43779960375\n",
            "13000.0 -979.9278672164\n",
            "16000.0 29327.425647070355\n",
            "30000.0 39129.4503351717\n",
            "50000.0 63322.364884403374\n",
            "42000.0 83719.32842680319\n",
            "10000.0 2419.5660565168946\n",
            "15000.0 36001.91592533707\n",
            "20000.0 12618.0478277168\n",
            "27000.0 20540.599038424145\n",
            "30000.0 17769.14630637713\n",
            "22000.0 42925.40134200356\n",
            "45000.0 48160.62198455285\n",
            "12000.0 12618.0478277168\n",
            "12000.0 16017.541751450104\n",
            "25000.0 19128.94387587045\n",
            "25000.0 22528.43779960375\n",
            "18000.0 25927.931723337053\n",
            "80000.0 76920.34057933658\n",
            "40000.0 38166.10984877694\n",
            "12000.0 9218.553903983498\n",
            "20000.0 9326.919187769105\n",
            "10000.0 5819.059980250204\n",
            "11000.0 -4379.421790949702\n",
            "24000.0 76920.34057933658\n",
            "10000.0 6107.151779563152\n",
            "50000.0 131312.2433590694\n",
            "12000.0 12618.0478277168\n",
            "25000.0 26324.388806435614\n",
            "80000.0 49724.38918947017\n",
            "55000.0 39129.4503351717\n",
            "95000.0 73124.38957250472\n",
            "12000.0 23496.42838366337\n",
            "9500.0 12906.139627029756\n",
            "20000.0 19525.40095896901\n",
            "75000.0 110627.18801735665\n",
            "36000.0 27287.729292830372\n",
            "33000.0 85470.9329817302\n",
            "11000.0 9218.553903983498\n",
            "11500.0 2419.5660565168946\n",
            "65000.0 34138.57473335735\n",
            "16000.0 19525.40095896901\n",
            "20000.0 -1268.0196665293624\n",
            "14000.0 26324.388806435614\n",
            "17000.0 12618.0478277168\n",
            "45000.0 59526.41387757151\n",
            "22000.0 22528.43779960375\n",
            "11000.0 9218.553903983498\n",
            "60000.0 81679.63207256321\n",
            "28000.0 36126.41349453696\n",
            "20000.0 5819.059980250204\n",
            "22500.0 25531.474640238495\n",
            "65000.0 39525.90741827026\n",
            "17000.0 12329.956028403845\n",
            "25000.0 22528.43779960375\n",
            "11000.0 19417.035675183404\n",
            "19000.0 15729.449952137147\n",
            "20000.0 15729.449952137147\n",
            "16000.0 5819.059980250204\n",
            "40000.0 22528.43779960375\n",
            "35000.0 78835.53936437667\n",
            "29500.0 25927.931723337053\n",
            "16000.0 8930.462104670543\n",
            "16000.0 9506.645703296454\n",
            "21500.0 26324.388806435614\n",
            "18000.0 15729.449952137147\n",
            "30000.0 66325.40172503811\n",
            "20000.0 16017.541751450104\n",
            "14000.0 19417.035675183404\n",
            "11000.0 -979.9278672164\n",
            "20000.0 19128.94387587045\n",
            "50000.0 39525.90741827026\n",
            "80000.0 117274.06382245962\n",
            "20000.0 9326.919187769105\n",
            "14000.0 7967.121618275785\n",
            "20000.0 56523.37703693677\n",
            "28000.0 15837.815235922753\n",
            "12500.0 -979.9278672164\n",
            "15000.0 12329.956028403845\n",
            "25000.0 36126.41349453696\n",
            "20000.0 25927.931723337053\n",
            "25000.0 22528.43779960375\n",
            "22000.0 15729.449952137147\n",
            "19000.0 22528.43779960375\n",
            "10000.0 2419.5660565168946\n",
            "35000.0 26216.023522650008\n",
            "17000.0 53411.97491251642\n",
            "14000.0 33123.37665390222\n",
            "11000.0 22816.529598916706\n",
            "22000.0 15729.449952137147\n",
            "20000.0 22528.43779960375\n",
            "20000.0 4511.12000381725\n",
            "16000.0 2419.5660565168946\n",
            "12500.0 9218.553903983498\n",
            "10000.0 19813.492758281962\n",
            "20000.0 11933.498945305288\n",
            "10000.0 15054.20126505534\n",
            "50000.0 36126.41349453696\n",
            "11000.0 9218.553903983498\n",
            "10000.0 22816.529598916706\n",
            "18000.0 5927.425264035803\n",
            "24000.0 32726.919570803657\n",
            "80000.0 15729.449952137147\n",
            "14000.0 12726.413111502407\n",
            "15000.0 29615.51744638331\n",
            "15000.0 5819.059980250204\n",
            "21000.0 19128.94387587045\n",
            "10000.0 26612.48060574857\n",
            "10000.0 16413.998834548667\n",
            "22000.0 8534.005021571986\n",
            "14000.0 12618.0478277168\n",
            "20000.0 15729.449952137147\n",
            "15000.0 36001.91592533707\n",
            "100000.0 90518.31627426979\n",
            "10000.0 -871.5625834308012\n",
            "16000.0 22924.894882702312\n",
            "14000.0 9218.553903983498\n",
            "30000.0 42925.40134200356\n",
            "11000.0 9218.553903983498\n",
            "30000.0 38166.10984877694\n",
            "16000.0 9218.553903983498\n",
            "12000.0 2419.5660565168946\n",
            "20000.0 21168.64023011043\n",
            "14000.0 2131.4742572039395\n",
            "17000.0 5819.059980250204\n",
            "28000.0 35729.9564114384\n",
            "28000.0 32726.919570803657\n",
            "18000.0 22528.43779960375\n",
            "70000.0 76920.34057933658\n",
            "15000.0 36001.91592533707\n",
            "18600.0 16017.541751450104\n",
            "50000.0 97600.74582338448\n",
            "180000.0 144622.12725468967\n",
            "18000.0 22528.43779960375\n",
            "30000.0 19128.94387587045\n",
            "12000.0 2419.5660565168946\n",
            "25000.0 29327.425647070355\n",
            "12000.0 12618.0478277168\n",
            "14000.0 36001.91592533707\n",
            "18000.0 19128.94387587045\n",
            "11000.0 16017.541751450104\n",
            "25000.0 31367.122001310338\n",
            "8500.0 9506.645703296454\n",
            "10000.0 8198.70572686351\n",
            "20000.0 19128.94387587045\n",
            "35000.0 22816.529598916706\n",
            "10000.0 12618.0478277168\n",
            "12500.0 9218.553903983498\n",
            "15000.0 -4379.421790949702\n",
            "35000.0 22816.529598916706\n",
            "10000.0 -871.5625834308012\n",
            "60000.0 117426.17586482325\n",
            "25000.0 37486.21106403028\n",
            "16000.0 19417.035675183404\n",
            "120000.0 91522.03216597548\n",
            "30000.0 17769.14630637713\n",
            "22000.0 13349.804205523837\n",
            "16500.0 17377.33932094342\n",
            "25000.0 32726.919570803657\n",
            "45000.0 59922.87096067007\n",
            "15000.0 26216.023522650008\n",
            "20000.0 19417.035675183404\n",
            "13000.0 16017.541751450104\n",
            "30000.0 39129.4503351717\n",
            "36000.0 62246.00901655815\n",
            "15000.0 5927.425264035803\n",
            "26000.0 29723.882730168916\n",
            "13000.0 20885.19852846233\n",
            "50000.0 59922.87096067007\n",
            "16000.0 29615.51744638331\n",
            "18000.0 2131.4742572039395\n",
            "16000.0 46433.26054952247\n",
            "15000.0 19813.492758281962\n",
            "90000.0 32726.919570803657\n",
            "80000.0 87118.82235053647\n",
            "35000.0 29039.3338477574\n",
            "16000.0 19128.94387587045\n",
            "47000.0 46404.367331960966\n",
            "14000.0 5927.425264035803\n",
            "33000.0 85470.9329817302\n",
            "40000.0 63034.27308509042\n",
            "17000.0 22816.529598916706\n",
            "50000.0 39525.90741827026\n",
            "10500.0 2419.5660565168946\n",
            "12000.0 9218.553903983498\n",
            "31000.0 22528.43779960375\n",
            "25000.0 36126.41349453696\n",
            "25000.0 2131.4742572039395\n",
            "10500.0 13014.504910815358\n",
            "16000.0 29327.425647070355\n",
            "15000.0 2707.6578558298497\n",
            "14000.0 46596.85477963553\n",
            "140000.0 114026.68194108995\n",
            "20000.0 25927.931723337053\n",
            "57000.0 36126.41349453696\n",
            "35000.0 36126.41349453696\n",
            "32000.0 29327.425647070355\n",
            "21000.0 32726.919570803657\n",
            "30000.0 32618.55428701805\n",
            "15500.0 11938.149042970144\n",
            "15000.0 36001.91592533707\n",
            "40000.0 70121.35273186998\n",
            "16000.0 2419.5660565168946\n",
            "50000.0 39129.4503351717\n",
            "25000.0 25927.931723337053\n",
            "19000.0 15729.449952137147\n",
            "25000.0 39525.90741827026\n",
            "12000.0 9218.553903983498\n",
            "16000.0 25927.931723337053\n",
            "65000.0 86830.73055122353\n",
            "35000.0 35729.9564114384\n",
            "37000.0 83322.87134370462\n",
            "11000.0 29615.51744638331\n",
            "32000.0 35838.321695224\n",
            "13000.0 16017.541751450104\n",
            "20000.0 26036.29700712266\n",
            "19000.0 12618.0478277168\n",
            "14000.0 5819.059980250204\n",
            "15000.0 12618.0478277168\n",
            "18000.0 5927.425264035803\n",
            "100000.0 77940.18875645657\n",
            "25000.0 19128.94387587045\n",
            "42000.0 49724.38918947017\n",
            "35000.0 29327.425647070355\n",
            "19000.0 16017.541751450104\n",
            "20000.0 31707.071393683666\n",
            "40000.0 63034.27308509042\n",
            "16000.0 9218.553903983498\n",
            "25000.0 19128.94387587045\n",
            "60000.0 93629.71839869014\n",
            "18000.0 16017.541751450104\n",
            "9000.0 12906.139627029756\n",
            "40000.0 63322.364884403374\n",
            "45000.0 32618.55428701805\n",
            "21000.0 22528.43779960375\n",
            "16000.0 9218.553903983498\n",
            "27000.0 29327.425647070355\n",
            "25000.0 29327.425647070355\n",
            "18000.0 36414.505293849914\n",
            "25000.0 25927.931723337053\n",
            "16000.0 6607.324048782466\n",
            "14000.0 22816.529598916706\n",
            "100000.0 36126.41349453696\n",
            "10000.0 6838.908157370184\n",
            "38000.0 23831.727678371844\n",
            "10500.0 15734.100049802004\n",
            "12000.0 26324.388806435614\n",
            "10000.0 -871.5625834308012\n",
            "20000.0 25531.474640238495\n",
            "50000.0 47288.23575213163\n",
            "19000.0 5927.425264035803\n",
            "10000.0 4067.455425323169\n",
            "100000.0 141222.63333095636\n",
            "25000.0 46721.352348835426\n",
            "20000.0 16017.541751450104\n",
            "33000.0 29327.425647070355\n",
            "35000.0 78223.63045810467\n",
            "16000.0 13349.804205523837\n",
            "21000.0 3779.363626010221\n",
            "14500.0 3779.363626010221\n",
            "50000.0 66325.40172503811\n",
            "250000.0 138111.231206536\n",
            "17000.0 15729.449952137147\n",
            "22000.0 36126.41349453696\n",
            "70000.0 32726.919570803657\n",
            "17000.0 22924.894882702312\n",
            "25000.0 19128.94387587045\n",
            "21000.0 19128.94387587045\n",
            "22000.0 16125.907035235707\n",
            "22000.0 15729.449952137147\n",
            "40000.0 45248.53939789165\n",
            "23000.0 2419.5660565168946\n",
            "250000.0 185416.05433948932\n",
            "11000.0 22816.529598916706\n",
            "60000.0 56235.285237623815\n",
            "16500.0 35729.9564114384\n",
            "12000.0 2419.5660565168946\n",
            "15000.0 15729.449952137147\n",
            "24000.0 16409.34873688381\n",
            "10500.0 15054.20126505534\n",
            "20000.0 19525.40095896901\n",
            "35000.0 29327.425647070355\n",
            "150000.0 107119.32880983772\n",
            "10500.0 12618.0478277168\n",
            "20000.0 15729.449952137147\n",
            "16000.0 2419.5660565168946\n",
            "18000.0 9326.919187769105\n",
            "13000.0 12329.956028403845\n",
            "35000.0 50404.28797421683\n",
            "90000.0 63034.27308509042\n",
            "14000.0 36001.91592533707\n",
            "10000.0 6215.517063348761\n",
            "16000.0 5819.059980250204\n",
            "20000.0 12278.098435343469\n",
            "23000.0 42925.40134200356\n",
            "18000.0 15729.449952137147\n",
            "12000.0 2419.5660565168946\n",
            "10000.0 9615.010987082063\n",
            "20000.0 41565.603772510236\n",
            "12000.0 26612.48060574857\n",
            "15000.0 22528.43779960375\n",
            "32000.0 25531.474640238495\n",
            "18000.0 33123.37665390222\n",
            "25000.0 29327.425647070355\n",
            "17000.0 26607.830508083713\n",
            "16000.0 12726.413111502407\n",
            "12500.0 9218.553903983498\n",
            "21000.0 34426.66653267031\n",
            "11000.0 -871.5625834308012\n",
            "10500.0 12618.0478277168\n",
            "14000.0 19808.842660617112\n",
            "16000.0 22816.529598916706\n",
            "12000.0 9218.553903983498\n",
            "22000.0 19128.94387587045\n",
            "13000.0 21157.158042360996\n",
            "9000.0 6215.517063348761\n",
            "30000.0 25927.931723337053\n",
            "35000.0 59922.87096067007\n",
            "40000.0 42925.40134200356\n",
            "14000.0 16017.541751450104\n",
            "90000.0 55163.57946744345\n",
            "15000.0 2419.5660565168946\n",
            "10000.0 9218.553903983498\n",
            "15000.0 46596.85477963553\n",
            "12000.0 9218.553903983498\n",
            "25000.0 22528.43779960375\n",
            "16000.0 17769.14630637713\n",
            "45000.0 55838.82815452525\n",
            "26000.0 49724.38918947017\n",
            "125000.0 83719.32842680319\n",
            "28000.0 25927.931723337053\n",
            "50000.0 70121.35273186998\n",
            "10000.0 9615.010987082063\n",
            "13000.0 9218.553903983498\n",
            "10000.0 33015.01137011661\n",
            "15000.0 2419.5660565168946\n",
            "25000.0 25927.931723337053\n",
            "19000.0 19128.94387587045\n",
            "14000.0 16017.541751450104\n",
            "11500.0 -979.9278672164\n",
            "110000.00000000001 104111.64187153814\n",
            "13000.0 16017.541751450104\n",
            "229999.99999999997 117426.17586482325\n",
            "10000.0 -979.9278672164\n",
            "27000.0 42528.944258905\n",
            "25000.0 36126.41349453696\n",
            "32000.0 26347.353181934486\n",
            "15000.0 15106.058858115715\n",
            "15000.0 19525.40095896901\n",
            "45000.0 36126.41349453696\n",
            "10000.0 22816.529598916706\n",
            "20000.0 36126.41349453696\n",
            "20000.0 16017.541751450104\n",
            "20000.0 24568.134153843734\n",
            "20000.0 12329.956028403845\n",
            "12000.0 9218.553903983498\n",
            "40000.0 76920.34057933658\n",
            "20000.0 15729.449952137147\n",
            "15000.0 12618.0478277168\n",
            "20000.0 24568.134153843734\n",
            "20000.0 32726.919570803657\n",
            "25000.0 36126.41349453696\n",
            "10000.0 29903.609245696265\n",
            "14000.0 8930.462104670543\n",
            "75000.0 81051.59088087692\n",
            "20000.0 2131.4742572039395\n",
            "18000.0 16017.541751450104\n",
            "20000.0 36126.41349453696\n",
            "24000.0 22528.43779960375\n",
            "20000.0 56523.37703693677\n",
            "13500.0 12618.0478277168\n",
            "32000.0 32330.4624877051\n",
            "13000.0 2419.5660565168946\n",
            "60000.0 72920.41993708072\n",
            "20000.0 10970.158458910526\n",
            "8000.0 -583.4707841178497\n",
            "35000.0 25531.474640238495\n",
            "13000.0 26216.023522650008\n",
            "12000.0 9218.553903983498\n",
            "10500.0 2419.5660565168946\n",
            "11000.0 6498.95876499686\n",
            "11000.0 9218.553903983498\n",
            "22000.0 22528.43779960375\n",
            "8000.0 16305.633550763065\n",
            "18000.0 8930.462104670543\n",
            "12000.0 16017.541751450104\n",
            "15000.0 22528.43779960375\n",
            "20000.0 9218.553903983498\n",
            "28000.0 29615.51744638331\n",
            "180000.0 151421.11510215627\n",
            "22000.0 32330.4624877051\n",
            "20000.0 32726.919570803657\n",
            "20000.0 21168.64023011043\n",
            "15500.0 25927.931723337053\n",
            "11000.0 7518.8069421168475\n",
            "10000.0 13014.504910815358\n",
            "20000.0 32726.919570803657\n",
            "18000.0 19525.40095896901\n",
            "90000.0 104116.291969203\n",
            "16000.0 8930.462104670543\n",
            "85000.0 94026.1754817887\n",
            "24000.0 8137.547938473428\n",
            "18000.0 36126.41349453696\n",
            "22000.0 8930.462104670543\n",
            "10000.0 9218.553903983498\n",
            "36000.0 70121.35273186998\n",
            "30000.0 42925.40134200356\n",
            "40000.0 39809.34911991836\n",
            "21000.0 19417.035675183404\n",
            "100000.0 70121.35273186998\n",
            "110000.00000000001 94597.70898274975\n",
            "40000.0 73124.38957250472\n",
            "180000.0 146945.26531057776\n",
            "18000.0 15729.449952137147\n",
            "35000.0 22528.43779960375\n",
            "20000.0 12618.0478277168\n",
            "30000.0 24228.184761470402\n",
            "75000.0 97317.3041217364\n",
            "22000.0 25927.931723337053\n",
            "27000.0 19128.94387587045\n",
            "20000.0 32330.4624877051\n",
            "25000.0 36126.41349453696\n",
            "10000.0 -583.4707841178497\n",
            "20000.0 32726.919570803657\n",
            "11500.0 12618.0478277168\n",
            "18000.0 7858.756334490179\n",
            "16000.0 13014.504910815358\n",
            "10000.0 2419.5660565168946\n",
            "30000.0 18732.48679277189\n",
            "13000.0 22816.529598916706\n",
            "360000.0 219410.99357682234\n",
            "15500.0 25927.931723337053\n",
            "15000.0 12618.0478277168\n",
            "16000.0 16017.541751450104\n",
            "280000.0 199014.0300344225\n",
            "15000.0 36001.91592533707\n",
            "10000.0 17377.33932094342\n",
            "11000.0 12726.413111502407\n",
            "16000.0 9218.553903983498\n",
            "19000.0 22924.894882702312\n",
            "20000.0 15729.449952137147\n",
            "11000.0 33303.10316942957\n",
            "22000.0 20540.599038424145\n",
            "26000.0 42925.40134200356\n",
            "150000.0 117029.71878172467\n",
            "35000.0 70121.35273186998\n",
            "15000.0 15729.449952137147\n",
            "22000.0 9326.919187769105\n",
            "13000.0 6215.517063348761\n",
            "30000.0 39525.90741827026\n",
            "12000.0 -11466.501437729268\n",
            "12000.0 19417.035675183404\n",
            "14000.0 2131.4742572039395\n",
            "250000.0 191926.95038764292\n",
            "18500.0 36182.92118526219\n",
            "12000.0 12618.0478277168\n",
            "30000.0 15332.99286903859\n",
            "70000.0 81679.63207256321\n",
            "15000.0 5927.425264035803\n",
            "21000.0 19128.94387587045\n",
            "200000.0 136547.4640016187\n",
            "15000.0 8930.462104670543\n",
            "25000.0 36126.41349453696\n",
            "25000.0 25927.931723337053\n",
            "17000.0 2419.5660565168946\n",
            "50000.0 70121.35273186998\n",
            "60000.0 58331.48928258903\n",
            "15000.0 -4667.513590262664\n",
            "45000.0 45928.4381826383\n",
            "11500.0 13014.504910815358\n",
            "12000.0 22816.529598916706\n",
            "13500.0 9218.553903983498\n",
            "10000.0 6498.95876499686\n",
            "11000.0 20776.833244676724\n",
            "15000.0 2419.5660565168946\n",
            "16000.0 19525.40095896901\n",
            "25000.0 19128.94387587045\n",
            "10000.0 6215.517063348761\n",
            "11500.0 22924.894882702312\n",
            "30000.0 29327.425647070355\n",
            "20000.0 9218.553903983498\n",
            "12500.0 2419.5660565168946\n",
            "30000.0 18732.48679277189\n",
            "15000.0 9218.553903983498\n",
            "40000.0 38166.10984877694\n",
            "10500.0 -979.9278672164\n",
            "30000.0 33123.37665390222\n",
            "12500.0 2419.5660565168946\n",
            "16000.0 22924.894882702312\n",
            "90000.0 95481.57740292042\n",
            "12000.0 16305.633550763065\n",
            "21000.0 32726.919570803657\n",
            "40000.0 55107.07177671822\n",
            "15000.0 15729.449952137147\n",
            "30000.0 39129.4503351717\n",
            "10000.0 16017.541751450104\n",
            "450000.0 171818.07864455608\n",
            "24000.0 19128.94387587045\n",
            "45000.0 79923.37741997132\n",
            "11000.0 -4379.421790949702\n",
            "12000.0 25536.124737903345\n",
            "25000.0 18449.04509112379\n",
            "114999.99999999999 104116.291969203\n",
            "10000.0 -583.4707841178497\n",
            "16000.0 12726.413111502407\n",
            "27000.0 42925.40134200356\n",
            "16000.0 12906.139627029756\n",
            "17000.0 3779.363626010221\n",
            "15000.0 46596.85477963553\n",
            "40000.0 66433.76700882372\n",
            "12000.0 26324.388806435614\n",
            "25000.0 29327.425647070355\n",
            "28000.0 25927.931723337053\n",
            "15500.0 32726.919570803657\n",
            "21000.0 22528.43779960375\n",
            "18000.0 8930.462104670543\n",
            "19000.0 20556.731323838438\n",
            "38000.0 76740.61406380923\n",
            "15000.0 -979.9278672164\n",
            "20000.0 8822.096820884937\n",
            "11000.0 13014.504910815358\n",
            "20000.0 22924.894882702312\n",
            "16000.0 24352.682330670297\n",
            "20000.0 12329.956028403845\n",
            "18000.0 29327.425647070355\n",
            "16000.0 16017.541751450104\n",
            "15000.0 12618.0478277168\n",
            "20000.0 15729.449952137147\n",
            "45000.0 70121.35273186998\n",
            "170000.0 87118.82235053647\n",
            "11000.0 2419.5660565168946\n",
            "22000.0 21576.579500958425\n",
            "24000.0 16817.288007731804\n",
            "120000.0 103828.20016989003\n",
            "14000.0 12618.0478277168\n",
            "20000.0 15729.449952137147\n",
            "15000.0 19525.40095896901\n",
            "25000.0 26324.388806435614\n",
            "20000.0 9218.553903983498\n",
            "22000.0 25927.931723337053\n",
            "37000.0 56523.37703693677\n",
            "17000.0 16017.541751450104\n",
            "10000.0 7178.857549743516\n",
            "60000.0 59526.41387757151\n",
            "12000.0 9218.553903983498\n",
            "19000.0 11990.006636030514\n",
            "17000.0 22528.43779960375\n",
            "50000.0 22528.43779960375\n",
            "10000.0 2419.5660565168946\n",
            "45000.0 83322.87134370462\n",
            "350000.0 331197.83597692277\n",
            "18000.0 46324.895265736865\n",
            "15000.0 16017.541751450104\n",
            "22000.0 23888.23536909707\n",
            "22000.0 22924.894882702312\n",
            "17000.0 15729.449952137147\n",
            "16000.0 16017.541751450104\n",
            "30000.0 29615.51744638331\n",
            "14000.0 22528.43779960375\n",
            "10000.0 35054.707724356595\n",
            "14000.0 46596.85477963553\n",
            "20000.0 9326.919187769105\n",
            "16300.0 19417.035675183404\n",
            "20000.0 19128.94387587045\n",
            "17000.0 26216.023522650008\n",
            "35000.0 36126.41349453696\n",
            "13000.0 12618.0478277168\n",
            "15000.0 2419.5660565168946\n",
            "14000.0 2023.108973418337\n",
            "11000.0 2419.5660565168946\n",
            "18000.0 9218.553903983498\n",
            "50000.0 32726.919570803657\n",
            "14000.0 16125.907035235707\n",
            "450000.0 171818.07864455608\n",
            "25000.0 15729.449952137147\n",
            "18000.0 8930.462104670543\n",
            "16000.0 8930.462104670543\n",
            "55000.0 66325.40172503811\n",
            "15000.0 26324.388806435614\n",
            "12500.0 13014.504910815358\n",
            "12000.0 2816.0231396154522\n",
            "16500.0 9218.553903983498\n",
            "100000.0 49724.38918947017\n",
            "45000.0 104116.291969203\n",
            "45000.0 66325.40172503811\n",
            "11000.0 -979.9278672164\n",
            "10000.0 2419.5660565168946\n",
            "20000.0 12726.413111502407\n",
            "11000.0 16017.541751450104\n",
            "16000.0 23344.316341299746\n",
            "30000.0 29327.425647070355\n",
            "924000.0 49724.38918947017\n",
            "280000.0 185704.14613880226\n",
            "65000.0 52944.15659767612\n",
            "35000.0 36126.41349453696\n",
            "17000.0 29615.51744638331\n",
            "30000.0 8642.370305357588\n",
            "27000.0 22816.529598916706\n",
            "12000.0 33015.01137011661\n",
            "32000.0 29327.425647070355\n",
            "16000.0 19128.94387587045\n",
            "12000.0 9218.553903983498\n",
            "20000.0 5530.968180937241\n",
            "16000.0 3439.4142336368895\n",
            "20000.0 5927.425264035803\n",
            "40000.0 26036.29700712266\n",
            "14000.0 16125.907035235707\n",
            "130000.0 90518.31627426979\n",
            "50000.0 54767.1223843449\n",
            "140000.0 117714.26766413619\n",
            "22000.0 19417.035675183404\n",
            "150000.0 100716.7980454697\n",
            "11000.0 16125.907035235707\n",
            "45000.0 2131.4742572039395\n",
            "19000.0 12329.956028403845\n",
            "20000.0 15729.449952137147\n",
            "13500.0 29615.51744638331\n",
            "20000.0 7858.756334490179\n",
            "22000.0 36126.41349453696\n",
            "17000.0 19417.035675183404\n",
            "75000.0 90230.22447495683\n",
            "50000.0 38913.99851199827\n",
            "25000.0 25536.124737903345\n",
            "14000.0 -4379.421790949702\n",
            "13000.0 22816.529598916706\n",
            "28000.0 28375.56734842503\n",
            "16000.0 19128.94387587045\n",
            "30000.0 17769.14630637713\n",
            "22000.0 19417.035675183404\n",
            "18000.0 14374.302480308685\n",
            "185000.0 208816.05472252387\n",
            "30000.0 29327.425647070355\n",
            "50000.0 36522.87057763552\n",
            "14000.0 9218.553903983498\n",
            "25000.0 59526.41387757151\n",
            "10000.0 -979.9278672164\n",
            "35000.0 49832.75447325577\n",
            "40000.0 43605.300126750226\n",
            "10000.0 16017.541751450104\n",
            "13500.0 22816.529598916706\n",
            "14000.0 9615.010987082063\n",
            "14000.0 16125.907035235707\n",
            "18000.0 19525.40095896901\n",
            "20000.0 9218.553903983498\n",
            "30000.0 22528.43779960375\n",
            "30000.0 22528.43779960375\n",
            "24000.0 24228.184761470402\n",
            "10000.0 16413.998834548667\n",
            "32500.0 31367.122001310338\n",
            "40000.0 45928.4381826383\n",
            "18000.0 24964.591236942295\n",
            "32000.0 22131.980716505193\n",
            "33000.0 45928.4381826383\n",
            "14000.0 5819.059980250204\n",
            "12000.0 26324.388806435614\n",
            "24000.0 42925.40134200356\n",
            "18000.0 29327.425647070355\n",
            "50000.0 39129.4503351717\n",
            "35000.0 29039.3338477574\n",
            "15000.0 15729.449952137147\n",
            "19000.0 19417.035675183404\n",
            "32000.0 53123.88311320347\n",
            "50000.0 71057.07874510101\n",
            "19000.0 8930.462104670543\n",
            "40000.0 32726.919570803657\n",
            "35000.0 25531.474640238495\n",
            "26000.0 42925.40134200356\n",
            "15000.0 8930.462104670543\n",
            "85000.0 97029.21232242342\n",
            "12000.0 16017.541751450104\n",
            "250000.0 202017.06687505724\n",
            "9000.0 12906.139627029756\n",
            "16000.0 15729.449952137147\n",
            "20000.0 31707.071393683666\n",
            "20000.0 36126.41349453696\n",
            "300000.0 165019.0907970895\n",
            "15000.0 15729.449952137147\n",
            "19000.0 16017.541751450104\n",
            "12000.0 16017.541751450104\n",
            "11500.0 -4379.421790949702\n",
            "12000.0 19525.40095896901\n",
            "28000.0 15729.449952137147\n",
            "16000.0 5819.059980250204\n",
            "20000.0 25927.931723337053\n",
            "14000.0 5819.059980250204\n",
            "14000.0 29723.882730168916\n",
            "17000.0 19128.94387587045\n",
            "12000.0 19417.035675183404\n",
            "250000.0 199014.0300344225\n",
            "15000.0 2131.4742572039395\n",
            "15000.0 19128.94387587045\n",
            "18000.0 9218.553903983498\n",
            "19000.0 36522.87057763552\n",
            "100000.0 114314.7737404029\n",
            "15000.0 9218.553903983498\n",
            "30000.0 36126.41349453696\n",
            "40000.0 37486.21106403028\n",
            "12000.0 16017.541751450104\n",
            "30000.0 29327.425647070355\n",
            "12000.0 19417.035675183404\n",
            "25000.0 37486.21106403028\n",
            "12000.0 12726.413111502407\n",
            "15000.0 22528.43779960375\n",
            "35000.0 45928.4381826383\n",
            "80000.0 59798.37339147017\n",
            "12000.0 19813.492758281962\n",
            "20000.0 15729.449952137147\n",
            "25000.0 35838.321695224\n",
            "22000.0 29723.882730168916\n",
            "27000.0 25927.931723337053\n",
            "20000.0 15729.449952137147\n",
            "18000.0 15729.449952137147\n",
            "25000.0 17769.14630637713\n",
            "20000.0 22528.43779960375\n",
            "26000.0 29723.882730168916\n",
            "11000.0 22816.529598916706\n",
            "30000.0 17769.14630637713\n",
            "40000.0 79923.37741997132\n",
            "23000.0 15729.449952137147\n",
            "47000.0 83719.32842680319\n",
            "13000.0 28255.71987688999\n",
            "8500.0 6215.517063348761\n",
            "30000.0 28930.9685639718\n",
            "145000.0 76920.34057933658\n",
            "20000.0 10630.209066537194\n",
            "9000.0 19813.492758281962\n",
            "20000.0 32726.919570803657\n",
            "30000.0 19128.94387587045\n",
            "14000.0 19417.035675183404\n",
            "25000.0 42925.40134200356\n",
            "12000.0 9218.553903983498\n",
            "16000.0 15729.449952137147\n",
            "16000.0 25927.931723337053\n",
            "15000.0 19128.94387587045\n",
            "25000.0 36126.41349453696\n",
            "25000.0 19128.94387587045\n",
            "18000.0 16017.541751450104\n",
            "50000.0 56235.285237623815\n",
            "15000.0 3779.363626010221\n",
            "85000.0 83719.32842680319\n",
            "16000.0 12329.956028403845\n",
            "14000.0 46596.85477963553\n",
            "20000.0 16017.541751450104\n",
            "20000.0 22924.894882702312\n",
            "20000.0 19128.94387587045\n",
            "30000.0 83431.23662749023\n",
            "40000.0 36126.41349453696\n",
            "33000.0 96065.87183602867\n",
            "15000.0 8930.462104670543\n",
            "26000.0 22528.43779960375\n",
            "15000.0 19128.94387587045\n",
            "9000.0 12906.139627029756\n",
            "65000.0 90230.22447495683\n",
            "12000.0 9218.553903983498\n",
            "10000.0 -871.5625834308012\n",
            "19000.0 5927.425264035803\n",
            "13000.0 26324.388806435614\n",
            "22000.0 8534.005021571986\n",
            "36000.0 46336.377453486304\n",
            "120000.0 117714.26766413619\n",
            "17000.0 2419.5660565168946\n",
            "9000.0 26216.023522650008\n",
            "22000.0 22924.894882702312\n",
            "35000.0 49724.38918947017\n",
            "24000.0 26324.388806435614\n",
            "13000.0 26324.388806435614\n",
            "10000.0 4119.313018383546\n",
            "15000.0 26324.388806435614\n",
            "140000.0 175325.937852075\n",
            "25000.0 36126.41349453696\n",
            "25000.0 43213.49314131652\n",
            "25000.0 35729.9564114384\n",
            "25000.0 29435.79093085596\n",
            "17000.0 29723.882730168916\n",
            "30000.0 49384.439797096835\n",
            "15000.0 12726.413111502407\n",
            "12000.0 2419.5660565168946\n",
            "12000.0 19417.035675183404\n",
            "15000.0 9218.553903983498\n",
            "400000.0 331197.83597692277\n",
            "16000.0 16125.907035235707\n",
            "72000.0 97029.21232242342\n",
            "27000.0 12001.488823779953\n",
            "21000.0 39922.36450136882\n",
            "11500.0 2419.5660565168946\n",
            "15000.0 3099.464841263558\n",
            "16000.0 16125.907035235707\n",
            "12000.0 -4379.421790949702\n",
            "17000.0 22528.43779960375\n",
            "18000.0 14369.652382643828\n",
            "14000.0 9218.553903983498\n",
            "25000.0 15729.449952137147\n",
            "18000.0 8930.462104670543\n",
            "40000.0 39525.90741827026\n",
            "15500.0 3779.363626010221\n",
            "12000.0 12906.139627029756\n",
            "20000.0 25927.931723337053\n",
            "13000.0 8930.462104670543\n",
            "17000.0 22816.529598916706\n",
            "10000.0 16305.633550763065\n",
            "16000.0 25927.931723337053\n",
            "25000.0 26036.29700712266\n",
            "16000.0 29327.425647070355\n",
            "70000.0 76344.15698071066\n",
            "17000.0 1059.7684870235753\n",
            "14000.0 2131.4742572039395\n",
            "12000.0 9218.553903983498\n",
            "12000.0 26324.388806435614\n",
            "20000.0 4511.12000381725\n",
            "15000.0 19525.40095896901\n",
            "35000.0 70121.35273186998\n",
            "10500.0 -979.9278672164\n",
            "14000.0 26324.388806435614\n",
            "19000.0 16017.541751450104\n",
            "50000.0 60246.68806762911\n",
            "30000.0 45644.9964809902\n",
            "16000.0 2131.4742572039395\n",
            "10000.0 7575.314632842081\n",
            "12000.0 9218.553903983498\n",
            "18000.0 17429.196914003798\n",
            "14000.0 12618.0478277168\n",
            "11500.0 9218.553903983498\n",
            "35000.0 55838.82815452525\n",
            "20000.0 19128.94387587045\n",
            "25000.0 8930.462104670543\n",
            "38000.0 32726.919570803657\n",
            "15000.0 16125.907035235707\n",
            "12000.0 9218.553903983498\n",
            "18000.0 16017.541751450104\n",
            "16000.0 16017.541751450104\n",
            "15000.0 19525.40095896901\n",
            "12000.0 33015.01137011661\n",
            "50000.0 71141.20090898997\n",
            "20000.0 15729.449952137147\n",
            "14000.0 12726.413111502407\n",
            "15000.0 2419.5660565168946\n",
            "17000.0 14029.702990270496\n",
            "15000.0 2419.5660565168946\n",
            "35000.0 42925.40134200356\n",
            "12000.0 9218.553903983498\n",
            "25000.0 15729.449952137147\n",
            "10000.0 5819.059980250204\n",
            "72000.0 22131.980716505193\n",
            "10000.0 2419.5660565168946\n",
            "10000.0 2419.5660565168946\n",
            "18000.0 15729.449952137147\n",
            "35000.0 32307.498112206224\n",
            "14000.0 16017.541751450104\n",
            "20000.0 11026.666149635756\n",
            "20000.0 15729.449952137147\n",
            "19000.0 2419.5660565168946\n",
            "14000.0 26324.388806435614\n",
            "7500.0 -4379.421790949702\n",
            "30000.0 29327.425647070355\n",
            "14000.0 2131.4742572039395\n",
            "30000.0 34766.61592504364\n",
            "14000.0 35390.00701906507\n",
            "19000.0 16413.998834548667\n",
            "75000.0 90230.22447495683\n",
            "11000.0 9218.553903983498\n",
            "80000.0 93233.26131559156\n",
            "18000.0 16017.541751450104\n",
            "15000.0 15729.449952137147\n",
            "18000.0 22528.43779960375\n",
            "35000.0 36126.41349453696\n",
            "25000.0 42925.40134200356\n",
            "28000.0 19128.94387587045\n",
            "16000.0 9218.553903983498\n",
            "15000.0 22816.529598916706\n",
            "10000.0 13014.504910815358\n",
            "15000.0 22924.894882702312\n",
            "18000.0 16017.541751450104\n",
            "30000.0 38166.10984877694\n",
            "27000.0 19525.40095896901\n",
            "18000.0 3779.363626010221\n",
            "10000.0 -979.9278672164\n",
            "16000.0 2811.373041950603\n",
            "16000.0 9218.553903983498\n",
            "16000.0 16017.541751450104\n",
            "260000.0 199014.0300344225\n",
            "14000.0 -583.4707841178497\n",
            "15000.0 16017.541751450104\n",
            "14000.0 9218.553903983498\n",
            "150000.0 121113.7615878695\n",
            "250000.0 158220.1029496229\n",
            "15000.0 23212.986682015267\n",
            "12000.0 33123.37665390222\n",
            "25000.0 36018.04821075135\n",
            "10000.0 13122.870194600968\n",
            "16000.0 12329.956028403845\n",
            "68000.0 63430.73016818898\n",
            "30000.0 22528.43779960375\n",
            "215000.0 127516.29235223755\n",
            "55000.0 67605.72722830734\n",
            "25000.0 36126.41349453696\n",
            "12000.0 9218.553903983498\n",
            "16000.0 15729.449952137147\n",
            "20000.0 16017.541751450104\n",
            "16000.0 16413.998834548667\n",
            "11000.0 22816.529598916706\n",
            "10000.0 12906.139627029756\n",
            "26000.0 32726.919570803657\n",
            "14500.0 19417.035675183404\n",
            "15000.0 9615.010987082063\n",
            "10000.0 29903.609245696265\n",
            "14000.0 5819.059980250204\n",
            "22000.0 19417.035675183404\n",
            "40000.0 11933.498945305288\n",
            "40000.0 29327.425647070355\n",
            "15000.0 15729.449952137147\n",
            "20000.0 16017.541751450104\n",
            "11000.0 9218.553903983498\n",
            "14000.0 36001.91592533707\n",
            "40000.0 97029.21232242342\n",
            "15000.0 -871.5625834308012\n",
            "12000.0 9218.553903983498\n",
            "18000.0 12618.0478277168\n",
            "16000.0 3779.363626010221\n",
            "14000.0 12726.413111502407\n",
            "90000.0 93521.35311490453\n",
            "280000.0 199014.0300344225\n",
            "45000.0 63034.27308509042\n",
            "30000.0 32726.919570803657\n",
            "25000.0 53123.88311320347\n",
            "10500.0 12618.0478277168\n",
            "17000.0 19128.94387587045\n",
            "16000.0 9218.553903983498\n",
            "60000.0 86722.36526743793\n",
            "20000.0 15729.449952137147\n",
            "16000.0 19525.40095896901\n",
            "14000.0 36001.91592533707\n",
            "150000.0 104116.291969203\n",
            "9000.0 6215.517063348761\n",
            "14000.0 19128.94387587045\n",
            "20000.0 35174.55519589163\n",
            "25000.0 35554.8799935759\n",
            "140000.0 117714.26766413619\n",
            "65000.0 68421.60577000333\n",
            "15000.0 19128.94387587045\n",
            "35000.0 22528.43779960375\n",
            "20000.0 15729.449952137147\n",
            "30000.0 40545.75559539025\n",
            "17000.0 9218.553903983498\n",
            "14000.0 9218.553903983498\n",
            "100000.0 90518.31627426979\n",
            "13000.0 16125.907035235707\n",
            "30000.0 22528.43779960375\n",
            "11000.0 -4379.421790949702\n",
            "15000.0 2419.5660565168946\n",
            "10500.0 5887.049858724866\n",
            "85000.0 73124.38957250472\n",
            "16000.0 20216.781931465106\n",
            "22000.0 15729.449952137147\n",
            "19000.0 22924.894882702312\n",
            "18000.0 36126.41349453696\n",
            "17000.0 16017.541751450104\n",
            "22000.0 22528.43779960375\n",
            "120000.0 93809.44491421749\n",
            "18000.0 19417.035675183404\n",
            "15000.0 19525.40095896901\n",
            "18000.0 16352.84104615858\n",
            "16000.0 26324.388806435614\n",
            "13500.0 -4379.421790949702\n",
            "20000.0 12329.956028403845\n",
            "15000.0 5819.059980250204\n",
            "20000.0 15729.449952137147\n",
            "110000.00000000001 42925.40134200356\n",
            "16000.0 16017.541751450104\n",
            "23500.0 29327.425647070355\n",
            "170000.0 219699.08537613528\n",
            "10000.0 16305.633550763065\n",
            "45000.0 56523.37703693677\n",
            "17000.0 2419.5660565168946\n",
            "30000.0 24908.083546217065\n",
            "19000.0 24964.591236942295\n",
            "18000.0 15729.449952137147\n",
            "20000.0 17973.115941801127\n",
            "14000.0 12618.0478277168\n",
            "15000.0 3779.363626010221\n",
            "19000.0 22816.529598916706\n",
            "45000.0 73124.38957250472\n",
            "12000.0 9218.553903983498\n",
            "14000.0 19417.035675183404\n",
            "25000.0 32726.919570803657\n",
            "17000.0 22528.43779960375\n",
            "200000.0 174821.11548519085\n",
            "300000.0 179528.5493853571\n",
            "15000.0 36001.91592533707\n",
            "9000.0 6215.517063348761\n",
            "20000.0 15729.449952137147\n",
            "59000.0 68761.55516237665\n",
            "19000.0 16017.541751450104\n",
            "20000.0 29327.425647070355\n",
            "24000.0 36126.41349453696\n",
            "30000.0 35729.9564114384\n",
            "20000.0 36126.41349453696\n",
            "23000.0 49327.932106371605\n",
            "16000.0 19525.40095896901\n",
            "25000.0 32726.919570803657\n",
            "15000.0 26324.388806435614\n",
            "15000.0 10346.767364889092\n",
            "18000.0 45928.4381826383\n",
            "24000.0 19128.94387587045\n",
            "15000.0 46596.85477963553\n",
            "9000.0 12906.139627029756\n",
            "16000.0 12906.139627029756\n",
            "10000.0 2419.5660565168946\n",
            "22000.0 8930.462104670543\n",
            "300000.0 181908.1951319704\n",
            "20000.0 22816.529598916706\n",
            "16000.0 15729.449952137147\n",
            "15000.0 5530.968180937241\n",
            "12000.0 9218.553903983498\n",
            "13000.0 5819.059980250204\n",
            "80000.0 104116.291969203\n",
            "12000.0 5819.059980250204\n",
            "15000.0 8930.462104670543\n",
            "15000.0 19128.94387587045\n",
            "15000.0 5819.059980250204\n",
            "20000.0 9218.553903983498\n",
            "13000.0 22816.529598916706\n",
            "11000.0 19525.40095896901\n",
            "25000.0 36126.41349453696\n",
            "16000.0 9218.553903983498\n",
            "15000.0 9218.553903983498\n",
            "250000.0 138111.231206536\n",
            "30000.0 25531.474640238495\n",
            "30000.0 29327.425647070355\n",
            "150000.0 86722.36526743793\n",
            "40000.0 56523.37703693677\n",
            "20000.0 26324.388806435614\n",
            "15000.0 2419.5660565168946\n",
            "18000.0 32726.919570803657\n",
            "14000.0 16017.541751450104\n",
            "11000.0 -583.4707841178497\n",
            "15000.0 19128.94387587045\n",
            "12000.0 9218.553903983498\n",
            "22000.0 22528.43779960375\n",
            "150000.0 100716.7980454697\n",
            "20000.0 26607.830508083713\n",
            "20000.0 32726.919570803657\n",
            "19000.0 44421.17866844621\n",
            "20000.0 63034.27308509042\n",
            "14000.0 2419.5660565168946\n",
            "18000.0 29327.425647070355\n",
            "18000.0 36126.41349453696\n",
            "25000.0 29327.425647070355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2=r2_score(y_test,y_pred)\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "EUvtVLxh6SNb",
        "outputId": "9d733e80-d3e7-464e-a8d7-267c5962d8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5947859235472304\n"
          ]
        }
      ]
    }
  ]
}